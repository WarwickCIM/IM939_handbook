{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Dimension reduction (2)\n",
    "\n",
    "In the previous lab, we learned how to work with dimension reduction in a relatively simple dataset. In this lab we are going to work with a much larger dataset: a subset of the US Census & Crime data [@misc_communities_and_crime_183]. Details about the various variables can be found [here](http://archive.ics.uci.edu/ml/datasets/communities+and+crime).\n",
    "\n",
    "How can we tackle a dataset with 100 different dimensions? We try using PCA and clustering. There is a lot you can do with this dataset. We encourage you to dig in. Try and find some interesting patterns in the data. Even upload your findings to the Teams channel and discuss.\n",
    "\n",
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('data/censusCrimeClean.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: callout-caution\n",
    "\n",
    "## Do-this-yourself: Let's say \"hi\" to the data\n",
    "\n",
    "You have probably learnt by now that the best start to working with a new data set is to get to know it. What would be a good way to approach this?\n",
    "\n",
    "A few steps we always take:\n",
    "- Use the internal spreadsheet viewer in JupyterLab, go into the data folder and have a look at the data.\n",
    "- Check the size of the dataset. Do you remember how to look at the `shape` of a dataset?\n",
    "- Get a summary of the data, observe the *descriptive* statistics.\n",
    "- Check the data types and see if there are mixes of different types, such as numerical, categorical, textual, etc..\n",
    "- Create a visualisation to get a first peek into the relations. Think about the number of features. Is a visualisation feasible?\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: callout-caution\n",
    "\n",
    "## Do-this-yourself: Check if we need to do any normalisation for this case?\n",
    "\n",
    "What are the descriptive statistics look like, see if we need to do anything more?\n",
    "\n",
    "Usually a standardisation is recommended before you apply PCA on a dataset. Here is that link again -- https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "\n",
    "**Note for the above:** In case you wondered, we have done some cleaning and preprocessing on this data to make things easier for you, so normalisation might not be needed or at least won't make a lot of difference to the results. But the real life will always be much more messier.\n",
    "\n",
    "OK, you probably have noticed that there is a lot of data features we have here. Let's start experimenting with the PCA technique and see what it could surface. Maybe it will help us reduce the dimensionality a little but and tell us something about the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = 2\n",
    " \n",
    "pca = PCA(n_components=n_components)\n",
    "df_pca = pca.fit(df.iloc[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "::: callout-caution\n",
    "\n",
    "## What do these numbers mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca_vals = pca.fit_transform(df.iloc[:,1:])\n",
    "df['c1'] = [item[0] for item in df_pca_vals]\n",
    "df['c2'] = [item[1] for item in df_pca_vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.scatterplot(data = df, x = 'c1', y = 'c2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: callout-caution\n",
    "\n",
    "Hmm, that looks problematic. What is going on? When you see patterns like this, you will need to take a step back and look at everything closely.\n",
    "\n",
    "We should look at the component loadings. Though there are going to be over 200 of them. We can put them into a Pandas dataframe and then sort them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'columns' : df.columns[1:102],\n",
    "        'component 1' : df_pca.components_[0],\n",
    "        'component 2' : df_pca.components_[1]}\n",
    "\n",
    "\n",
    "loadings = pd.DataFrame(data)\n",
    "loadings.sort_values(by=['component 1'], ascending=False, key=abs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings.sort_values(by=['component 2'], ascending=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: callout-caution\n",
    "\n",
    "Do you notice anything extraordinary within these variable loadings?\n",
    "\n",
    "Remember, the variable loadings tell us to what extent the information encoded along a principal component axis is determined by a single variable. In other words, how important a variable is. Haev a look and see if anything is too problematic here.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fold variable is messing with our PCA. What does that variable look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fold.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fold'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha! It looks to be some sort of categorical variable. Looking into the dataset more, it is actually a variable used for cross tabling.\n",
    "\n",
    "We should not include this variable in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:, 2:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = 2\n",
    "\n",
    "pca_no_fold = PCA(n_components=n_components)\n",
    "df_pca_no_fold = pca_no_fold.fit(df.iloc[:, 2:-2])\n",
    "\n",
    "df_pca_vals = pca_no_fold.fit_transform(df.iloc[:, 2:-2])\n",
    "\n",
    "df['c1_no_fold'] = [item[0] for item in df_pca_vals]\n",
    "df['c2_no_fold'] = [item[1] for item in df_pca_vals]\n",
    "\n",
    "sns.scatterplot(data = df, x = 'c1_no_fold', y = 'c2_no_fold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do our component loadings look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'columns' : df.iloc[:, 2:-4].columns,\n",
    "        'component 1' : df_pca_no_fold.components_[0],\n",
    "        'component 2' : df_pca_no_fold.components_[1]}\n",
    "\n",
    "\n",
    "loadings = pd.DataFrame(data)\n",
    "loadings_sorted = loadings.sort_values(by=['component 1'], ascending=False)\n",
    "loadings_sorted.iloc[1:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings_sorted = loadings.sort_values(by=['component 2'], ascending=False)\n",
    "loadings_sorted.iloc[1:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting that our first component variables are income related whereas our second component variables are immigration related. If we look at the projections of the model, coloured by Crime, what do we see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = df,\n",
    "                x = 'c1_no_fold',\n",
    "                y = 'c2_no_fold',\n",
    "                hue = 'ViolentCrimesPerPop',\n",
    "                size = 'ViolentCrimesPerPop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "What about clustering using these `income` and `immigration` components?\n",
    "\n",
    "### Defining number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "ks = range(1, 10)\n",
    "inertias = []\n",
    "for k in ks:\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "    model = KMeans(n_clusters=k, n_init = 10)\n",
    "    \n",
    "    # Fit model to samples\n",
    "    model.fit(df[['c1_no_fold', 'c2_no_fold']])\n",
    "    \n",
    "    # Append the inertia to the list of inertias\n",
    "    inertias.append(model.inertia_)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(ks, inertias, '-o', color='black')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(ks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four clusters looks good.\n",
    "\n",
    "## Visualising clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means_4 = KMeans(n_clusters = 3, init = 'random', n_init = 10)\n",
    "k_means_4.fit(df[['c1_no_fold', 'c2_no_fold']])\n",
    "df['Four clusters'] = pd.Series(k_means_4.predict(df[['c1_no_fold', 'c2_no_fold']].values), index = df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = df, x = 'c1_no_fold', y = 'c2_no_fold', hue = 'Four clusters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, might we interpret this plot? The plot is unclear. Let us assume c1 is an income component and c2 is an immigration component. Cluster 0 are places high on income and low on immigration. Cluster 2 are low on income and low on immigration. The interesting group are those high on immigration and relatively low on income.\n",
    "\n",
    "We can include crime on the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| column: screen\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "sns.scatterplot(data = df, x = 'c1_no_fold', y = 'c2_no_fold', hue = 'Four clusters', size = 'ViolentCrimesPerPop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alas, we stop here in this exercise. You could work on this further. There are outliers you might want to remove. Or, at this stage, you might want to look at more components, 3 or 4 and look for some other factors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
