{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Hate crimes\n",
    "\n",
    "In the session 7 (week 8) we discussed data and society: discourses on the social, political and ethical aspects of data science. We also discussed how one can responsibly carry out data science research on social phenomena, what ethical and social frameworks can help us to critically approach data science practices and its effects on society, and about ethical practices for data scientists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets \n",
    "\n",
    "This week we will work with the following datasets:\n",
    "\n",
    "- **[Hate crimes](https://fivethirtyeight.com/features/higher-rates-of-hate-crimes-are-tied-to-income-inequality/)**\n",
    "- **[World Bank Indicators Dataset](https://databank.worldbank.org/source/world-development-indicators#)**\n",
    "- **Office for National Statistics (ONS)**: [Gender Pay Gap](https://www.ons.gov.uk/employmentandlabourmarket/peopleinwork/earningsandworkinghours/datasets/annualsurveyofhoursandearningsashegenderpaygaptables) \n",
    "\n",
    "\n",
    "### Further datasets\n",
    "- **[OECD Poverty gap](https://data.oecd.org/inequality/poverty-rate.htm)**\n",
    "- **Poverty & Equity Data Portal**: From [Organisation for Economic Co-operation and Development (OECD)](https://data.oecd.org/inequality/income-inequality.htm#indicator-chart) or from the [WorldBank](https://povertydata.worldbank.org/poverty/home/)\n",
    "- **NHS**: multiple files. The NHS inequality challenge https://www.nuffieldtrust.org.uk/project/nhs-visual-data-challengeÂ \n",
    "  - [Health state life expectancies by Index of Multiple Deprivation](https://www.ons.gov.uk/peoplepopulationandcommunity/healthandsocialcare/healthinequalities/) (IMD 2015 and IMD 2019): England, all ages\n",
    "multiple publications\n",
    "\n",
    "\n",
    "### Additional Readings\n",
    "- **Indicators - critical reviews**: The Poverty of Statistics and the Statistics of Poverty: https://www.tandfonline.com/doi/full/10.1080/01436590903321844?src=recsys\n",
    "- **Indicators in global health**: arguments: indicators are usually comprehensible to a small group of experts. Why use indicators then? \"Because indicators used in global HIV finance offer openings for engagement to promote accountability (...) some indicators and data truly are better than others, and as they were all created by humans, they all can be deconstructed and remade in other forms\" Davis, S. (2020). The Uncounted: Politics of Data in Global Health, Cambridge. doi:10.1017/9781108649544"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hate Crimes \n",
    "\n",
    "In this notebook we will be using  the [Hate Crimes dataset from Fivethirtyeight](https://github.com/fivethirtyeight/data/tree/master/hate-crimes), which was used in the story [Higher Rates Of Hate Crimes Are Tied To Income Inequality](https://fivethirtyeight.com/features/higher-rates-of-hate-crimes-are-tied-to-income-inequality/).\n",
    "\n",
    "### Variables:\n",
    "| Header | Definition |\n",
    "| --- | --- |\n",
    "| `NAME` | State name |\n",
    "| `median_household_income` | Median household income, 2016 |\n",
    " | `share_unemployed_seasonal` | Share of the population that is unemployed (seasonally adjusted), Sept. 2016 | \n",
    " | `share_population_in_metro_areas` | Share of the population that lives in metropolitan areas, 2015 | \n",
    " | `share_population_with_high_school_degree` | Share of adults 25 and older with a high-school degree, 2009 | \n",
    " | `share_non_citizen` | Share of the population that are not U.S. citizens, 2015 | \n",
    " | `share_white_poverty` | Share of white residents who are living in poverty, 2015 | \n",
    " | `gini_index` | Gini Index, 2015 | \n",
    " | `share_non_white` | Share of the population that is not white, 2015 | \n",
    " | `share_voters_voted_trump` | Share of 2016 U.S. presidential voters who voted for Donald Trump | \n",
    " | `hate_crimes_per_100k_splc` | Hate crimes per 100,000 population, Southern Poverty Law Center, Nov. 9-18, 2016 | \n",
    " | `avg_hatecrimes_per_100k_fbi` | Average annual hate crimes per 100,000 population, FBI, 2010-2015 | \n",
    "\n",
    "::: aside\n",
    "\n",
    "**Gini Index:** measures income inequality. `Gini Index` values can range between `0` and `1`, where 0 indicates perfect equality and everyone has the same income, and 1 indicates perfect inequality. You can read more about Gini Index here: <https://databank.worldbank.org/metadataglossary/world-development-indicators/series/SI.POV.GINI>\n",
    "\n",
    "![Map of income inequality Gini coefficients by country (%). Based on World Bank data for 2019. Source: [Wikipedia](https://en.wikipedia.org/wiki/Gini_coefficient)](media/Gini_Coefficient_of_Wealth_Inequality_source_(2019).png){width=300px}\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: callout-note\n",
    "\n",
    "### Select the IM939 environment before you begin\n",
    "We will again be using Altair and Geopandas this week. If you are using the course's virtual environment, this should be installed for you the first time you set up your environment for the module. Refer to @sec-setup for instructions on how to set up your environment.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "# Remove warnings for this notebook.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('data/hate_Crimes_v2.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A reminder: anything with a pd. prefix comes from pandas (since pd is the alias we have created for the pandas library). This is particulary useful for preventing a module from overwriting inbuilt Python functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve the last ten rows of the df dataframe\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Is df indeed a DataFrame, let's do a quick check\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# What about the data type (Dtype) of the columns in df. We better also be aware of these to help us understand about manipulating them effectively\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values\n",
    "Let's explore the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above shows that we have some missing data for some of the states, as also shown by df.tail(10) earlier on. Let's check again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm, the column 'share_non_citizen' does indeed have some missing data. How about the column NAME (state names). Is that looking ok?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.unique(df.NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There aren't any unexpected values in 'NAME' for the USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# And how many states do we have the data for?\n",
    "count_states = df['NAME'].nunique()\n",
    "print(count_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh...one extra state! Which one is it? And is it a state? Even if you don't get into investigating this immediately, if you realise that this entry is a particularly interesting one down the line in your analysis, you may wish to dig deeper into the context! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping hate crime across the USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We need  the geospatial polygons of the states in America \n",
    "# You can remind yourself about shape polygons from the lab material last week too\n",
    "import geopandas as gpd \n",
    "import altair as alt\n",
    "\n",
    "# Read geospatial data as geospatial data frame -gdf\n",
    "gdf = gpd.read_file('data/gz_2010_us_040_00_500k.json')\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm what type geo_states is...\n",
    "type(gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the previous week, we have got a column called geometry that would allow us to project the data in a 2D map format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the Altair alias (alt) to help us create the map of USA - more technically speaking, \n",
    "# creating a Chart object using Altair with the following properties\n",
    "alt.Chart(gdf, title='US states').mark_geoshape().encode(\n",
    ").properties(\n",
    "    width=500,\n",
    "    height=300\n",
    ").project(\n",
    "    type='albersUsa'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the data\n",
    "# You might recall that the df DataFrame and the geostates GeoDataFrame both have a NAME column\n",
    "# Revisit the merge concept again (from last week and earlier weeks to refresh your memory)\n",
    "\n",
    "geo_states = gdf.merge(df, on='NAME')\n",
    "\n",
    "geo_states.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the merged GeoDataFrame\n",
    "geo_states.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start making our visualisations and see if we can spot any trends or patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first check how hate crimes looked pre-election \n",
    "chart_pre_election = alt.Chart(geo_states, title='PRE-election Hate crime per 100k').mark_geoshape().encode(\n",
    "    color='avg_hatecrimes_per_100k_fbi',\n",
    "    tooltip=['NAME', 'avg_hatecrimes_per_100k_fbi']\n",
    ").properties(\n",
    "    width=500,\n",
    "    height=300\n",
    ").project(\n",
    "    type='albersUsa'\n",
    ")\n",
    "chart_pre_election"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, Altair has chosen a color for each US state based on the range of values in the `avg_hatecrimes_per_100k_fbi` column. We have also created a tooltip, so hover over the map and check the crime rates. Which ones are particularly high? average? low? \n",
    "\n",
    "Also, is there a dark spot between Virginia and Maryland - Did you notice that? What's happening there? Remember: Context always matters for data analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, what about the post election status?\n",
    "chart_post_election = alt.Chart(geo_states, title='POST-election Hate crime per 100k').mark_geoshape().encode(\n",
    "    color='hate_crimes_per_100k_splc',\n",
    "    tooltip=['NAME', 'hate_crimes_per_100k_splc']\n",
    ").properties(\n",
    "    width=500,\n",
    "    height=300\n",
    ").project(\n",
    "    type='albersUsa'\n",
    ")\n",
    "chart_post_election"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| column: page\n",
    "\n",
    "# Perhaps we can arrange the maps side-by-side to compare better?\n",
    "pre_and_post_map = chart_pre_election | chart_post_election\n",
    "pre_and_post_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: callout-note\n",
    "\n",
    "Oh, what's happening here? We better investigate:\n",
    "\n",
    "1. Identify why the maps (particularly) one of them looks so different now. Go back to the original maps to check. Go back to the description of the variables to check. Also, visit the source of the article.\n",
    "\n",
    "2. Once you have identified the issue, can you find a way to address the issue?\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| column: page\n",
    "import seaborn as sns\n",
    "sns.pairplot(data = df.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot may be hard to read without squinting our eyes (and take a bit longer to run on some devices), but it's definitely worth a closer look if you are able to. Check the histograms along the diagonal - what do they show about the distribution of each variable. For example, what does the `gini_index` distribution tell us? With respect to the scatter plots, some are more random while others show likely positive or negative correlations. You may wish to investigate what's happening! And, you might remember (as we also discussed in the video recordings this week), correlation != causation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the income range in the country\n",
    "df.boxplot(column=['median_household_income'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the average hatecrimes based on FBI data next (also, average over what? check the Variables description again to remind you if need be)\n",
    "df.boxplot(column=['avg_hatecrimes_per_100k_fbi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may want to drop some states (remove them). See more [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html). \n",
    "\n",
    "Let us drop Hawaii (which is one of the states outside mainland USA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find out the index value of the state in the DataFrame df\n",
    "df[df.NAME == 'Hawaii']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at a summary of numeric columns prior to dropping Hawaii\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now drop Hawaii\n",
    "df = df.drop(df.index[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now check again for the statistical summary\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be some changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's dig in deeper to the correlation between median household income and hatecrimes based on FBI data\n",
    "df.plot(x = 'avg_hatecrimes_per_100k_fbi', y = 'median_household_income', kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the relationship between median household income and hatecrimes based on SPLC data\n",
    "df.plot(x = 'hate_crimes_per_100k_splc', y = 'median_household_income', kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm, there doesn't appear to be a strong (linear!) correlation, but surely there is a cluster and some outliers! That's our cue - let's find out which states might be outliers by using the standard deviation function 'std'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.hate_crimes_per_100k_splc > (np.std(df.hate_crimes_per_100k_splc) * 2.5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we discussed about 'context' earlier when we got 51 states? If your investigation paid off there, you can make better sense of the outliers here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to make the outliers more obvious\n",
    "import matplotlib.pyplot as plt\n",
    "outliers_df = df[df.hate_crimes_per_100k_splc > (np.std(df.hate_crimes_per_100k_splc) * 2.5)]\n",
    "df.plot(x = 'hate_crimes_per_100k_splc', y = 'median_household_income', kind='scatter')\n",
    "plt.scatter(outliers_df.hate_crimes_per_100k_splc, outliers_df.median_household_income ,c='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a pivot table to focus on specific columns of interest\n",
    "df_pivot = df.pivot_table(index=['NAME'], values=['hate_crimes_per_100k_splc', 'avg_hatecrimes_per_100k_fbi', 'median_household_income'])\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the pivot table seems sorted by state names, let's sort by FBI hate crime data instead \n",
    "df_pivot.sort_values(by=['avg_hatecrimes_per_100k_fbi'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's standardise our data before we attempt further modelling using the data\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "# Get the column names first\n",
    "df_selected_std = df[['median_household_income','share_unemployed_seasonal', 'share_population_in_metro_areas'\n",
    "               , 'share_population_with_high_school_degree', 'share_non_citizen', 'share_white_poverty', 'gini_index'\n",
    "               , 'share_non_white', 'share_voters_voted_trump', 'hate_crimes_per_100k_splc', 'avg_hatecrimes_per_100k_fbi']]\n",
    "names = df_selected_std.columns\n",
    "\n",
    "# Create the Scaler object for standardising the data\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# Fit our data on the scaler object\n",
    "df2 = scaler.fit_transform(df_selected_std)\n",
    "\n",
    "# check what type is df2\n",
    "type(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's convert the numpy array into a DataFrame before further processing\n",
    "df2 = pd.DataFrame(df2, columns=names)\n",
    "df2.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| column: page-inset-right\n",
    "\n",
    "# Now that our data has been standardised, let's look at the distribution across all columns\n",
    "ax = sns.boxplot(data=df2, orient=\"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "# Let's create a correlation matrix by computing the pairwise correlation of numerical columns, rounding correlation values to two places\n",
    "corrMatrix = df2.corr(numeric_only=True).round(2)\n",
    "print (corrMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: callout-warning\n",
    "\n",
    "### Time for reflection:\n",
    "\n",
    "Look at the positive and negative correlation values above. What do they suggest and how strong, weak or moderate is the correlation. \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a heatmap to visualse the pairwise correlations for better understanding\n",
    "corrMatrix = df2.corr(numeric_only=True).round(1)  #Rounding to (1) so it's easier to read given number of variables\n",
    "sns.heatmap(corrMatrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now perform a linear regression on our data\n",
    "# Try the commented code after you run this first\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "# identify the independent variables\n",
    "x = df2[['median_household_income', 'share_population_with_high_school_degree', 'share_voters_voted_trump']]\n",
    "# identify the dependent variable\n",
    "y = df2[['avg_hatecrimes_per_100k_fbi']]\n",
    "# What if we change the data (y variable)\n",
    "#y = df2[['hate_crimes_per_100k_splc']]\n",
    "\n",
    "lin_model = LinearRegression(fit_intercept = True) \n",
    "lin_model.fit(x, y)\n",
    "\n",
    "print(\"Coefficients:\", lin_model.coef_)\n",
    "print (\"Intercept:\", lin_model.intercept_)\n",
    "\n",
    "# Generate predictions from our linear regression model, and check the MSE, Rsquared and Variance measures to assess performance\n",
    "y_hat = lin_model.predict(x)\n",
    "print (\"MSE:\", metrics.mean_squared_error(y, y_hat))\n",
    "print (\"R^2:\", metrics.r2_score(y, y_hat))\n",
    "print (\"var:\", y.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do these values suggest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: callout-note\n",
    "\n",
    "Remember the earlier note about the maps looking different? Were you able to identify what's going there? \n",
    "\n",
    "Let's revisit that part again\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maps did not share comparable scales. The first map was showing the annual crime rate per 100k residents, while the second map was showing the total incident numbers per 100k resident only for the 10-days following the 2016 election. How can we fix this discrepency? \n",
    "\n",
    "::: callout-warning\n",
    "\n",
    "### Your Turn\n",
    "\n",
    "Tweak the `??`s below to visualise changes\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| error: true\n",
    "\n",
    "# We can generete two new \"features\" and add them to the DataFrame df \n",
    "df['hate_crimes_per_100k_splc_perday'] = df['hate_crimes_per_100k_splc'] / ??\n",
    "# the 'avg_hatecrimes_per_100k_fbi' column is an annual incidence average between 2010- 15, so each data value is the number of incidences (per 100k residents) in an average year. \n",
    "df['avg_hatecrimes_per_100k_fbi_perday'] = df['avg_hatecrimes_per_100k_fbi'] / ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| error: true\n",
    "\n",
    "# Update geo_states\n",
    "geo_states = geo_states.merge(df, on='????')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot again\n",
    "# First the PRE election map\n",
    "pre_election_map = alt.Chart(geo_states, title='PRE-election Hate crime per 100k per day').mark_geoshape().encode(\n",
    "    alt.Color('avg_hatecrimes_per_100k_fbi_perday', scale=alt.Scale(domain=[0, 0.15])),\n",
    "    tooltip=['NAME', 'avg_hatecrimes_per_100k_fbi_perday']\n",
    ").properties(\n",
    "    width=500,\n",
    "    height=300\n",
    ").project(\n",
    "    type='albersUsa'\n",
    ")\n",
    "\n",
    "post_election_map = alt.Chart(geo_states, title='POST-election Hate crime per 100k per day').mark_geoshape().encode(\n",
    "    alt.Color('hate_crimes_per_100k_splc_perday', scale=alt.Scale(domain=[0, 0.15])),\n",
    "    tooltip=['NAME', 'hate_crimes_per_100k_splc_perday']\n",
    ").properties(\n",
    "    width=500,\n",
    "    height=300\n",
    ").project(\n",
    "    type='albersUsa'\n",
    ")\n",
    "\n",
    "new_combined_map = pre_election_map | post_election_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is that now?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
