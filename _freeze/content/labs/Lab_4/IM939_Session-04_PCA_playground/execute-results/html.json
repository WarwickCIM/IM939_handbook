{
  "hash": "cda416cc702a49cde1c4dc12e41f7cc7",
  "result": {
    "markdown": "# Working with Dimension Reduction Methods\n\n*A live document to complement the lectures in Week-04*\n\nLet's work with the very simple but highly popular Iris Dataset\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n%matplotlib inline  \n\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\ncolumn_names = [\"Sepal Length\", \"Sepal Width\", \"Petal Length\", \"Petal Width\"]\n```\n:::\n\n\n## Exploratory analysis\n\nLet's do a bit of exploratory analysis to check the characteristics of the features and look for redundancies\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Plot the training points\n\ncolumnIndexToUseX = 2\ncolumnIndexToUseY = 3\n\nx_min, x_max = X[:, columnIndexToUseX].min() - .5, X[:, columnIndexToUseX].max() + .5\ny_min, y_max = X[:, columnIndexToUseY].min() - .5, X[:, columnIndexToUseY].max() + .5\n\nplt.figure(figsize=(8, 8))\nplt.scatter(X[:, columnIndexToUseX], X[:, columnIndexToUseY], c=y, cmap=plt.cm.Set1,\n            edgecolor='k' )\nplt.xlabel(column_names[columnIndexToUseX])\nplt.ylabel(column_names[columnIndexToUseY])\n\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.xticks(())\nplt.yticks(())\n\nplt.show()\n\n\n```\n\n::: {.cell-output .cell-output-display}\n![](IM939_Session-04_PCA_playground_files/figure-html/cell-3-output-1.png){width=633 height=629}\n:::\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# It would be good to have a look at the pairwise correlations. \n# This might help one to identify some redundant variables at least as long as pairs of dimensions are considered.\nimport seaborn as sns\nsns.set(style=\"ticks\")\n\n## we are cheating here and using a copy of the Iris data as made available as a Pandas DataFrame within the Seaborn package\ndf = sns.load_dataset(\"iris\")\nsns.pairplot(df, hue=\"species\")\n```\n\n::: {.cell-output .cell-output-display}\n![](IM939_Session-04_PCA_playground_files/figure-html/cell-4-output-1.png){width=1068 height=943}\n:::\n:::\n\n\n## Principal Component Analysis \n\nHere we start working with PCA to find linear mappings of the original dimension space\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.decomposition import PCA\n\n\n# I am stating how many components I would like my final projection space to have\nn_components = 2\n\n# This line creates a PCA computation object\npca = PCA(n_components=n_components)\n\n# And this line \"projects the data to the newly generated space\"\nX_pca = pca.fit_transform(X)\n\n# I set the colours that I can use to colour the different classes of iris flowers with. I may not always have this class information but in this instance, we have this information so we can use it for colouring.\ncolors = ['navy', 'turquoise', 'darkorange']\n\nplt.figure(figsize=(8, 8))\nfor color, i, target_name in zip(colors, [0, 1, 2], iris.target_names):\n    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1],\n                color=color, lw=2, label=target_name)\n\nplt.title(\"PCA of iris dataset\")\n\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](IM939_Session-04_PCA_playground_files/figure-html/cell-5-output-1.png){width=656 height=657}\n:::\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nprint(column_names)\npca.components_\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\narray([[ 0.36138659, -0.08452251,  0.85667061,  0.3582892 ],\n       [ 0.65658877,  0.73016143, -0.17337266, -0.07548102]])\n```\n:::\n:::\n\n\n\n### A bit on interpretation\n\nOK, what are we looking at here ...\n\nThe array of numbers we are looking at above are actually the coefficients of the first two principal components, so these \"are\" the components.\n\nThe first line consists of the \"loadings\" of the variables for the first component, this means: \n\n**Sepal Length**'s loading in principal component 1 (PC1) is 0.36138659, **Sepal Width**'s is -0.08452251, **Petal Length**'s is 0.85667061 and **Petal Width**'s is 0.3582892. And the second line of numbers correspond to the loadings of the variables within the \"second\" principal component.\n\nBut then, what do these numbers tell us? One analysis we can do here is to look at the \"magnitude\" of these numbers. If you look at the four numbers, we notice that the largest in magnitude is for **Petal Length** (0.85667061) which indicates that the most **important** dimension for this component is **Petal Length**. As far as PCA is concerned, the notion of importance is how much of the \"variance\" of the data is explained by a principal component, so Petal Length is the feature that has the most influence on the variation within the data along the first component. And the first component is the computed axis that explains most of the variance in the data (i.e., has the highest eigenvalue in the computations -- see the slides). We can do a similar analysis for the second component, and notice that **Sepal Widht** is the most key variable in this component but I **Sepal Length** is also important as seen from its higher loading value.\n\nIf we look at these four variables, the only variable that didn't get a high loading is **Petal Width**, this is a sign that this feature might not carry similar levels of variation as the others.\n\n#### How about the \"projection\"?\n\nWhat we look at above in the scatterplot, what we are seeing is all the observations \"projected\" onto this new space defined by the two principal components. This is why dimension reduction methods are often referred to as \"projection\" or \"embedding\" methods. The idea is that the new components give you a new \"space\" where you can observe your data points. This is what the `X_pca = pca.fit_transform(X)` line is doing in the code above.\n\n## LDA\nLinear Discriminant Analysis is a method which considers the class labels as well as the data during the projection\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nlda = LinearDiscriminantAnalysis(n_components=2)\nX_lda = lda.fit(X, y).transform(X)\n# Notice here that we are also feeding in the class labels\n# Since we have three classes but LDA is a binary method, \n# it will automatically run 1 vs. 2 other classes for all the combinations\n\nplt.figure(figsize=(8, 8))\nfor color, i, target_name in zip(colors, [0, 1, 2], iris.target_names):\n    plt.scatter(X_lda[y == i, 0], X_lda[y == i, 1],\n                color=color, lw=2, label=target_name)\n\nplt.title(\"LDA of iris dataset\")\n\n\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-display}\n![](IM939_Session-04_PCA_playground_files/figure-html/cell-7-output-1.png){width=644 height=657}\n:::\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nprint(column_names)\nlda.coef_\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\narray([[  6.31475846,  12.13931718, -16.94642465, -20.77005459],\n       [ -1.53119919,  -4.37604348,   4.69566531,   3.06258539],\n       [ -4.78355927,  -7.7632737 ,  12.25075935,  17.7074692 ]])\n```\n:::\n:::\n\n\n## A regularised method -- SparcePCA\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# A regularised version of PCA -- SparsePCA -- this is one of those Embedded methods that we talked about\nfrom sklearn.decomposition import SparsePCA\n\ns_pca = SparsePCA(n_components=n_components)\nSparse_PCA_embdedded = s_pca.fit_transform(X)\n\ncolors = ['navy', 'turquoise', 'darkorange']\n\nplt.figure(figsize=(8, 8))\nfor color, i, target_name in zip(colors, [0, 1, 2], iris.target_names):\n    plt.scatter(Sparse_PCA_embdedded[y == i, 0], Sparse_PCA_embdedded[y == i, 1],\n                color=color, lw=2, label=target_name)\n\nplt.title(\"SparcePCA of iris dataset\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](IM939_Session-04_PCA_playground_files/figure-html/cell-9-output-1.png){width=656 height=657}\n:::\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n# Let's see if we observe differences\nprint(column_names)\ns_pca.components_\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\narray([[ 0.34155543, -0.04951592,  0.87443905,  0.34094634],\n       [ 0.68542067,  0.72814731,  0.        ,  0.        ]])\n```\n:::\n:::\n\n\n## A non-linear method called tSNE\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nfrom sklearn.manifold import TSNE\n\nTSNE_Model = TSNE(n_components=n_components)\ntSNE_embdedded = TSNE_Model.fit_transform(X)\n\nplt.figure(figsize=(8, 8))\nfor color, i, target_name in zip(colors, [0, 1, 2], iris.target_names):\n    plt.scatter(tSNE_embdedded[y == i, 0], tSNE_embdedded[y == i, 1],\n                color=color, lw=2, label=target_name)\n\nplt.title(\"tSNE of iris dataset\")\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](IM939_Session-04_PCA_playground_files/figure-html/cell-11-output-1.png){width=644 height=657}\n:::\n:::\n\n\n### How is tSNE results different?\nNotice how different the results came up with tSNE. The clusters are much more visible and better seperated. However, the spread within the clusters are mostly gone. And now the axes are not a linear combination of the original features any more.\n\n\n",
    "supporting": [
      "IM939_Session-04_PCA_playground_files"
    ],
    "filters": [],
    "includes": {}
  }
}