{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IM939 - Lab 3 - Regression\n",
    "\n",
    "Our aim is to introduce simple linear regression in Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit Learn\n",
    "\n",
    "We are going to use Scikit Learn library. More information about the librry you can find here:\n",
    "https://www.tutorialspoint.com/scikit_learn/scikit_learn_linear_regression.htm\n",
    "\n",
    "## Sea Ice Dataset\n",
    "\n",
    "The script below is the based on the Sea Ice dataset (Igual, Segui 2017).\n",
    "You can find the data here: ftp://sidads.colorado.edu/DATASETS/NOAA/\n",
    "or read more about the project here: https://nsidc.org/data/seaice_index\n",
    "\n",
    "This notebook will walk you through an example of a simple linear regression and the other notebook \"IM939 Lab 3 - Linear Regression Exercise.ipynb\" will include the example of a multiple linear regression, too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple and Multiple Linear Regression\n",
    "\n",
    "In the **linear model** the response $\\textbf{y}$ depends linearly from the covariates $\\textbf{x}_i$.\n",
    "\n",
    "In the **simple** linear regression, with a single variable, we described the relationship between the predictor and the response with a straight line. The general linear model:\n",
    "$$ \\textbf{y}  =  a_0+ a_1 \\textbf{x}_1 $$\n",
    "\n",
    "The parameter $a_0$ is called the *constant* term or the *intercept*.\n",
    "\n",
    "In the case of **multiple** linear regression we extend this idea by fitting a m-dimensional hyperplane to our m predictors.\n",
    "\n",
    "$$ \\textbf{y}  =  a_1 \\textbf{x}_1  + \\dots + a_m \\textbf{x}_{m} $$\n",
    "\n",
    "The $a_i$ are termed the *parameters* of the model or the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares\n",
    "\n",
    "Ordinary Least Squares (OLS) is the simplest and most common **estimator** in which the coefficients $a$'s \n",
    "of the simple linear regression: $\\textbf{y} = a_0+a_1 \\textbf{x}$, \n",
    "are chosen to minimize the **square of the distance between the predicted values and the actual values**. \n",
    "\n",
    "$$ ||a_0 + a_1 \\textbf{x} -  \\textbf{y} ||^2_2 = \\sum_{j=1}^n (a_0+a_1 x_{j} -  y_j )^2,$$ \n",
    "\n",
    "This expression is often called **sum of squared errors of prediction (SSE)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case study: Climate Change and Sea Ice Extent\n",
    "\n",
    "The question: Has there been a decrease in the amount of ice in the last years?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data\n",
    "\n",
    "There are five steps. First, let's upload the data that is already in the folder: SeaIce.txt. It is a text file.\n",
    "\n",
    "\n",
    "The file *'SeaIce.txt'* is a ``Tab`` separated file containing:\n",
    "+ Year:\t4-digit year\n",
    "+ mo:\t1- or 2-digit month\n",
    "+ data_type:\tInput data set (Goddard/NRTSI-G)\n",
    "+ region:\tHemisphere that this data covers (N: Northern; S: Southern)\n",
    "+ extent:\tSea ice extent in millions of square km\n",
    "+ area:\tSea ice area in millions of square km\n",
    "\n",
    "Once we upload the data, we can create a *DataFrame* using Pandas. A reminder what is DataFrame: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ice = pd.read_csv('SeaIce.txt', \n",
    "                  delim_whitespace=True)\n",
    "\n",
    "print('shape:', ice.shape) #this returns number of rows and columns in a dataset\n",
    "ice.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the mean for that interval of time (1981 through 2010), before data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ice.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did we receive a negative mean...?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualisation to explore data\n",
    "\n",
    "Do you remember Seaborn? \n",
    "We will use lmplot() function from Seaborn to explore linear relationship of different forms, e.g. relationship between the month of the year (variable) and extent (responses):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "x = ice.year\n",
    "y = ice.extent\n",
    "plt.scatter(x, y, color = 'red')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Extent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We detect some outlier or missing data.\n",
    "we are going to use function np.unique and find the unique elements of an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?np.unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Different values in data_type field:', np.unique(ice.data_type.values))   # there is a -9999 value!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what type of data we have, other than the ones printed above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (ice[(ice.data_type != 'Goddard')\n",
    "          & (ice.data_type != 'NRTSI-G')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning: we checked all the values and notice -9999 values in data_type field which should contain 'Goddard' or 'NRTSI-G' (some type of input dataset).\n",
    "So we clean them by removing these instances  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can easily clean the data now:\n",
    "ice2 = ice[ice.data_type != '-9999']\n",
    "print ('shape:', ice2.shape)\n",
    "# And repeat the plot\n",
    "x = ice2.year\n",
    "y = ice2.extent\n",
    "plt.scatter(x, y, color = 'red')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Extent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(\"mo\", \"extent\", ice2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above you can see ice extent data by month.\n",
    "You can see a monthly fluctuation of the sea ice extent, as would be expected for the different seasons of the year. In order to run regression, and avoid this fluctuation we can normalize data. This will let us see the evolution of the extent over the years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lmplot() function from the Seaborn module is intended for exploring linear relationships of different forms in multidimensional datesets. Input data must be in a Pandas DataFrame. To plot them, we provide the predictor and response variable names along with the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(\"mo\", \"extent\", ice2, height = 5.2, aspect = 2);\n",
    "plt.savefig(\"IceExtentCleanedByMonth.png\", dpi = 300, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean for each month.\n",
    "grouped = ice2.groupby('mo')\n",
    "month_means = grouped.extent.mean()\n",
    "month_variances = grouped.extent.var()\n",
    "print ('Means:', month_means)\n",
    "print ('Variances:',month_variances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To capture variation per month, we can compute mean for the i-th interval of time (using 1979-2014) and subtract it from the set of extent values for that month . This can be converted to a relative pecentage difference by dividing it by the total avareage (1979-2014) and multiplying by 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data normalization\n",
    "for i in range(12):\n",
    "    ice2.extent[ice2.mo == i+1] = 100*(ice2.extent[ice2.mo == i+1] - month_means[i+1])/month_means.mean()\n",
    "    \n",
    "sns.lmplot(\"mo\", \"extent\", ice2, height = 5.2, aspect = 2);\n",
    "plt.savefig(\"IceExtentNormalizedByMonth.png\", dpi = 300, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('mean:', ice2.extent.mean())\n",
    "print ('var:', ice2.extent.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider the entire year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(\"year\", \"extent\", ice2, height = 5.2, aspect = 2);\n",
    "plt.savefig(\"IceExtentAllMonthsByYearlmplot.png\", dpi = 300, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson's correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate Pearson's correlation coefficient and the p-value for testing non-correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "scipy.stats.pearsonr(ice2.year.values, ice2.extent.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple OLS\n",
    "\n",
    "We can also compute the trend as a simple linear regression (OLS) and quantitatively evaluate it.\n",
    "\n",
    "For that we use using **Scikit-learn**, library that provides a variety of both supervised and unsupervised machine learning techniques.\n",
    "Scikit-learn provides an object-oriented interface centered around the concept of an Estimator. \n",
    "The <code>Estimator.fit</code> method sets the state of the estimator based on the training data. Usually, the data is comprised of a two-dimensional numpy array $X$ of shape <code>(n_samples, n_predictors)</code> that holds the so-called feature matrix and a one-dimensional numpy array $\\textbf{y}$ that holds the responses. Some estimators allow the user to control the fitting behavior. \n",
    "For example, the <code>sklearn.linear_model.LinearRegression</code> estimator allows the user to specify whether or not to fit an intercept term. This is done by setting the corresponding constructor arguments of the estimator object.\n",
    "During the fitting process, the state of the estimator is stored in instance attributes that have a trailing underscore ('_'). For example, the coefficients of a LinearRegression estimator are stored in the attribute coef_.\n",
    "\n",
    "Estimators that can generate predictions provide a ``Estimator.predict`` method. \n",
    "In the case of regression, ``Estimator.predict`` will return the predicted regression values, $\\hat{\\textbf{y}}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "est = LinearRegression(fit_intercept = True)\n",
    "\n",
    "x = ice2[['year']]\n",
    "y = ice2[['extent']]\n",
    "\n",
    "est.fit(x, y)\n",
    "\n",
    "print(\"Coefficients:\", est.coef_)\n",
    "print (\"Intercept:\", est.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the model fitting by computing the mean squared error ($MSE$) and the coefficient of determination ($R^2$) of the model.\n",
    "The coefficient $R^2$ is defined as $(1 - \\textbf{u}/\\textbf{v})$, where $\\textbf{u}$ is the residual sum of squares $\\sum (\\textbf{y} - \\hat{\\textbf{y}})^2$ and $\\textbf{v}$ is the regression sum of squares $\\sum (\\textbf{y} - \\bar{\\textbf{y}})^2$, where $\\bar{\\textbf{y}}$ is the mean.\n",
    "The best possible score for $R^2$ is 1.0: lower values are worse.\n",
    "These measures can provide a quantitative answer to the question we are facing: Is there a negative trend in the evolution of sea ice extent over recent years?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Analysis for all months together.\n",
    "x = ice2[['year']]\n",
    "y = ice2[['extent']]\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)\n",
    "y_hat = model.predict(x)\n",
    "plt.plot(x, y,'o', alpha = 0.5)\n",
    "plt.plot(x, y_hat, 'r', alpha = 0.5)\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('extent (All months)')\n",
    "print (\"MSE:\", metrics.mean_squared_error(y_hat, y))\n",
    "print (\"R^2:\", metrics.r2_score(y_hat, y))\n",
    "print (\"var:\", y.var())\n",
    "plt.savefig(\"IceExtentLinearRegressionAllMonthsByYearPrediction.png\", dpi = 300, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude that the data show a long-term negative trend in recent years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis for a particular month.\n",
    "#For January\n",
    "jan = ice2[ice2.mo == 1];\n",
    "\n",
    "x = jan[['year']]\n",
    "y = jan[['extent']]\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)\n",
    "\n",
    "y_hat = model.predict(x)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, y,'-o', alpha = 0.5)\n",
    "plt.plot(x, y_hat, 'r', alpha = 0.5)\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('extent (January)')\n",
    "\n",
    "print (\"MSE:\", metrics.mean_squared_error(y_hat, y))\n",
    "print (\"R^2:\", metrics.r2_score(y_hat, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also estimate the extent value for 2025. For that we use the function predict of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(2025) \n",
    "y_hat = model.predict(X.reshape(-1, 1))\n",
    "j = 1 # January\n",
    "# Original value (before normalization)\n",
    "y_hat = (y_hat*month_means.mean()/100) + month_means[j]\n",
    "print (\"Prediction of extent for January 2025 (in millions of square km):\", y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction of extent for January 2025 (in millions of square km): [ 13.14449923]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
