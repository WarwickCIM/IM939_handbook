{
  "hash": "87548d482fd5e0caffd132d4f8132e0b",
  "result": {
    "markdown": "# Lab: Clustering and Ground Truth\n\n## Workflow\n\nOur labs have focused on data analysis. The goal here is to try and understand informative patterns in our data. These patterns allow us to answer questions.\n\nTo do this we:\n\n1. Read data into Python.\n2. Look at our data. \n3. Wrangling our data. Often exploring raw data and dealing with missing values (imputation techniques), transformaing, normalising, standardising, outliers or reshaping.\n4. Carry out a series of analysis to better understand our data via clustering, regressions analysis, dimension reduction, and many other techniques.\n5. Reflect on what the patterns in our data can tell us.\n\nThese are not mutually exclusive processes and are not exhaustive. One may review our data after cleaning, load in more data, carry out additional analysis and/or fit multiple models, tweak data summaries or adopt new techniques. Reflecting on the patterns are in our data can give way to additional analysis and processing.\n\n## So far\n\nA quick reminder, our toolkit comprises of:\n\n* [Pandas](https://pandas.pydata.org/) - table like data structures. Packed with methods for summarising and manipulating data. [Documentation](https://pandas.pydata.org/docs/user_guide/dsintro.html#dsintro). [Cheat sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf).\n* [Seaborn](https://seaborn.pydata.org/) - helping us create statistical data visualisations. [Tutorials](https://seaborn.pydata.org/tutorial.html).\n* [Scikit-learn](https://scikit-learn.org/stable/index.html) - An accessible collection of functions and object for analysing data. These analysis include dimension reduction, clustering, regressions and evaluating our models. [Examples](https://scikit-learn.org/stable/auto_examples/index.html).\n\nThese tools comprise some of the core Python data science stack and allow us to tackle many of the elements from each week.\n\nWeek 2\nTidy data, data types, wrangling data, imputation ([missing data](https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html)), transformations.\n\nWeek 3\n[Descriptive statistics](https://pandas.pydata.org/pandas-docs/stable/user_guide/computation.html), distributions, models (e.g., [regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)).\n\nWeek 4\nFeature selection, dimension reduction (e.g., [Principle Component Analysis](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html), [Multidimensional scaling](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html?highlight=mds#sklearn.manifold.MDS), [Linear Discriminant Analysis](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#:~:text=sklearn.discriminant_analysis.LinearDiscriminantAnalysis%C2%B6.%20Linear%20Discriminant%20Analysis.%20A%20classifier%20with%20a,that%20all%20classes%20share%20the%20same%20covariance%20matrix.), [t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html?highlight=tsne#sklearn.manifold.TSNE), Correspondance Analysis), [clustering](https://scikit-learn.org/stable/modules/clustering.html) (e.g., [Hierarchical Clustering](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering), Partioning-based clustering such as [K-means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#:~:text=sklearn.cluster%20.KMeans%20%C2%B6%20%20,fit%20%28X%203%20more%20rows%20)).\n\nWe have also encountered two dataset sources.\n\n* [sklearn example datasets](https://scikit-learn.org/stable/datasets/index.html#:~:text=The%20sklearn.datasets%20package%20embeds%20some%20small%20toy%20datasets,on%20data%20that%20comes%20from%20the%20%E2%80%98real%20world%E2%80%99.)\n* [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets.php)\n\nYou can learn a lot by picking a dataset, choosing a possible research question and carrying a series of analysis. I encourage you to do so outside of the session. It certainly forces one to read the documentation and explore the wonderful possabilities.\n\n## This week\n\nTrying to understand patterns in data often requires us to fit multiple models. We need to consider how well a given model (a kmeans cluster, a linear regression, dimension reduction, etc.) performs.\n\nSpecifically, we will look at:\n\n1. Comparing clusters to the 'ground truth' - the wine dataset\n2. Cross validation of linear regression - the crime dataset\n3. Investigating multidimensional scaling - the london borough dataset\n4. Visualising the overlap in clustering results\n\n\n### Clustering and ground truth\n\nLoad in the wine dataset. Details of the dataset are [here]https://archive.ics.uci.edu/ml/datasets/wine).\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\n\ndf = pd.read_csv('data/wine.csv')\n```\n:::\n\n\nLook at our data.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Class label</th>\n      <th>Alcohol</th>\n      <th>Malic acid</th>\n      <th>Ash</th>\n      <th>Alcalinity of ash</th>\n      <th>Magnesium</th>\n      <th>Total phenols</th>\n      <th>Flavanoids</th>\n      <th>Nonflavanoid phenols</th>\n      <th>Proanthocyanins</th>\n      <th>Color intensity</th>\n      <th>Hue</th>\n      <th>OD280/OD315 of diluted wines</th>\n      <th>Proline</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>14.23</td>\n      <td>1.71</td>\n      <td>2.43</td>\n      <td>15.6</td>\n      <td>127</td>\n      <td>2.80</td>\n      <td>3.06</td>\n      <td>0.28</td>\n      <td>2.29</td>\n      <td>5.64</td>\n      <td>1.04</td>\n      <td>3.92</td>\n      <td>1065</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>13.20</td>\n      <td>1.78</td>\n      <td>2.14</td>\n      <td>11.2</td>\n      <td>100</td>\n      <td>2.65</td>\n      <td>2.76</td>\n      <td>0.26</td>\n      <td>1.28</td>\n      <td>4.38</td>\n      <td>1.05</td>\n      <td>3.40</td>\n      <td>1050</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>13.16</td>\n      <td>2.36</td>\n      <td>2.67</td>\n      <td>18.6</td>\n      <td>101</td>\n      <td>2.80</td>\n      <td>3.24</td>\n      <td>0.30</td>\n      <td>2.81</td>\n      <td>5.68</td>\n      <td>1.03</td>\n      <td>3.17</td>\n      <td>1185</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>14.37</td>\n      <td>1.95</td>\n      <td>2.50</td>\n      <td>16.8</td>\n      <td>113</td>\n      <td>3.85</td>\n      <td>3.49</td>\n      <td>0.24</td>\n      <td>2.18</td>\n      <td>7.80</td>\n      <td>0.86</td>\n      <td>3.45</td>\n      <td>1480</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>13.24</td>\n      <td>2.59</td>\n      <td>2.87</td>\n      <td>21.0</td>\n      <td>118</td>\n      <td>2.80</td>\n      <td>2.69</td>\n      <td>0.39</td>\n      <td>1.82</td>\n      <td>4.32</td>\n      <td>1.04</td>\n      <td>2.93</td>\n      <td>735</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThere is a column called Class label that gives us the ground truth. The wines come from three different cultivars. Knowing the actual grouping helps us to identify how well our methods can capture this ground truth.\n\nFollowing our process above, we should first get a sense of our data.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndf.describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Class label</th>\n      <th>Alcohol</th>\n      <th>Malic acid</th>\n      <th>Ash</th>\n      <th>Alcalinity of ash</th>\n      <th>Magnesium</th>\n      <th>Total phenols</th>\n      <th>Flavanoids</th>\n      <th>Nonflavanoid phenols</th>\n      <th>Proanthocyanins</th>\n      <th>Color intensity</th>\n      <th>Hue</th>\n      <th>OD280/OD315 of diluted wines</th>\n      <th>Proline</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1.938202</td>\n      <td>13.000618</td>\n      <td>2.336348</td>\n      <td>2.366517</td>\n      <td>19.494944</td>\n      <td>99.741573</td>\n      <td>2.295112</td>\n      <td>2.029270</td>\n      <td>0.361854</td>\n      <td>1.590899</td>\n      <td>5.058090</td>\n      <td>0.957449</td>\n      <td>2.611685</td>\n      <td>746.893258</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.775035</td>\n      <td>0.811827</td>\n      <td>1.117146</td>\n      <td>0.274344</td>\n      <td>3.339564</td>\n      <td>14.282484</td>\n      <td>0.625851</td>\n      <td>0.998859</td>\n      <td>0.124453</td>\n      <td>0.572359</td>\n      <td>2.318286</td>\n      <td>0.228572</td>\n      <td>0.709990</td>\n      <td>314.907474</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>11.030000</td>\n      <td>0.740000</td>\n      <td>1.360000</td>\n      <td>10.600000</td>\n      <td>70.000000</td>\n      <td>0.980000</td>\n      <td>0.340000</td>\n      <td>0.130000</td>\n      <td>0.410000</td>\n      <td>1.280000</td>\n      <td>0.480000</td>\n      <td>1.270000</td>\n      <td>278.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1.000000</td>\n      <td>12.362500</td>\n      <td>1.602500</td>\n      <td>2.210000</td>\n      <td>17.200000</td>\n      <td>88.000000</td>\n      <td>1.742500</td>\n      <td>1.205000</td>\n      <td>0.270000</td>\n      <td>1.250000</td>\n      <td>3.220000</td>\n      <td>0.782500</td>\n      <td>1.937500</td>\n      <td>500.500000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>2.000000</td>\n      <td>13.050000</td>\n      <td>1.865000</td>\n      <td>2.360000</td>\n      <td>19.500000</td>\n      <td>98.000000</td>\n      <td>2.355000</td>\n      <td>2.135000</td>\n      <td>0.340000</td>\n      <td>1.555000</td>\n      <td>4.690000</td>\n      <td>0.965000</td>\n      <td>2.780000</td>\n      <td>673.500000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>3.000000</td>\n      <td>13.677500</td>\n      <td>3.082500</td>\n      <td>2.557500</td>\n      <td>21.500000</td>\n      <td>107.000000</td>\n      <td>2.800000</td>\n      <td>2.875000</td>\n      <td>0.437500</td>\n      <td>1.950000</td>\n      <td>6.200000</td>\n      <td>1.120000</td>\n      <td>3.170000</td>\n      <td>985.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>3.000000</td>\n      <td>14.830000</td>\n      <td>5.800000</td>\n      <td>3.230000</td>\n      <td>30.000000</td>\n      <td>162.000000</td>\n      <td>3.880000</td>\n      <td>5.080000</td>\n      <td>0.660000</td>\n      <td>3.580000</td>\n      <td>13.000000</td>\n      <td>1.710000</td>\n      <td>4.000000</td>\n      <td>1680.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nNo missing data. The scales of our features vary (e.g., Magnesium is in the 100s whereas Hue is in the low single digits).\n\nHow about our feature distributions?\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ndf_long = df.melt(id_vars='Class label')\n```\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nimport seaborn as sns\n\nsns.violinplot(data = df_long, x = 'variable', y = 'value')\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n<Axes: xlabel='variable', ylabel='value'>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_5_1_files/figure-pdf/cell-6-output-2.pdf){fig-pos='H'}\n:::\n:::\n\n\nMakes sense to normalise our data.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import MinMaxScaler\n\n# create a scaler object\nscaler = MinMaxScaler()\n\n# fit and transform the data\ndf_norm = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\ndf_long = df_norm.melt(id_vars='Class label')\ndf_long\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Class label</th>\n      <th>variable</th>\n      <th>value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>Alcohol</td>\n      <td>0.842105</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>Alcohol</td>\n      <td>0.571053</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>Alcohol</td>\n      <td>0.560526</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>Alcohol</td>\n      <td>0.878947</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>Alcohol</td>\n      <td>0.581579</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2309</th>\n      <td>1.0</td>\n      <td>Proline</td>\n      <td>0.329529</td>\n    </tr>\n    <tr>\n      <th>2310</th>\n      <td>1.0</td>\n      <td>Proline</td>\n      <td>0.336662</td>\n    </tr>\n    <tr>\n      <th>2311</th>\n      <td>1.0</td>\n      <td>Proline</td>\n      <td>0.397290</td>\n    </tr>\n    <tr>\n      <th>2312</th>\n      <td>1.0</td>\n      <td>Proline</td>\n      <td>0.400856</td>\n    </tr>\n    <tr>\n      <th>2313</th>\n      <td>1.0</td>\n      <td>Proline</td>\n      <td>0.201141</td>\n    </tr>\n  </tbody>\n</table>\n<p>2314 rows × 3 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nsns.violinplot(data = df_long, x = 'variable', y = 'value')\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n<Axes: xlabel='variable', ylabel='value'>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_5_1_files/figure-pdf/cell-8-output-2.pdf){fig-pos='H'}\n:::\n:::\n\n\nAre there any patterns?\n\nHow about a pairplot?\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nsns.pairplot(data = df_norm.iloc[:,1:])\n```\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_5_1_files/figure-pdf/cell-9-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nHmm, a few interesting correlations. Some of our variables are skewed. We could apply some PCA here to look at fewer dimension or even log transform some of the skewed variables.\n\nFor now we will just run a kmeans cluster and then check our results against the ground truth.\n\nLets decide how many clusters we need.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nfrom sklearn.cluster import KMeans\n\nks = range(1, 10)\ninertias = []\nfor k in ks:\n    # Create a KMeans instance with k clusters: model\n    model = KMeans(n_clusters=k, n_init = 10)\n    \n    # Fit model to samples\n    model.fit(df.iloc[:,1:])\n    \n    # Append the inertia to the list of inertias\n    inertias.append(model.inertia_)\n\nimport matplotlib.pyplot as plt\n\nplt.plot(ks, inertias, '-o', color='black')\nplt.xlabel('number of clusters, k')\nplt.ylabel('inertia')\nplt.xticks(ks)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_5_1_files/figure-pdf/cell-10-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nWhat happens if we use the normalised data instead?\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nfrom sklearn.cluster import KMeans\n\nks = range(1, 10)\ninertias = []\nfor k in ks:\n    # Create a KMeans instance with k clusters: model\n    model = KMeans(n_clusters=k, n_init = 10)\n    \n    # Fit model to samples\n    model.fit(df_norm.iloc[:,1:])\n    \n    # Append the inertia to the list of inertias\n    inertias.append(model.inertia_)\n\nimport matplotlib.pyplot as plt\n\nplt.plot(ks, inertias, '-o', color='black')\nplt.xlabel('number of clusters, k')\nplt.ylabel('inertia')\nplt.xticks(ks)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_5_1_files/figure-pdf/cell-11-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nBoth of the graphs are the same. Is that what you would expect?\n\nThree clusters seems about right (and matches our number of origonal labels).\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\ndf['Class label'].value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\nClass label\n2    71\n1    59\n3    48\nName: count, dtype: int64\n```\n:::\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\n# Create a KMeans instance with k clusters: model\nk_means = KMeans(n_clusters=3)\n\n# Fit model to samples\ndf_k_means = k_means.fit(df.iloc[:,1:])\n\ndf['Three clusters'] = pd.Series(df_k_means.predict(df.iloc[:,1:].values), index = df.index)\ndf\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but KMeans was fitted with feature names\n  warnings.warn(\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=12}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Class label</th>\n      <th>Alcohol</th>\n      <th>Malic acid</th>\n      <th>Ash</th>\n      <th>Alcalinity of ash</th>\n      <th>Magnesium</th>\n      <th>Total phenols</th>\n      <th>Flavanoids</th>\n      <th>Nonflavanoid phenols</th>\n      <th>Proanthocyanins</th>\n      <th>Color intensity</th>\n      <th>Hue</th>\n      <th>OD280/OD315 of diluted wines</th>\n      <th>Proline</th>\n      <th>Three clusters</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>14.23</td>\n      <td>1.71</td>\n      <td>2.43</td>\n      <td>15.6</td>\n      <td>127</td>\n      <td>2.80</td>\n      <td>3.06</td>\n      <td>0.28</td>\n      <td>2.29</td>\n      <td>5.64</td>\n      <td>1.04</td>\n      <td>3.92</td>\n      <td>1065</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>13.20</td>\n      <td>1.78</td>\n      <td>2.14</td>\n      <td>11.2</td>\n      <td>100</td>\n      <td>2.65</td>\n      <td>2.76</td>\n      <td>0.26</td>\n      <td>1.28</td>\n      <td>4.38</td>\n      <td>1.05</td>\n      <td>3.40</td>\n      <td>1050</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>13.16</td>\n      <td>2.36</td>\n      <td>2.67</td>\n      <td>18.6</td>\n      <td>101</td>\n      <td>2.80</td>\n      <td>3.24</td>\n      <td>0.30</td>\n      <td>2.81</td>\n      <td>5.68</td>\n      <td>1.03</td>\n      <td>3.17</td>\n      <td>1185</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>14.37</td>\n      <td>1.95</td>\n      <td>2.50</td>\n      <td>16.8</td>\n      <td>113</td>\n      <td>3.85</td>\n      <td>3.49</td>\n      <td>0.24</td>\n      <td>2.18</td>\n      <td>7.80</td>\n      <td>0.86</td>\n      <td>3.45</td>\n      <td>1480</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>13.24</td>\n      <td>2.59</td>\n      <td>2.87</td>\n      <td>21.0</td>\n      <td>118</td>\n      <td>2.80</td>\n      <td>2.69</td>\n      <td>0.39</td>\n      <td>1.82</td>\n      <td>4.32</td>\n      <td>1.04</td>\n      <td>2.93</td>\n      <td>735</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>173</th>\n      <td>3</td>\n      <td>13.71</td>\n      <td>5.65</td>\n      <td>2.45</td>\n      <td>20.5</td>\n      <td>95</td>\n      <td>1.68</td>\n      <td>0.61</td>\n      <td>0.52</td>\n      <td>1.06</td>\n      <td>7.70</td>\n      <td>0.64</td>\n      <td>1.74</td>\n      <td>740</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>174</th>\n      <td>3</td>\n      <td>13.40</td>\n      <td>3.91</td>\n      <td>2.48</td>\n      <td>23.0</td>\n      <td>102</td>\n      <td>1.80</td>\n      <td>0.75</td>\n      <td>0.43</td>\n      <td>1.41</td>\n      <td>7.30</td>\n      <td>0.70</td>\n      <td>1.56</td>\n      <td>750</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>175</th>\n      <td>3</td>\n      <td>13.27</td>\n      <td>4.28</td>\n      <td>2.26</td>\n      <td>20.0</td>\n      <td>120</td>\n      <td>1.59</td>\n      <td>0.69</td>\n      <td>0.43</td>\n      <td>1.35</td>\n      <td>10.20</td>\n      <td>0.59</td>\n      <td>1.56</td>\n      <td>835</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>176</th>\n      <td>3</td>\n      <td>13.17</td>\n      <td>2.59</td>\n      <td>2.37</td>\n      <td>20.0</td>\n      <td>120</td>\n      <td>1.65</td>\n      <td>0.68</td>\n      <td>0.53</td>\n      <td>1.46</td>\n      <td>9.30</td>\n      <td>0.60</td>\n      <td>1.62</td>\n      <td>840</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>177</th>\n      <td>3</td>\n      <td>14.13</td>\n      <td>4.10</td>\n      <td>2.74</td>\n      <td>24.5</td>\n      <td>96</td>\n      <td>2.05</td>\n      <td>0.76</td>\n      <td>0.56</td>\n      <td>1.35</td>\n      <td>9.20</td>\n      <td>0.61</td>\n      <td>1.60</td>\n      <td>560</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>178 rows × 15 columns</p>\n</div>\n```\n:::\n:::\n\n\nDo our cluster labels match our ground truth? Did our cluster model capture reality?\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nct = pd.crosstab(df['Three clusters'], df['Class label'])\nct\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Class label</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n    </tr>\n    <tr>\n      <th>Three clusters</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>13</td>\n      <td>20</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>46</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>50</td>\n      <td>19</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nIt might be easier to see as a stacked plot (see [this post](https://stackoverflow.com/questions/43544694/using-pandas-crosstab-with-seaborn-stacked-barplots)).\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nct.plot.bar(stacked=True)\nplt.legend(title='Class label')\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\n<matplotlib.legend.Legend at 0x15b0ff1d0>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_5_1_files/figure-pdf/cell-15-output-2.pdf){fig-pos='H'}\n:::\n:::\n\n\nHow has the kmeans model done compared to our ground truth?\n\nWe need to be really careful here. We notice that it is not easily possible to compare the known class labels to clustering labels. The reason is that the clustering algorithm labels are just arbitrary and not assigned to any deterministic criteria. Each time you run the algorithm, you might get a different id for the labels. The reason is that the label itself doesn't actually mean anything, what is important is the list of items that are in the same cluster and their relations.\n\nA way to come over this ambiguity and evaluate the results is to look at a visualisations of the results and compare. But this brings in the question of what type of visualisation to use for looking at the clusters. An immediate alternative is to use scatterplots. However, it is not clear which axis to use for clustering. A common method to apply at this stage is to make use of PCA to get a 2D plane where we can project the data points and visualise them over this projection.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\ndf.iloc[:,1:14]\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Alcohol</th>\n      <th>Malic acid</th>\n      <th>Ash</th>\n      <th>Alcalinity of ash</th>\n      <th>Magnesium</th>\n      <th>Total phenols</th>\n      <th>Flavanoids</th>\n      <th>Nonflavanoid phenols</th>\n      <th>Proanthocyanins</th>\n      <th>Color intensity</th>\n      <th>Hue</th>\n      <th>OD280/OD315 of diluted wines</th>\n      <th>Proline</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>14.23</td>\n      <td>1.71</td>\n      <td>2.43</td>\n      <td>15.6</td>\n      <td>127</td>\n      <td>2.80</td>\n      <td>3.06</td>\n      <td>0.28</td>\n      <td>2.29</td>\n      <td>5.64</td>\n      <td>1.04</td>\n      <td>3.92</td>\n      <td>1065</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>13.20</td>\n      <td>1.78</td>\n      <td>2.14</td>\n      <td>11.2</td>\n      <td>100</td>\n      <td>2.65</td>\n      <td>2.76</td>\n      <td>0.26</td>\n      <td>1.28</td>\n      <td>4.38</td>\n      <td>1.05</td>\n      <td>3.40</td>\n      <td>1050</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13.16</td>\n      <td>2.36</td>\n      <td>2.67</td>\n      <td>18.6</td>\n      <td>101</td>\n      <td>2.80</td>\n      <td>3.24</td>\n      <td>0.30</td>\n      <td>2.81</td>\n      <td>5.68</td>\n      <td>1.03</td>\n      <td>3.17</td>\n      <td>1185</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>14.37</td>\n      <td>1.95</td>\n      <td>2.50</td>\n      <td>16.8</td>\n      <td>113</td>\n      <td>3.85</td>\n      <td>3.49</td>\n      <td>0.24</td>\n      <td>2.18</td>\n      <td>7.80</td>\n      <td>0.86</td>\n      <td>3.45</td>\n      <td>1480</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>13.24</td>\n      <td>2.59</td>\n      <td>2.87</td>\n      <td>21.0</td>\n      <td>118</td>\n      <td>2.80</td>\n      <td>2.69</td>\n      <td>0.39</td>\n      <td>1.82</td>\n      <td>4.32</td>\n      <td>1.04</td>\n      <td>2.93</td>\n      <td>735</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>173</th>\n      <td>13.71</td>\n      <td>5.65</td>\n      <td>2.45</td>\n      <td>20.5</td>\n      <td>95</td>\n      <td>1.68</td>\n      <td>0.61</td>\n      <td>0.52</td>\n      <td>1.06</td>\n      <td>7.70</td>\n      <td>0.64</td>\n      <td>1.74</td>\n      <td>740</td>\n    </tr>\n    <tr>\n      <th>174</th>\n      <td>13.40</td>\n      <td>3.91</td>\n      <td>2.48</td>\n      <td>23.0</td>\n      <td>102</td>\n      <td>1.80</td>\n      <td>0.75</td>\n      <td>0.43</td>\n      <td>1.41</td>\n      <td>7.30</td>\n      <td>0.70</td>\n      <td>1.56</td>\n      <td>750</td>\n    </tr>\n    <tr>\n      <th>175</th>\n      <td>13.27</td>\n      <td>4.28</td>\n      <td>2.26</td>\n      <td>20.0</td>\n      <td>120</td>\n      <td>1.59</td>\n      <td>0.69</td>\n      <td>0.43</td>\n      <td>1.35</td>\n      <td>10.20</td>\n      <td>0.59</td>\n      <td>1.56</td>\n      <td>835</td>\n    </tr>\n    <tr>\n      <th>176</th>\n      <td>13.17</td>\n      <td>2.59</td>\n      <td>2.37</td>\n      <td>20.0</td>\n      <td>120</td>\n      <td>1.65</td>\n      <td>0.68</td>\n      <td>0.53</td>\n      <td>1.46</td>\n      <td>9.30</td>\n      <td>0.60</td>\n      <td>1.62</td>\n      <td>840</td>\n    </tr>\n    <tr>\n      <th>177</th>\n      <td>14.13</td>\n      <td>4.10</td>\n      <td>2.74</td>\n      <td>24.5</td>\n      <td>96</td>\n      <td>2.05</td>\n      <td>0.76</td>\n      <td>0.56</td>\n      <td>1.35</td>\n      <td>9.20</td>\n      <td>0.61</td>\n      <td>1.60</td>\n      <td>560</td>\n    </tr>\n  </tbody>\n</table>\n<p>178 rows × 13 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nfrom sklearn.decomposition import PCA\n\nn_components = 2\n\npca = PCA(n_components=n_components)\ndf_pca = pca.fit(df.iloc[:,1:14])\ndf_pca_vals = df_pca.transform(df.iloc[:,1:14])\n```\n:::\n\n\nGrab our projections and plot along with our cluster names.\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\ndf['c1'] = [item[0] for item in df_pca_vals]\ndf['c2'] = [item[1] for item in df_pca_vals]\n\nax = sns.scatterplot(data = df, x = 'c1', y = 'c2', hue = 'Class label')\nax.set_title('Known labels visualised over PCs')\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\nText(0.5, 1.0, 'Known labels visualised over PCs')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_5_1_files/figure-pdf/cell-18-output-2.pdf){fig-pos='H'}\n:::\n:::\n\n\nIn the figure above, we colored the points based on the actual labels, we observe that there has been several misclassifications in the figure above (i.e., in the algorithm's results). So one may choose to use an alternative algorithm or devise a better distance metric.\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nax = sns.scatterplot(data = df, x = 'c1', y = 'c2', hue = 'Three clusters')\nax.set_title('Results of the algorithm visualised over PCs')\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\nText(0.5, 1.0, 'Results of the algorithm visualised over PCs')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_5_1_files/figure-pdf/cell-19-output-2.pdf){fig-pos='H'}\n:::\n:::\n\n\nThis shows the parallelism between the clustering algorithm and PCA. By looking at the PCA loadings, we can find out what the x-axis mean and try to interpret the clusters (We leave this as an additional exercise for those interested).\n\nHow might your interpret the above plots? Did the kmeans model identify the ground truth?\n\nHow robust is our clustering? It may be that the kmeans algorithm becamse stuck or that a few outliers have biased the clustering.\n\nTwo ways to check are:\n\n* Running the model multiple times with different initial values.\n* Removing some data and running the modelling multiple times.\n\nRun the below cell a few times. What do you see?\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\n# Create a KMeans instance with k clusters: model\nk_means = KMeans(n_clusters=3, init='random', n_init = 10)\n\n# Fit model to samples\ndf_k_means = k_means.fit(df.iloc[:,1:14])\n\ndf['Three clusters'] = pd.Series(df_k_means.predict(df.iloc[:,1:14].values), index = df.index)\n\nax = sns.scatterplot(data = df, x = 'c1', y = 'c2', hue = 'Three clusters')\nax.set_title('Results of the algorithm visualised over PCs')\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but KMeans was fitted with feature names\n  warnings.warn(\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\nText(0.5, 1.0, 'Results of the algorithm visualised over PCs')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_5_1_files/figure-pdf/cell-20-output-3.pdf){fig-pos='H'}\n:::\n:::\n\n\nHow about with only 80% of the data?\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\ndf_sample = df.sample(frac=0.8, replace=False)\n\n# Create a KMeans instance with k clusters: model\nk_means = KMeans(n_clusters=3, init='random', n_init = 10)\n\n# Fit model to samples\ndf_k_means = k_means.fit(df_sample.iloc[:,1:14])\n\ndf_sample['Three clusters'] = pd.Series(df_k_means.predict(df_sample.iloc[:,1:14].values), index = df_sample.index)\n\nax = sns.scatterplot(data = df_sample, x = 'c1', y = 'c2', hue = 'Three clusters')\nax.set_title('Results of the algorithm visualised over PCs')\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but KMeans was fitted with feature names\n  warnings.warn(\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\nText(0.5, 1.0, 'Results of the algorithm visualised over PCs')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_5_1_files/figure-pdf/cell-21-output-3.pdf){fig-pos='H'}\n:::\n:::\n\n\nWe may want to automate the process of resampling the data or rerunning the model then perhaps plotting the different inertia values or creating different plots.\n\nDo you think our clustering algorithm is stable and provide similiar results even when some data is removed or the initial values are random?\n\nIf so, then is our algorithm capturing the ground truth?\n\n",
    "supporting": [
      "IM939_Lab_5_1_files/figure-pdf"
    ],
    "filters": []
  }
}