{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false \n",
    "\n",
    "# We are suppressing warnings for now. Comment lines below if you want to see the warnings, as they tend to be informative.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Clustering and Ground Truth\n",
    "\n",
    "\n",
    "We are going to use the Wine Quality Dataset from @cortezWineQuality2009 that you may be familiar with by now (but if you don't, tou can find more information about it here: <https://doi.org/10.24432/C56S3T>).\n",
    "\n",
    "## Data Wrangling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/wine.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a column called Class label that gives us the ground truth. The wines come from three different cultivars. Knowing the actual grouping helps us to identify how well our methods can capture this ground truth.\n",
    "\n",
    "Following our process above, we should first get a sense of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing data. The scales of our features vary (e.g., Magnesium is in the 100s whereas Hue is in the low single digits).\n",
    "\n",
    "How about our feature distributions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = df.melt(id_vars='Class label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.violinplot(data = df_long, x = 'variable', y = 'value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Makes sense to normalise our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# create a scaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# fit and transform the data\n",
    "df_norm = pd.DataFrame(scaler.fit_transform(df), columns = df.columns)\n",
    "\n",
    "df_long = df_norm.melt(id_vars='Class label')\n",
    "df_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create seaborn violin plot\n",
    "my_plot = sns.violinplot(data = df_long, x = 'variable', y = 'value')\n",
    "\n",
    "#rotate x-axis labels\n",
    "my_plot.set_xticklabels(my_plot.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there any patterns?\n",
    "\n",
    "How about a pairplot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| column: page\n",
    "sns.pairplot(data = df_norm.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, a few interesting correlations. Some of our variables are skewed. We could apply some PCA here to look at fewer dimension or even log transform some of the skewed variables.\n",
    "\n",
    "## Cluster analysis\n",
    "\n",
    "For now we will just run a kmeans cluster and then check our results against the ground truth.\n",
    "\n",
    "### Determining the number of clusters\n",
    "\n",
    "Lets decide how many clusters we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "ks = range(1, 10)\n",
    "inertias = []\n",
    "for k in ks:\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "    model = KMeans(n_clusters=k, n_init = 10)\n",
    "    \n",
    "    # Fit model to samples\n",
    "    model.fit(df.iloc[:,1:])\n",
    "    \n",
    "    # Append the inertia to the list of inertias\n",
    "    inertias.append(model.inertia_)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(ks, inertias, '-o', color='black')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(ks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we use the normalised data instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "ks = range(1, 10)\n",
    "inertias = []\n",
    "for k in ks:\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "    model = KMeans(n_clusters=k, n_init = 10)\n",
    "    \n",
    "    # Fit model to samples\n",
    "    model.fit(df_norm.iloc[:,1:])\n",
    "    \n",
    "    # Append the inertia to the list of inertias\n",
    "    inertias.append(model.inertia_)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(ks, inertias, '-o', color='black')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(ks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::callout-warning\n",
    "### Pause for thought\n",
    "\n",
    "Both of the graphs are the same. Is that what you would expect?\n",
    "\n",
    ":::\n",
    "\n",
    "Three clusters seems about right (and matches our number of origonal labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Class label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KMeans instance with k clusters: model\n",
    "k_means = KMeans(n_clusters=3)\n",
    "\n",
    "# Fit model to samples\n",
    "df_k_means = k_means.fit(df.iloc[:,1:])\n",
    "\n",
    "df['Three clusters'] = pd.Series(df_k_means.predict(df.iloc[:,1:].values), index = df.index)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusters and Ground Truth\n",
    "Now that we have created three clusters, we may ask ourselves: Do our cluster labels match our ground truth? Did our cluster model capture reality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = pd.crosstab(df['Three clusters'], df['Class label'])\n",
    "ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be easier to see as a stacked plot (see [this post](https://stackoverflow.com/questions/43544694/using-pandas-crosstab-with-seaborn-stacked-barplots))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "ct.plot.bar(stacked=True)\n",
    "plt.legend(title='Class label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How has the kmeans model done compared to our ground truth?\n",
    "\n",
    "::: callout-important\n",
    "\n",
    "We need to be really careful here. We notice that it is not easily possible to compare the known class labels to clustering labels. The reason is that the clustering algorithm labels are just arbitrary and not assigned to any deterministic criteria. Each time you run the algorithm, you might get a different id for the labels. **The reason is that the label itself doesn't actually mean anything, what is important is the list of items that are in the same cluster and their relations.**\n",
    "\n",
    ":::\n",
    "\n",
    "### Principal Components Analysis\n",
    "\n",
    "A way to come over this ambiguity and evaluate the results is to look at a visualisations of the results and compare. But this brings in the question of what type of visualisation to use for looking at the clusters. An immediate alternative is to use scatterplots. However, it is not clear which axis to use for clustering. A common method to apply at this stage is to make use of PCA to get a 2D plane where we can project the data points and visualise them over this projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,1:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = 2\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "df_pca = pca.fit(df.iloc[:,1:14])\n",
    "df_pca_vals = df_pca.transform(df.iloc[:,1:14])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab our projections and plot along with our cluster names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['c1'] = [item[0] for item in df_pca_vals]\n",
    "df['c2'] = [item[1] for item in df_pca_vals]\n",
    "\n",
    "ax = sns.scatterplot(data = df, x = 'c1', y = 'c2', hue = 'Class label')\n",
    "ax.set_title('Known labels visualised over PCs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above, we colored the points based on the actual labels, we observe that there has been several misclassifications in the figure above (i.e., in the algorithm's results). So one may choose to use an alternative algorithm or devise a better distance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(data = df, x = 'c1', y = 'c2', hue = 'Three clusters')\n",
    "ax.set_title('Results of the algorithm visualised over PCs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows the parallelism between the clustering algorithm and PCA. By looking at the PCA loadings, we can find out what the x-axis mean and try to interpret the clusters (We leave this as an additional exercise for those interested).\n",
    "\n",
    "How might your interpret the above plots? Did the kmeans model identify the ground truth?\n",
    "\n",
    "How robust is our clustering? It may be that the kmeans algorithm becamse stuck or that a few outliers have biased the clustering.\n",
    "\n",
    "Two ways to check are:\n",
    "\n",
    "* Running the model multiple times with different initial values.\n",
    "* Removing some data and running the modelling multiple times.\n",
    "\n",
    "Run the below cell a few times. What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KMeans instance with k clusters: model\n",
    "k_means = KMeans(n_clusters=3, init='random', n_init = 10)\n",
    "\n",
    "# Fit model to samples\n",
    "df_k_means = k_means.fit(df.iloc[:,1:14])\n",
    "\n",
    "df['Three clusters'] = pd.Series(df_k_means.predict(df.iloc[:,1:14].values), index = df.index)\n",
    "\n",
    "ax = sns.scatterplot(data = df, x = 'c1', y = 'c2', hue = 'Three clusters')\n",
    "ax.set_title('Results of the algorithm visualised over PCs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about with only 80% of the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.sample(frac=0.8, replace=False)\n",
    "\n",
    "# Create a KMeans instance with k clusters: model\n",
    "k_means = KMeans(n_clusters=3, init='random', n_init = 10)\n",
    "\n",
    "# Fit model to samples\n",
    "df_k_means = k_means.fit(df_sample.iloc[:,1:14])\n",
    "\n",
    "df_sample['Three clusters'] = pd.Series(df_k_means.predict(df_sample.iloc[:,1:14].values), index = df_sample.index)\n",
    "\n",
    "ax = sns.scatterplot(data = df_sample, x = 'c1', y = 'c2', hue = 'Three clusters')\n",
    "ax.set_title('Results of the algorithm visualised over PCs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may want to automate the process of resampling the data or rerunning the model then perhaps plotting the different inertia values or creating different plots.\n",
    "\n",
    "Do you think our clustering algorithm is stable and provide similiar results even when some data is removed or the initial values are random?\n",
    "\n",
    "If so, then is our algorithm capturing the ground truth?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
