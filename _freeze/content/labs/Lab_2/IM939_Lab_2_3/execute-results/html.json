{
  "hash": "392c498ddd2652db3056268aa5c1dfbb",
  "result": {
    "markdown": "\n\n# Lab: Missing data {#sec-pandas-transforming-data}\n\nIn these exercises, we have provided \"good\" datasets where data is properly structured in rows and columns, there's no missing data, or mixed types... Unfortunately, in real-world cases this may not be as common as we may think of. One of the main issues we may encounter is that some data may be missing. \n\nThis can be due to a number of reasons i.e., because the measurements failed during a specific period of time, because criteria changed... and the implications of this can be serious depending on the proportion of missing data and how we deal with it. Unfortunately there's not a single, correct way of dealing with missing data: some times we may want to get rid of them if they are anecdotal, sometimes we may want to infer their values based on existing data...\n\nIn this notebook we will be working with a modified version of the _The Office_ dataset that we have been using, where some values have been removed.\n\n\n## Assessing Missing values\n\nIn this case we will be reading the modified dataset stored in `office_ratings_missing.csv` where some values have been removed. The first thing we may want to do is to know where and how many of those values have been removed to inform what to do next with them.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\ndf = pd.read_csv('data/raw/office_ratings_missing.csv', encoding = 'UTF-8')\n```\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=51}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>season</th>\n      <th>episode</th>\n      <th>title</th>\n      <th>imdb_rating</th>\n      <th>total_votes</th>\n      <th>air_date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Pilot</td>\n      <td>7.6</td>\n      <td>NaN</td>\n      <td>24/03/2005</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2</td>\n      <td>Diversity Day</td>\n      <td>8.3</td>\n      <td>3566.0</td>\n      <td>29/03/2005</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>3</td>\n      <td>Health Care</td>\n      <td>7.9</td>\n      <td>2983.0</td>\n      <td>05/04/2005</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>4</td>\n      <td>The Alliance</td>\n      <td>8.1</td>\n      <td>2886.0</td>\n      <td>12/04/2005</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>5</td>\n      <td>Basketball</td>\n      <td>8.4</td>\n      <td>3179.0</td>\n      <td>19/04/2005</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nDid you notice something weird on the row number 1?\n\n\n::: callout-important\n\n### Storing missing values\n\nAt this stage, it is important to understand how Python stores missing values. For python, any missing data will be represented as any of these values: `NaN` (Not a Number), `NA` (Not Available) or `None`. This is to differenciate that with cases where we may see a \"gap\" in the dataframe that may look like a missing data, but it is an empty string instead (`\"\"`) or a string with a white space (`\" \"`). These are not considered empty values.\n\n:::\n\n\nRegretfully, exploring the head or the tail of the dataframe may not be a good idea, especially in large datasets. We may want to use `info` instead:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ndf.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 188 entries, 0 to 187\nData columns (total 6 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   season       188 non-null    int64  \n 1   episode      188 non-null    int64  \n 2   title        188 non-null    object \n 3   imdb_rating  170 non-null    float64\n 4   total_votes  168 non-null    float64\n 5   air_date     188 non-null    object \ndtypes: float64(2), int64(2), object(2)\nmemory usage: 8.9+ KB\n```\n:::\n:::\n\n\nWe are missing values in our `imdb_rating` and `total_votes` columns. Now, we need to know how many values are missing. We can do that in multiple ways. One is to combine the method `.isna` , which returns either `True` (`1`) or `False` (`2`), and then sum the values that are true (and thus, are null):\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Count missing values\ndf.isna().sum()\n```\n\n::: {.cell-output .cell-output-display execution_count=53}\n```\nseason          0\nepisode         0\ntitle           0\nimdb_rating    18\ntotal_votes    20\nair_date        0\ndtype: int64\n```\n:::\n:::\n\n\nAnother method is to get the maximum possible values and substract the sum of existing values:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# Count missing values\ndf.shape[0] - df.count()\n# df.shape returns the size of the dataframe and count() will only count the rows where there is a value\n```\n\n::: {.cell-output .cell-output-display execution_count=54}\n```\nseason          0\nepisode         0\ntitle           0\nimdb_rating    18\ntotal_votes    20\nair_date        0\ndtype: int64\n```\n:::\n:::\n\n\nNow we have an understanding of where the issues are and how many are there. Now the question is: _What to do with missing data?_ \n\n## Inputting values{#sec-inputting-missing-data}\n\nA quick solution is to replace missing values with either `0 ` or give them a roughtly central value (the mean). \n\nTo do this we use the `fillna` method, which _fills_ any missing values (`NA` -Not Available or `NaN` )\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ndf['imdb_rating_with_0'] = df['imdb_rating'].fillna(0)\ndf.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 188 entries, 0 to 187\nData columns (total 7 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   season              188 non-null    int64  \n 1   episode             188 non-null    int64  \n 2   title               188 non-null    object \n 3   imdb_rating         170 non-null    float64\n 4   total_votes         168 non-null    float64\n 5   air_date            188 non-null    object \n 6   imdb_rating_with_0  188 non-null    float64\ndtypes: float64(3), int64(2), object(2)\nmemory usage: 10.4+ KB\n```\n:::\n:::\n\n\nThis worked! `imdb_rating_with_0` does not have any missing value. However, we are significantly altering some statistical values:\n\n\n::: callout-important\n\nIn the above, we are saving the \"fixed\" (in more technical terms, \"imputed\") data column in `imdb_rating` in a new column named `imdb_rating_with_0`.\nIt is a good practice to keep the \"original\" data and make \"copies\" of data features where you have made changes. \nIn this way, you can always go back and do things differently and also compare the data sets with and without the intervention (for instance, in the following, we will be able to compare the descriptive statistics of the two columns)\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ndf[['imdb_rating', 'imdb_rating_with_0']].describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=56}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>imdb_rating</th>\n      <th>imdb_rating_with_0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>170.000000</td>\n      <td>188.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>8.261176</td>\n      <td>7.470213</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.514084</td>\n      <td>2.485781</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>6.800000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>7.900000</td>\n      <td>7.800000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>8.200000</td>\n      <td>8.200000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>8.600000</td>\n      <td>8.500000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>9.700000</td>\n      <td>9.700000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nIn order to try to avoid that, we can fill them with the mean:\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ndf['imdb_rating_with_mean'] = df['imdb_rating'].fillna(df['imdb_rating'].mean())\n\ndf[['imdb_rating', 'imdb_rating_with_0', 'imdb_rating_with_mean']].describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=57}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>imdb_rating</th>\n      <th>imdb_rating_with_0</th>\n      <th>imdb_rating_with_mean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>170.000000</td>\n      <td>188.000000</td>\n      <td>188.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>8.261176</td>\n      <td>7.470213</td>\n      <td>8.261176</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.514084</td>\n      <td>2.485781</td>\n      <td>0.488716</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>6.800000</td>\n      <td>0.000000</td>\n      <td>6.800000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>7.900000</td>\n      <td>7.800000</td>\n      <td>8.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>8.200000</td>\n      <td>8.200000</td>\n      <td>8.261176</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>8.600000</td>\n      <td>8.500000</td>\n      <td>8.500000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>9.700000</td>\n      <td>9.700000</td>\n      <td>9.700000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWe can plot these to see what looks most reasonable (you can probably also make an educated guess here).\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\ndf['imdb_rating_with_mean'].plot()\n```\n\n::: {.cell-output .cell-output-display execution_count=58}\n```\n<Axes: >\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_2_3_files/figure-html/cell-11-output-2.png){width=571 height=411}\n:::\n:::\n\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\ndf['imdb_rating_with_0'].plot()\n```\n\n::: {.cell-output .cell-output-display execution_count=59}\n```\n<Axes: >\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_2_3_files/figure-html/cell-12-output-2.png){width=566 height=411}\n:::\n:::\n\n\nGoing with the mean seems quite sensible in this case. Especially as the data is gaussian so the mean is probably an accurate represenation of the central value.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\n# Create a histogram\nax = df['imdb_rating'].hist()\n# Plot the histogram and add a vertical line on the mean\nax.axvline(df['imdb_rating'].mean(), color='k', linestyle='--')\n```\n\n::: {.cell-output .cell-output-display execution_count=60}\n```\n<matplotlib.lines.Line2D at 0x14ddd6e90>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_2_3_files/figure-html/cell-13-output-2.png){width=566 height=411}\n:::\n:::\n\n\n## Transformations\n\nSome statistical models, such as standard linear regression, require the predicted variable to be gaussian distributed (a single central point and a roughly symmetrical decrease in frequency, see [this Wolfram alpha page](https://www.wolframalpha.com/input/?i=gaussian+0+1).\n\nThe distribution of votes is positively skewed (most values are low).\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\ndf['total_votes'].hist()\n```\n\n::: {.cell-output .cell-output-display execution_count=61}\n```\n<Axes: >\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_2_3_files/figure-html/cell-14-output-2.png){width=566 height=411}\n:::\n:::\n\n\nA log transformation can make this data closer to a gaussian distributed data variable. \n\n::: callout-caution\n\n## `numpy` Library\n\nWe are now brining in another library called numpy -- see here: https://numpy.org/\n\nNumpy is a central library that most of data analysis protocols make use of. Pandas makes regular use of Numpy for instance. Several of the underlying data structures such as arrays, indexes we use in Pandas and elsewhere will build on Numpy structures. Don't worry too much about it for now. We'll use Numpy more later -- Here is a useful user guide to give you an idea: https://numpy.org/doc/stable/user/index.html#user\n\nFor the log transformation we are going to use `log2` method provided by `numpy` (numerical python).\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\n# This is how we import numpy, usually the convention is to use np as the short variable name and you can access numpy functions by `np.`\nimport numpy as np\n\ndf['total_votes_log'] = np.log2(df['total_votes'])\n```\n:::\n\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\ndf['total_votes_log'].hist()\n```\n\n::: {.cell-output .cell-output-display execution_count=63}\n```\n<Axes: >\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_2_3_files/figure-html/cell-16-output-2.png){width=566 height=411}\n:::\n:::\n\n\nThat is less skewed, but not ideal. Perhaps a _square root_ transformation instead?\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\ndf['total_votes_sqrt'] = np.sqrt(df['total_votes'])\ndf['total_votes_sqrt'].hist()\n```\n\n::: {.cell-output .cell-output-display execution_count=64}\n```\n<Axes: >\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_2_3_files/figure-html/cell-17-output-2.png){width=566 height=411}\n:::\n:::\n\n\n...well, maybe a inverse/reciprocal transformation. It is possible we have hit the limit on what we can do.\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\ndf['total_votes_recip'] = np.reciprocal(df['total_votes'])\ndf['total_votes_recip'].hist()\n```\n\n::: {.cell-output .cell-output-display execution_count=65}\n```\n<Axes: >\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_2_3_files/figure-html/cell-18-output-2.png){width=566 height=411}\n:::\n:::\n\n\nAt this point, I think we should conceded that we can make the distribution less positively skewed. However, transformation are not magic and we cannot turn a heavily positively skewed distribution into a normally distributed one.\n\nOh well.\n\nWe can calculate `z` scores though so we can plot both `total_votes` and `imdb_ratings` on a single plot. Currently, the IMDB scores vary between 0 and 10 whereas the number of votes number in the thousands.\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\ndf['total_votes_z'] = (df['total_votes'] - df['total_votes'].mean()) / df['total_votes'].std()\ndf['imdb_rating_z'] = (df['imdb_rating'] - df['imdb_rating'].mean()) / df['imdb_rating'].std()\n```\n:::\n\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\ndf['total_votes_z'].hist()\n```\n\n::: {.cell-output .cell-output-display execution_count=67}\n```\n<Axes: >\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_2_3_files/figure-html/cell-20-output-2.png){width=566 height=411}\n:::\n:::\n\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\ndf['imdb_rating_z'].hist()\n```\n\n::: {.cell-output .cell-output-display execution_count=68}\n```\n<Axes: >\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_2_3_files/figure-html/cell-21-output-2.png){width=566 height=411}\n:::\n:::\n\n\nNow we can compare the trends in score and number of votes on a single plot.\n\n::: callout-warning\nWe are going to use a slightly different approach to creating the plots. Called to the `plot()` method from Pandas actually use a library called `matplotlib`. We are going to use the `pyplot` module of `matplotlib` directly.\n:::\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n```\n:::\n\n\nConvert the `air_date` into a datetime object.\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\ndf['air_date'] =  pd.to_datetime(df['air_date'], dayfirst=True)\n```\n:::\n\n\nThen call the subplots function fom pyplot to create two plots. From this we take the two plot axis (`ax1`, `ax2`) and call the method `scatter` for each to plot `imdb_rating_z` and `total_votes_z`.\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nplt.style.use('ggplot')\n\nf, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\nax1.scatter( df['air_date'], df['imdb_rating_z'], color = 'red')\nax1.set_title('IMDB rating')\nax2.scatter( df['air_date'], df['total_votes_z'], color = 'blue')\nax2.set_title('Total votes')\n```\n\n::: {.cell-output .cell-output-display execution_count=71}\n```\nText(0.5, 1.0, 'Total votes')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_2_3_files/figure-html/cell-24-output-2.png){width=569 height=434}\n:::\n:::\n\n\nWe can do better than that.\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\nplt.scatter(df['air_date'], df['imdb_rating_z'], color = 'red', alpha = 0.1)\nplt.scatter(df['air_date'], df['total_votes_z'], color = 'blue', alpha = 0.1)\n```\n\n::: {.cell-output .cell-output-display execution_count=72}\n```\n<matplotlib.collections.PathCollection at 0x14e426290>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_2_3_files/figure-html/cell-25-output-2.png){width=569 height=411}\n:::\n:::\n\n\nWe have done a lot so far. Exploring data in part 1, plotting data with the inbuilt Pandas methods in part 2 and dealing with both missing data and transfromations in part 3.\n\nIn part 4, we will look at creating your own functions, a plotting library called seaborn and introduce a larger dataset.\n\n",
    "supporting": [
      "IM939_Lab_2_3_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}