{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Data Processing and Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will be exploring different functions in data processing and summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.spatial.distance import cdist # This is the function that computes the Mahalanobis distance\n",
    "from scipy.stats import skew, kurtosis # The skew and kurtosis of a distribution\n",
    "from statsmodels.robust.scale import mad # This computes the median absolute deviation of a distribution\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline # Display matplotlib plots inline with the text. +info: https://stackoverflow.com/a/43028034/1913457"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "\n",
    "In this section we will be applying several methods to identify outliers. We will be using a custom dataset called `accord_sedan.csv` that contains cars properties. The dataset is provided as a csv file.\n",
    "\n",
    "### Assess data\n",
    "\n",
    "As usual, we will want to start loading the data and assessing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "df = pd.read_csv('data/raw/accord_sedan.csv')\n",
    "\n",
    "# Inspecting the few first rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can get some summary statistics, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this might be difficult to spot outliers. This is where We can use data visualisations to identify outliers visually. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Identify outliers visually\n",
    "\n",
    "In this case we will **explore variables individually (1D)** by creating a _boxplot_[^boxplot]. We will visualise the columns `price` and `mileage` using matplotlib's subplots, which combines multiple plots into a single figure:\n",
    "\n",
    "[^boxplot]: Boxplots may be difficult to interpret if we are not used to them. It is used to visualise distributions, where the box represents the _Interquartile range (IQR)_, whereas the whiskers can be defined in multiple ways (e.g. the first and third quartiles, 1.5 IQR...) . ![A visualisation depicting how a Box Plot works. Source: [Wikipedia](https://en.wikipedia.org/wiki/Box_plot)](figs/440px-Boxplot_vs_PDF.svg.png) From [Wikipedia](https://en.wikipedia.org/wiki/Box_plot): \"In descriptive statistics, a box plot or boxplot is a method for graphically demonstrating the locality, spread and skewness groups of numerical data through their quartiles.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D Visualizations\n",
    "plt.figure(figsize=(9, 5)) # Setting the figure's size: format width, height (in inches)\n",
    "plt.subplot(1,2,1) # subplot(nrows, ncols, index, **kwargs)\n",
    "plt.boxplot(df.price)\n",
    "plt.title(\"Boxplot of the Price attribute\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.boxplot(df.mileage)\n",
    "plt.title(\"Boxplot of the Mileage attribute\");\n",
    "# A semicolon in Python denotes separation, rather than termination. \n",
    "# It allows you to write multiple statements on the same line. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we may want to **identify the 2D outliers** using a scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D Visualization\n",
    "plt.scatter(df.price, df.mileage, alpha = .5)\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Mileage')\n",
    "plt.title('Mileage vs. Price\\n');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: callout-note\n",
    "Visually, outliers appear to be outside the 'normal' range of the rest of the points. A few outliers are quite obvious to spot, but the choice of the threshold (the limit after which you decide to label a point as an outlier) visually remains a very subjective matter.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing outliers' threshold: standard deviation\n",
    "\n",
    "Add two new columns to the dataframe called `isOutlierPrice` and `isOutlierMileage`. We will define our threshold as **2 times standard deviations away from the mean** for `price`  and `mileage`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the isOutlierPrice column\n",
    "upper_threshold_price = df.price.mean() + 2*df.price.std() \n",
    "lower_threshold_price = df.price.mean() - 2*df.price.std()\n",
    "df['isOutlierPrice'] = ((df.price > upper_threshold_price) | (df.price < lower_threshold_price))\n",
    "\n",
    "# Computing the isOutlierMileage column\n",
    "upper_threshold_mileage = df.mileage.mean() + 2*df.mileage.std()\n",
    "lower_threshold_mileage = df.mileage.mean() - 2*df.mileage.std()\n",
    "df['isOutlierMileage'] = ((df.mileage > upper_threshold_mileage) | (df.mileage < lower_threshold_mileage))\n",
    "\n",
    "# Inspect the new DataFrame with the added columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: callout-tip\n",
    "\n",
    "### Alternative method\n",
    "\n",
    "We may want to use this more succint approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second way of doing the above using the np.where() function\n",
    "df['isOutlierPrice'] = np.where(abs(df.price - df.price.mean()) < 2*df.price.std(), False, True)\n",
    "df['isOutlierMileage'] = np.where(abs(df.mileage - df.mileage.mean()) < 2*df.mileage.std(), False, True)\n",
    "\n",
    "# Inspect the new DataFrame with the added columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we'd expect, both methods produce the same output.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have these two new columns, we could visualize these values with a different color in the plot. Observe whether they are the same as you would mark them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing outliers in a different color\n",
    "col = ['tomato' if i+j else 'seagreen' for i,j in zip(df.isOutlierPrice, df.isOutlierMileage)]\n",
    "plt.scatter(df.price, df.mileage, color = col)\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Mileage')\n",
    "plt.title('Mileage vs. Price : Outliers 2+ std\\'s away from the mean\\n');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually filtering out outliers can be an effective tactic if we're just trying to conduct a quick and dirty experimentation. However, when we need to perform a solid and founded analysis, it's better to have a robust justification for our choices.  \n",
    "\n",
    "In this case, we can use the deviation from the mean to define a threshold that separates 'normal' values from 'outliers'. Here, we opted for a two standard deviation threshold. \n",
    "\n",
    "The mathematical intuition behind this, is that under the normality assumption (if we assume our variable is normally distributed, which it almost is, refer to the next plot), then the probability of it having a value two standard deviations OR MORE away from the mean, is around 5%, which is very unlikely to happen. This is why we label these data points as outliers with respect to the (assumed) probability distribution of the variable. But this remains a way to identify 1D outliers only (identifying outliers within each column separately)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms of Price and Mileage (checking the normality assumption)\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(df.price, bins = 12)\n",
    "plt.title('Price')\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(df.mileage, bins = 15)\n",
    "plt.title('Mileage');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing outliers' threshold: Mahalanobis distance\n",
    "\n",
    "1) Using the 2D Mahalanobis distance to find outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| column: page-inset-right\n",
    "\n",
    "# Mean vector (computing the mean returns a Series, which we need to convert back to a DataFrame because cdist requires it)\n",
    "mean_v = df.iloc[:, 0:2].mean().to_frame().T # DataFrame.T returns the Transpose of the DataFrame\n",
    "#mean_v = np.asarray([df.price.mean(), df.mileage.mean() ]).reshape(1,2) # This is a better way of writing the line before (for our use case : cdist function)\n",
    "\n",
    "# Computing the Mahalanobis distance of each row to the mean vector\n",
    "d = cdist(df.iloc[:, 0:2], mean_v, metric='mahalanobis')\n",
    "#d = cdist(df[['price', 'mileage']].values, mean_v, metric='mahalanobis') # Another way of writing the line before\n",
    "\n",
    "# Visualizing the scatter plot while coloring each point (i.e row) with a color from a chosen gradient colormap corresponding to the mahalanobis score\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.scatter(df.price, df.mileage, c = d.flatten(), cmap = 'plasma') # in order to know why we use flatten() on d, try printing d with and without flatten\n",
    "plt.colorbar() # to show the colorbar\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Mileage')\n",
    "plt.title('Mileage vs. Price colored by a 2D Mahalanobis score\\n');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Q Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "In this case, we will be using data reported by countries to WHO and estimates of tuberculosis burden generated by WHO for the _Global Tuberculosis Report_, as part of WHO's [Global Tuberculosis Programme](https://www.who.int/teams/global-tuberculosis-programme/data). You can refer to [this section](https://www.who.int/teams/global-tuberculosis-programme/data#csv_files) at WHO's website to know more about the data and their variables.\n",
    "\n",
    "For your convenience, we have stored a copy in a csv file, but you could try to replicate the code with more up-to-date data from the original website. As usual, it is a best practice to explore the data before proceeding any further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the Data\n",
    "df_tuber = pd.read_csv('data/raw/TB_burden_countries_2014-09-29.csv')\n",
    "\n",
    "df_tuber.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tuber.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we have a pretty large dataset numerical and categorical variables, some of which have many missing values. In this case we will be filling missing values with the mean[^missing-data]\n",
    "\n",
    "[^missing-data]: Refer to @sec-inputting-missing-data for a detailed explanation and alternative methods to inputting missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing numeric values ONLY (categorical values will be untouched)\n",
    "df_tuber = df_tuber.fillna(value=df_tuber.mean(numeric_only = True))\n",
    "\n",
    "# Count missing values\n",
    "df_tuber.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Pick one of the columns from the Tuberculosis data and copy it into a numpy array as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picking a column (I created a variable for this so I (and you (: ) can modify the column easily here and the change will be carried out everywhere I use the variable colname)\n",
    "colname = 'e_prev_100k'\n",
    "\n",
    "# Creating a numpy array from our column\n",
    "col = np.array(df_tuber[colname])\n",
    "\n",
    "# Printing the type of our newly created column\n",
    "print(type(col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Compare this selected column to a Normal distribution. Then Sample from a Normal distribution and show a second Q-Q plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Q-Q Plot for our column\n",
    "sm.qqplot(col, line='r')\n",
    "plt.title('Q-Q Plot of the \"{}\" column of our dataset'.format(colname));\n",
    "\n",
    "# Plotting the Q-Q Plot for the log of our column\n",
    "sm.qqplot(np.log(col), line='r')\n",
    "plt.title('Q-Q Plot of the Log of the \"{}\" column'.format(colname));\n",
    "\n",
    "# Sampling from a Gaussian and a uniform distribution\n",
    "sample_norm = np.random.randn(1000)\n",
    "sample_unif = np.random.rand(1000)\n",
    "\n",
    "# Plotting the second Q-Q Plot for our sample (that was generated using a normal distribution)\n",
    "sm.qqplot(sample_norm, line='r')\n",
    "plt.title('Q-Q Plot of the generated sample (Gaussian)')\n",
    "sm.qqplot(sample_unif, line='r')\n",
    "plt.title('Q-Q Plot of the generated sample (Uniform)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Go ahead and change the colname variable (question 1) into a different column name (that you can pick from the list you have just before question 1 (but do pick a numeric column). And re-execute the code from question 1 and question 2 and you'll see your new Q-Q plot of the column you just picked.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Have a look at the slides from Week 03 for different shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Ok ? Now try to guess the shape of the distribution of our selected column (shape of its histogram) from its Q-Q Plot above.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Visualise the column on a histogram and reflect on whether the shape you inferred from Q-Q plots and the shape of the histogram correlate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| column: page-inset-right\n",
    "\n",
    "# Histogramming the column we picked (not sure the verb exists though)\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('Histogram of \"{}\"'.format(colname))\n",
    "plt.hist(col)\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Histogram of Log \"{}\"'.format(colname))\n",
    "plt.hist(np.log(col));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Of course it does ! From the shape of the Q-Q Plot above (convex, slope upwards) and the Slide of Q-Q Plots from Week 3, we could conclude before looking at the histogram that our distribution was right tailed (or positively skewed if you're into complex vocabulary lol). And it is !\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions, Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Inspecting the effect of sample size on descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a few variables se we can change their values easiliy without having to change the rest of the code\n",
    "n = [5, 20, 100, 2000]\n",
    "stds = [0.5, 1, 3]\n",
    "\n",
    "# Initializing empty 2D arrays where we're going to store the results of our simulation\n",
    "mean = np.empty([len(n), len(stds)])\n",
    "std = np.empty([len(n), len(stds)])\n",
    "skewness = np.empty([len(n), len(stds)])\n",
    "kurtos = np.empty([len(n), len(stds)])\n",
    "\n",
    "# Conducting the experiments and storing the results in the respective 2D arrays\n",
    "for i, sample_size in enumerate(n):\n",
    "    for j, theoritical_std in enumerate(stds):\n",
    "        sample = np.random.normal(loc=0, scale=theoritical_std, size=sample_size)\n",
    "        mean[i,j] = sample.mean()\n",
    "        std[i,j] = sample.std()\n",
    "        skewness[i,j] = skew(sample)\n",
    "        kurtos[i,j] = kurtosis(sample)\n",
    "\n",
    "# Turning the mean 2D array into a pandas dataframe\n",
    "mean = pd.DataFrame(mean, columns = stds, index = n)\n",
    "mean = mean.rename_axis('Sample Size').rename_axis(\"Standard Deviation\", axis=\"columns\")\n",
    "\n",
    "# Turning the std 2D array into a pandas dataframe\n",
    "std = pd.DataFrame(std, columns = stds, index = n)\n",
    "std = std.rename_axis('Sample Size').rename_axis(\"Standard Deviation\", axis=\"columns\")\n",
    "\n",
    "# Turning the skewness 2D array into a pandas dataframe\n",
    "skewness = pd.DataFrame(skewness, columns = stds, index = n)\n",
    "skewness = skewness.rename_axis('Sample Size').rename_axis(\"Standard Deviation\", axis=\"columns\")\n",
    "\n",
    "# Turning the kurtosis 2D array into a pandas dataframe\n",
    "kurtos = pd.DataFrame(kurtos, columns = stds, index = n)\n",
    "kurtos = kurtos.rename_axis('Sample Size').rename_axis(\"Standard Deviation\", axis=\"columns\")\n",
    "\n",
    "print(\"GAUSSIAN DISTRIBUTION\\n\")\n",
    "print('Results for the Mean :')\n",
    "mean # This is a dataframe containing the means of the samples generated with different values of std and sample size\n",
    "print('Results for the Standard Deviation :')\n",
    "std # This is a dataframe containing the standard deviations of the samples generated with different values of std and sample size\n",
    "print('Results for the Skewness :')\n",
    "skewness # This is a dataframe containing the skews of the samples generated with different values of std and sample size\n",
    "print('Results for the Kurtosis :')\n",
    "kurtos # This is a dataframe containing the kurtosis of the samples generated with different values of std and sample size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Basically, the more data you have (the bigger your sample), the more accurate your empirical estimates are going to be. Observe for example the values of mean (1st DataFrame) and variance (2nd DataFrame) for the 2000 sample size (last row). In the first one, the values are all close to 0, because we generated our sample from a Gaussian with mean 0, and the values in the second one are all close to the values in the column names (which refer to the variance of the distribution of the sample). This means that for with a sample size of 2000, our estimates are really close to the \"True\" values (with which we generated the sample). Also, the Skew of a Gaussian distribution should be 0, and it is confirmed in the 3rd DataFrame where the values are close to 0 in the last row (i.e big sample size).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2) Same as before but with a Poisson distribution (which has just one parameter lambda instead of 2 like the gaussian)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a few variables se we can change their values easiliy without having to change the rest of the code\n",
    "n = [5, 20, 100, 2000]\n",
    "lambd = [0.5, 1, 3] # In a gaussian we had two parameters, a var specified here and a mean we chose to be 0 \n",
    "#everywhere. Here we have one parameter called lambda.\n",
    "\n",
    "# Initializing empty 2D arrays where we're going to store the results of our simulation\n",
    "mean = np.empty([len(n), len(lambd)])\n",
    "std = np.empty([len(n), len(lambd)])\n",
    "skewness = np.empty([len(n), len(lambd)])\n",
    "kurtos = np.empty([len(n), len(lambd)])\n",
    "\n",
    "# Conducting the experiments and storing the results in the respective 2D arrays\n",
    "for i, sample_size in enumerate(n):\n",
    "    for j, theoritical_lambd in enumerate(lambd):\n",
    "        #**********************************************************************\n",
    "        sample = np.random.poisson(lam = theoritical_lambd, size = sample_size) # THIS IS WHAT WE CHANGED IN Q2 !\n",
    "        #**********************************************************************\n",
    "        mean[i,j] = sample.mean()\n",
    "        std[i,j] = sample.std()\n",
    "        skewness[i,j] = skew(sample)\n",
    "        kurtos[i,j] = kurtosis(sample)\n",
    "\n",
    "# Turning the mean 2D array into a pandas dataframe\n",
    "mean = pd.DataFrame(mean, columns = lambd, index = n)\n",
    "mean = mean.rename_axis('Sample Size').rename_axis(\"Lambda\", axis=\"columns\")\n",
    "\n",
    "# Turning the std 2D array into a pandas dataframe\n",
    "std = pd.DataFrame(std, columns = lambd, index = n)\n",
    "std = std.rename_axis('Sample Size').rename_axis(\"Lambda\", axis=\"columns\")\n",
    "\n",
    "# Turning the skewness 2D array into a pandas dataframe\n",
    "skewness = pd.DataFrame(skewness, columns = lambd, index = n)\n",
    "skewness = skewness.rename_axis('Sample Size').rename_axis(\"Lambda\", axis=\"columns\")\n",
    "\n",
    "# Turning the kurtosis 2D array into a pandas dataframe\n",
    "kurtos = pd.DataFrame(kurtos, columns = lambd, index = n)\n",
    "kurtos = kurtos.rename_axis('Sample Size').rename_axis(\"Lambda\", axis=\"columns\")\n",
    "\n",
    "print(\"POISSON DISTRIBUTION\\n\")\n",
    "print('Results for the Mean :')\n",
    "mean # This is a dataframe containing the means of the samples generated with different values of std and sample size\n",
    "print('Results for the Standard Deviation :')\n",
    "std # This is a dataframe containing the standard deviations of the samples generated with different values of std \n",
    "#and sample size\n",
    "print('Results for the Skewness :')\n",
    "skewness # This is a dataframe containing the skews of the samples generated with different values of std and sample \n",
    "#size\n",
    "print('Results for the Kurtosis :')\n",
    "kurtos # This is a dataframe containing the kurtosis of the samples generated with different values of std and sample \n",
    "#size\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Just remember, the lambda parameter that defines the Poisson distribution is also the mean of the distribution. This is confirmed in the first DataFrame where the values (means of samples) are close to the column labels (theoretical lambda which is also equal to theoretical mean), especially in the last row.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Choose a number of columns with different shapes, for instance, \"e_prev_100k_hi\" is left skewed or some columns where the variation is high or you notice potential outliers. You can make use of a series of boxplots to exploratively analyse the data for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing the columns\n",
    "df_tuber.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| column: screen-inset\n",
    "\n",
    "# Alright I already know a few columns with outliers but let's try to find them together exploratively using BoxPlots\n",
    "colname = 'c_cdr' # change the column name by choosing different ones from above (numeric ones)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(df_tuber[colname])\n",
    "plt.title('Histogram of \"{}\"'.format(colname))\n",
    "plt.subplot(1,2,2)\n",
    "plt.boxplot(df_tuber[colname]);\n",
    "plt.title('Boxplot of \"{}\"'.format(colname));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chosen columns : I picked 3, feel free to change them and experiment\n",
    "chosen_colnames = ['e_pop_num', 'e_prev_100k', 'c_cdr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2) For the chosen columns, estimate both the conventional and the robust descriptive statistics and compare. Observe how these pairs deviate from each other based on the characteristics of the underlying data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Central Tendency : Mean vs Median (Median is the robust version of the mean, because it takes into account \n",
    "#the ordering of the points and not the actual values like the mean does)\n",
    "df_tuber[chosen_colnames].describe().loc[['mean', '50%'], :] # The 50% is the median (50% quantile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at how the values are different between the mean and the median ... LOOOOOOOK ! This is why when you have a skewed (unsymmetrical) distribution it's usually more interesting to use the median as a measure of the central tendency of the data. One important thing to note here, for the two first attributes, the mean is higher than the median, but for the last it's the opposite. This can tell you a thing or two about the shape of your distribution : if the mean is higher than the median, this means that the distribution is skewed to the right (right tail) which pulls the mean higher. And vice-versa.\n",
    "\n",
    "\n",
    "Moral of the story is ... outliers are a pain in the a**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spread : Standard Deviation vs Inter-Quartile Range vs Median Absolute Deviation (MAD)\n",
    "stds = df_tuber[chosen_colnames].std()\n",
    "iqrs = df_tuber[chosen_colnames].quantile(0.75) - df_tuber[chosen_colnames].quantile(0.25)\n",
    "medianAD = mad(df_tuber[chosen_colnames])\n",
    "\n",
    "output = pd.DataFrame(stds, columns = ['std']).T\n",
    "output = pd.concat([output, pd.DataFrame(iqrs, columns = ['IQR']).T], ignore_index=False)\n",
    "output = pd.concat([output, pd.DataFrame(medianAD, columns = ['MAD'], index = chosen_colnames).T], ignore_index=False, names = ['std', 'iqr', 'mad'])\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values here are different as well, maybe more so for the \"e_pop_num\" attribute than the others, but that is just because of the scaling : \"e_pop_num\" takes big values overall compared to the other columns, which you can check with the mean values right above.\n",
    "\n",
    "For the first attribute, the standard deviation is higher, and both the IQR and MAD are close to each other. For the second attribute, the inter-quartile range is slightly higher than the standard deviation, but the MAD is far below (less than half) the other two values, and the reason for that is a little bit involved : Basically, the standard deviation measures the spread by computing the squared deviation from the mean while the median absolute deviation evaluates the spread by computing the absolute deviation. This means that when the outliers have much bigger values than the \"normal\" points, the squared difference explodes (figuratively of course ;p) compared to the absolute difference. And this is actually the case for our second distribution (e_prev_100k) where most values are between 50 and 300 while many outliers lay above the 750 mark and go all the way up to 1800 (look at the boxplots below). For the third attribute the values are somewhat close, especially the std and the MAD, that's because if you inspect the boxplot, this column doesn't have many outliers to begin with.\n",
    "\n",
    "Nonetheless, the differences are real, and if we don't want to have to handle outliers, then we should be using robust statistics like the median to describe the central tendency and inter-quartile range or median absolute deviation to measure the spread of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| column: page-inset-right\n",
    "\n",
    "# Boxplots of the different columns\n",
    "plt.figure(figsize=(12,20))\n",
    "\n",
    "plt.subplot(3,2,1)\n",
    "plt.hist(df_tuber[chosen_colnames[0]])\n",
    "plt.title('Histogram of \"{}\"'.format(chosen_colnames[0]))\n",
    "plt.subplot(3,2,2)\n",
    "plt.boxplot(df_tuber[chosen_colnames[0]])\n",
    "plt.title('Boxplot of \"{}\"'.format(chosen_colnames[0]))\n",
    "\n",
    "plt.subplot(3,2,3)\n",
    "plt.hist(df_tuber[chosen_colnames[1]])\n",
    "plt.title('Histogram of \"{}\"'.format(chosen_colnames[1]))\n",
    "plt.subplot(3,2,4)\n",
    "plt.boxplot(df_tuber[chosen_colnames[1]])\n",
    "plt.title('Boxplot of \"{}\"'.format(chosen_colnames[1]))\n",
    "\n",
    "plt.subplot(3,2,5)\n",
    "plt.hist(df_tuber[chosen_colnames[2]])\n",
    "plt.title('Histogram of \"{}\"'.format(chosen_colnames[2]))\n",
    "plt.subplot(3,2,6)\n",
    "plt.boxplot(df_tuber[chosen_colnames[2]])\n",
    "plt.title('Boxplot of \"{}\"'.format(chosen_colnames[2]));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
