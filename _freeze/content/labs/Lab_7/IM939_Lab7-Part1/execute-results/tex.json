{
  "hash": "6a7707de7e192882863cd6e190619126",
  "result": {
    "markdown": "# Lab: Hate crimes\n\nIn the session 7 (week 8) we discussed data and society: academic and practices discourse on the social, political and ethical aspects of data science, and discussed how one can responsibly carry out data science research on social phenomena, what ethical and social frameworks can help us to critically approach data science practices and its effects on society,\nand what are ethical practices for data scientists.\n\n## Datasets \n\n- **[Hate crimes](https://fivethirtyeight.com/features/higher-rates-of-hate-crimes-are-tied-to-income-inequality/)**: a csv file \n- **[OECD Poverty gap](https://data.oecd.org/inequality/poverty-rate.htm)**: a csv file\n- **Poverty & Equity Data Portal**: From [Organisation for Economic Co-operation and Development (OECD)](https://data.oecd.org/inequality/income-inequality.htm#indicator-chart) or from [worldbank](https://povertydata.worldbank.org/poverty/home/)\n\n\n\n### Further datasets\n- **NHS**: multiple files. The NHS inequality challenge https://www.nuffieldtrust.org.uk/project/nhs-visual-data-challenge \n- **Office for National Statistics (ONS)**:\n  - [Gender Pay Gap](https://www.ons.gov.uk/employmentandlabourmarket/peopleinwork/earningsandworkinghours/datasets/annualsurveyofhoursandearningsashegenderpaygaptables) \n  - [Health state life expectancies by Index of Multiple Deprivation](https://www.ons.gov.uk/peoplepopulationandcommunity/healthandsocialcare/healthinequalities/) (IMD 2015 and IMD 2019): England, all ages\nmultiple publications\n\n\n### Additional Readings\n- **Indicators - critical reviews**: The Poverty of Statistics and the Statistics of Poverty: https://www.tandfonline.com/doi/full/10.1080/01436590903321844?src=recsys\n- **Indicators in global health**: arguments: indicators are usually comprehensible to a small group of experts. Why use indicators then? „Because indicators used in global HIV finance offer openings for engagement to promote accountability (...) some indicators and data truly are better than others, and as they were all created by humans, they all can be deconstructed and remade in other forms\" Davis, S. (2020). The Uncounted: Politics of Data in Global Health, Cambridge. doi:10.1017/9781108649544\n\n**Indicators - conceptualization**\n\n## Hate Crimes \n\n### Source: \nhttps://github.com/fivethirtyeight/data/tree/master/hate-crimes\n\n\n### Variables:\n| Header | Definition |\n| --- | --- |\n| state | State name |\n| median_household_income | Median household income, 2016 |\n | share_unemployed_seasonal | Share of the population that is unemployed (seasonally adjusted), Sept. 2016 | \n | share_population_in_metro_areas | Share of the population that lives in metropolitan areas, 2015 | \n | share_population_with_high_school_degree | Share of adults 25 and older with a high-school degree, 2009 | \n | share_non_citizen | Share of the population that are not U.S. citizens, 2015 | \n | share_white_poverty | Share of white residents who are living in poverty, 2015 | \n | gini_index | Gini Index, 2015 | \n | share_non_white | Share of the population that is not white, 2015 | \n | share_voters_voted_trump | Share of 2016 U.S. presidential voters who voted for Donald Trump | \n | hate_crimes_per_100k_splc | Hate crimes per 100,000 population, Southern Poverty Law Center, Nov. 9-18, 2016 | \n | avg_hatecrimes_per_100k_fbi | Average annual hate crimes per 100,000 population, FBI, 2010-2015 | \n\n## Data exploration\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\ndf = pd.read_excel('data/hate_Crimes_v2.xlsx')\n```\n:::\n\n\nA reminder: anything with a pd. prefix comes from pandas. This is particulary useful for preventing a module from overwriting inbuilt Python functionality.\n\nLet's have a look at our dataset\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndf.tail()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>NAME</th>\n      <th>median_household_income</th>\n      <th>share_unemployed_seasonal</th>\n      <th>share_population_in_metro_areas</th>\n      <th>share_population_with_high_school_degree</th>\n      <th>share_non_citizen</th>\n      <th>share_white_poverty</th>\n      <th>gini_index</th>\n      <th>share_non_white</th>\n      <th>share_voters_voted_trump</th>\n      <th>hate_crimes_per_100k_splc</th>\n      <th>avg_hatecrimes_per_100k_fbi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>46</th>\n      <td>Virginia</td>\n      <td>66155</td>\n      <td>0.043</td>\n      <td>0.89</td>\n      <td>0.866</td>\n      <td>0.06</td>\n      <td>0.07</td>\n      <td>0.459</td>\n      <td>0.38</td>\n      <td>0.45</td>\n      <td>0.36</td>\n      <td>1.72</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>Washington</td>\n      <td>59068</td>\n      <td>0.052</td>\n      <td>0.86</td>\n      <td>0.897</td>\n      <td>0.08</td>\n      <td>0.09</td>\n      <td>0.441</td>\n      <td>0.31</td>\n      <td>0.38</td>\n      <td>0.67</td>\n      <td>3.81</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>West Virginia</td>\n      <td>39552</td>\n      <td>0.073</td>\n      <td>0.55</td>\n      <td>0.828</td>\n      <td>0.01</td>\n      <td>0.14</td>\n      <td>0.451</td>\n      <td>0.07</td>\n      <td>0.69</td>\n      <td>0.32</td>\n      <td>2.03</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>Wisconsin</td>\n      <td>58080</td>\n      <td>0.043</td>\n      <td>0.69</td>\n      <td>0.898</td>\n      <td>0.03</td>\n      <td>0.09</td>\n      <td>0.430</td>\n      <td>0.22</td>\n      <td>0.48</td>\n      <td>0.22</td>\n      <td>1.12</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>Wyoming</td>\n      <td>55690</td>\n      <td>0.040</td>\n      <td>0.31</td>\n      <td>0.918</td>\n      <td>0.02</td>\n      <td>0.09</td>\n      <td>0.423</td>\n      <td>0.15</td>\n      <td>0.70</td>\n      <td>0.00</td>\n      <td>0.26</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ntype(df)\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\npandas.core.frame.DataFrame\n```\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ndf.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 51 entries, 0 to 50\nData columns (total 12 columns):\n #   Column                                    Non-Null Count  Dtype  \n---  ------                                    --------------  -----  \n 0   NAME                                      51 non-null     object \n 1   median_household_income                   51 non-null     int64  \n 2   share_unemployed_seasonal                 51 non-null     float64\n 3   share_population_in_metro_areas           51 non-null     float64\n 4   share_population_with_high_school_degree  51 non-null     float64\n 5   share_non_citizen                         48 non-null     float64\n 6   share_white_poverty                       51 non-null     float64\n 7   gini_index                                51 non-null     float64\n 8   share_non_white                           51 non-null     float64\n 9   share_voters_voted_trump                  51 non-null     float64\n 10  hate_crimes_per_100k_splc                 51 non-null     float64\n 11  avg_hatecrimes_per_100k_fbi               51 non-null     float64\ndtypes: float64(10), int64(1), object(1)\nmemory usage: 4.9+ KB\n```\n:::\n:::\n\n\n### Missing values\nLet's explore the dataset\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ndf.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 51 entries, 0 to 50\nData columns (total 12 columns):\n #   Column                                    Non-Null Count  Dtype  \n---  ------                                    --------------  -----  \n 0   NAME                                      51 non-null     object \n 1   median_household_income                   51 non-null     int64  \n 2   share_unemployed_seasonal                 51 non-null     float64\n 3   share_population_in_metro_areas           51 non-null     float64\n 4   share_population_with_high_school_degree  51 non-null     float64\n 5   share_non_citizen                         48 non-null     float64\n 6   share_white_poverty                       51 non-null     float64\n 7   gini_index                                51 non-null     float64\n 8   share_non_white                           51 non-null     float64\n 9   share_voters_voted_trump                  51 non-null     float64\n 10  hate_crimes_per_100k_splc                 51 non-null     float64\n 11  avg_hatecrimes_per_100k_fbi               51 non-null     float64\ndtypes: float64(10), int64(1), object(1)\nmemory usage: 4.9+ KB\n```\n:::\n:::\n\n\nThe above tables shows that we have some missing data for some of states. See below too.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ndf.isna().sum()\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\nNAME                                        0\nmedian_household_income                     0\nshare_unemployed_seasonal                   0\nshare_population_in_metro_areas             0\nshare_population_with_high_school_degree    0\nshare_non_citizen                           3\nshare_white_poverty                         0\ngini_index                                  0\nshare_non_white                             0\nshare_voters_voted_trump                    0\nhate_crimes_per_100k_splc                   0\navg_hatecrimes_per_100k_fbi                 0\ndtype: int64\n```\n:::\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nimport numpy as np\nnp.unique(df.NAME)\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\narray(['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California',\n       'Colorado', 'Connecticut', 'Delaware', 'District of Columbia',\n       'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana',\n       'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland',\n       'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi',\n       'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire',\n       'New Jersey', 'New Mexico', 'New York', 'North Carolina',\n       'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania',\n       'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee',\n       'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington',\n       'West Virginia', 'Wisconsin', 'Wyoming'], dtype=object)\n```\n:::\n:::\n\n\nThere aren't any unexpected values in 'state'. \n\n## Mapping hate crime across the USA\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n#using James' code from the last lab: we need  the geospatial polygons of the states in America  \nimport geopandas as gpd \nimport pandas as pd\nimport altair as alt\n\ngeo_states = gpd.read_file('data/gz_2010_us_040_00_500k.json')\n#df = pd.read_excel('data/hate_Crimes_v2.xlsx')\ngeo_states.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>GEO_ID</th>\n      <th>STATE</th>\n      <th>NAME</th>\n      <th>LSAD</th>\n      <th>CENSUSAREA</th>\n      <th>geometry</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0400000US23</td>\n      <td>23</td>\n      <td>Maine</td>\n      <td></td>\n      <td>30842.923</td>\n      <td>MULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0400000US25</td>\n      <td>25</td>\n      <td>Massachusetts</td>\n      <td></td>\n      <td>7800.058</td>\n      <td>MULTIPOLYGON (((-70.83204 41.60650, -70.82373 ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0400000US26</td>\n      <td>26</td>\n      <td>Michigan</td>\n      <td></td>\n      <td>56538.901</td>\n      <td>MULTIPOLYGON (((-88.68443 48.11579, -88.67563 ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0400000US30</td>\n      <td>30</td>\n      <td>Montana</td>\n      <td></td>\n      <td>145545.801</td>\n      <td>POLYGON ((-104.05770 44.99743, -104.25015 44.9...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0400000US32</td>\n      <td>32</td>\n      <td>Nevada</td>\n      <td></td>\n      <td>109781.180</td>\n      <td>POLYGON ((-114.05060 37.00040, -114.04999 36.9...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nalt.Chart(geo_states, title='US states').mark_geoshape().encode(\n).properties(\n    width=500,\n    height=300\n).project(\n    type='albersUsa'\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\nalt.Chart(...)\n```\n:::\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# Add the data\n#should i rename 'state' to 'NAME'?\ngeo_states = geo_states.merge(df, on='NAME')\n```\n:::\n\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\ngeo_states.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>GEO_ID</th>\n      <th>STATE</th>\n      <th>NAME</th>\n      <th>LSAD</th>\n      <th>CENSUSAREA</th>\n      <th>geometry</th>\n      <th>median_household_income</th>\n      <th>share_unemployed_seasonal</th>\n      <th>share_population_in_metro_areas</th>\n      <th>share_population_with_high_school_degree</th>\n      <th>share_non_citizen</th>\n      <th>share_white_poverty</th>\n      <th>gini_index</th>\n      <th>share_non_white</th>\n      <th>share_voters_voted_trump</th>\n      <th>hate_crimes_per_100k_splc</th>\n      <th>avg_hatecrimes_per_100k_fbi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0400000US23</td>\n      <td>23</td>\n      <td>Maine</td>\n      <td></td>\n      <td>30842.923</td>\n      <td>MULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...</td>\n      <td>51710</td>\n      <td>0.044</td>\n      <td>0.54</td>\n      <td>0.902</td>\n      <td>NaN</td>\n      <td>0.12</td>\n      <td>0.437</td>\n      <td>0.09</td>\n      <td>0.45</td>\n      <td>0.61</td>\n      <td>2.62</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0400000US25</td>\n      <td>25</td>\n      <td>Massachusetts</td>\n      <td></td>\n      <td>7800.058</td>\n      <td>MULTIPOLYGON (((-70.83204 41.60650, -70.82373 ...</td>\n      <td>63151</td>\n      <td>0.046</td>\n      <td>0.97</td>\n      <td>0.890</td>\n      <td>0.09</td>\n      <td>0.08</td>\n      <td>0.475</td>\n      <td>0.27</td>\n      <td>0.34</td>\n      <td>0.63</td>\n      <td>4.80</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0400000US26</td>\n      <td>26</td>\n      <td>Michigan</td>\n      <td></td>\n      <td>56538.901</td>\n      <td>MULTIPOLYGON (((-88.68443 48.11579, -88.67563 ...</td>\n      <td>52005</td>\n      <td>0.050</td>\n      <td>0.87</td>\n      <td>0.879</td>\n      <td>0.04</td>\n      <td>0.09</td>\n      <td>0.451</td>\n      <td>0.24</td>\n      <td>0.48</td>\n      <td>0.40</td>\n      <td>3.20</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0400000US30</td>\n      <td>30</td>\n      <td>Montana</td>\n      <td></td>\n      <td>145545.801</td>\n      <td>POLYGON ((-104.05770 44.99743, -104.25015 44.9...</td>\n      <td>51102</td>\n      <td>0.041</td>\n      <td>0.34</td>\n      <td>0.908</td>\n      <td>0.01</td>\n      <td>0.10</td>\n      <td>0.435</td>\n      <td>0.10</td>\n      <td>0.57</td>\n      <td>0.49</td>\n      <td>2.95</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0400000US32</td>\n      <td>32</td>\n      <td>Nevada</td>\n      <td></td>\n      <td>109781.180</td>\n      <td>POLYGON ((-114.05060 37.00040, -114.04999 36.9...</td>\n      <td>49875</td>\n      <td>0.067</td>\n      <td>0.87</td>\n      <td>0.839</td>\n      <td>0.10</td>\n      <td>0.08</td>\n      <td>0.448</td>\n      <td>0.50</td>\n      <td>0.46</td>\n      <td>0.14</td>\n      <td>2.11</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nalt.Chart(geo_states, title='PRE-election Hate crime per 100k').mark_geoshape().encode(\n    color='avg_hatecrimes_per_100k_fbi',\n    tooltip=['NAME', 'avg_hatecrimes_per_100k_fbi']\n).properties(\n    width=500,\n    height=300\n).project(\n    type='albersUsa'\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\nalt.Chart(...)\n```\n:::\n:::\n\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nalt.Chart(geo_states, title='POST-election Hate crime per 100k').mark_geoshape().encode(\n    color='hate_crimes_per_100k_splc',\n    tooltip=['NAME', 'hate_crimes_per_100k_splc']\n).properties(\n    width=500,\n    height=300\n).project(\n    type='albersUsa'\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\nalt.Chart(...)\n```\n:::\n:::\n\n\n### Exploring data\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nimport seaborn as sns\nsns.pairplot(data = df.iloc[:,1:])\n```\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab7-Part1_files/figure-pdf/cell-15-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\ndf.boxplot(column=['median_household_income'])\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\n<Axes: >\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab7-Part1_files/figure-pdf/cell-16-output-2.pdf){fig-pos='H'}\n:::\n:::\n\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\ndf.boxplot(column=['avg_hatecrimes_per_100k_fbi'])\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\n<Axes: >\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab7-Part1_files/figure-pdf/cell-17-output-2.pdf){fig-pos='H'}\n:::\n:::\n\n\nWe may want to drop columns (remove them). Details are [here](https://chrisalbon.com/python/data_wrangling/pandas_dropping_column_and_rows/).\n\nLet us drop Hawaii.\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\ndf[df.NAME == 'Hawaii']\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>NAME</th>\n      <th>median_household_income</th>\n      <th>share_unemployed_seasonal</th>\n      <th>share_population_in_metro_areas</th>\n      <th>share_population_with_high_school_degree</th>\n      <th>share_non_citizen</th>\n      <th>share_white_poverty</th>\n      <th>gini_index</th>\n      <th>share_non_white</th>\n      <th>share_voters_voted_trump</th>\n      <th>hate_crimes_per_100k_splc</th>\n      <th>avg_hatecrimes_per_100k_fbi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>11</th>\n      <td>Hawaii</td>\n      <td>71223</td>\n      <td>0.034</td>\n      <td>0.76</td>\n      <td>0.904</td>\n      <td>0.08</td>\n      <td>0.07</td>\n      <td>0.433</td>\n      <td>0.81</td>\n      <td>0.3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\ndf = df.drop(df.index[11])\n```\n:::\n\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\ndf.describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>median_household_income</th>\n      <th>share_unemployed_seasonal</th>\n      <th>share_population_in_metro_areas</th>\n      <th>share_population_with_high_school_degree</th>\n      <th>share_non_citizen</th>\n      <th>share_white_poverty</th>\n      <th>gini_index</th>\n      <th>share_non_white</th>\n      <th>share_voters_voted_trump</th>\n      <th>hate_crimes_per_100k_splc</th>\n      <th>avg_hatecrimes_per_100k_fbi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>50.000000</td>\n      <td>50.000000</td>\n      <td>50.000000</td>\n      <td>50.000000</td>\n      <td>47.000000</td>\n      <td>50.000000</td>\n      <td>50.000000</td>\n      <td>50.000000</td>\n      <td>50.00000</td>\n      <td>50.000000</td>\n      <td>50.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>54903.620000</td>\n      <td>0.049880</td>\n      <td>0.750000</td>\n      <td>0.868420</td>\n      <td>0.054043</td>\n      <td>0.092200</td>\n      <td>0.454180</td>\n      <td>0.305800</td>\n      <td>0.49380</td>\n      <td>0.281200</td>\n      <td>2.363200</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>9010.994814</td>\n      <td>0.010571</td>\n      <td>0.183425</td>\n      <td>0.034049</td>\n      <td>0.031184</td>\n      <td>0.024767</td>\n      <td>0.020889</td>\n      <td>0.150551</td>\n      <td>0.11674</td>\n      <td>0.255779</td>\n      <td>1.714502</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>35521.000000</td>\n      <td>0.028000</td>\n      <td>0.310000</td>\n      <td>0.799000</td>\n      <td>0.010000</td>\n      <td>0.040000</td>\n      <td>0.419000</td>\n      <td>0.060000</td>\n      <td>0.04000</td>\n      <td>0.000000</td>\n      <td>0.260000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>48358.500000</td>\n      <td>0.042250</td>\n      <td>0.630000</td>\n      <td>0.839750</td>\n      <td>0.030000</td>\n      <td>0.080000</td>\n      <td>0.440000</td>\n      <td>0.192500</td>\n      <td>0.42000</td>\n      <td>0.130000</td>\n      <td>1.290000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>54613.000000</td>\n      <td>0.051000</td>\n      <td>0.790000</td>\n      <td>0.874000</td>\n      <td>0.040000</td>\n      <td>0.090000</td>\n      <td>0.454500</td>\n      <td>0.275000</td>\n      <td>0.49500</td>\n      <td>0.215000</td>\n      <td>1.980000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>60652.750000</td>\n      <td>0.057750</td>\n      <td>0.897500</td>\n      <td>0.897750</td>\n      <td>0.080000</td>\n      <td>0.100000</td>\n      <td>0.466750</td>\n      <td>0.420000</td>\n      <td>0.57750</td>\n      <td>0.345000</td>\n      <td>3.182500</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>76165.000000</td>\n      <td>0.073000</td>\n      <td>1.000000</td>\n      <td>0.918000</td>\n      <td>0.130000</td>\n      <td>0.170000</td>\n      <td>0.532000</td>\n      <td>0.630000</td>\n      <td>0.70000</td>\n      <td>1.520000</td>\n      <td>10.950000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\ndf.plot(x = 'avg_hatecrimes_per_100k_fbi', y = 'median_household_income', kind='scatter')\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\n<Axes: xlabel='avg_hatecrimes_per_100k_fbi', ylabel='median_household_income'>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab7-Part1_files/figure-pdf/cell-21-output-2.pdf){fig-pos='H'}\n:::\n:::\n\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\ndf.plot(x = 'hate_crimes_per_100k_splc', y = 'median_household_income', kind='scatter')\n```\n\n::: {.cell-output .cell-output-display execution_count=21}\n```\n<Axes: xlabel='hate_crimes_per_100k_splc', ylabel='median_household_income'>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab7-Part1_files/figure-pdf/cell-22-output-2.pdf){fig-pos='H'}\n:::\n:::\n\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\ndf[df.hate_crimes_per_100k_splc > (np.std(df.hate_crimes_per_100k_splc) * 2.5)]\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>NAME</th>\n      <th>median_household_income</th>\n      <th>share_unemployed_seasonal</th>\n      <th>share_population_in_metro_areas</th>\n      <th>share_population_with_high_school_degree</th>\n      <th>share_non_citizen</th>\n      <th>share_white_poverty</th>\n      <th>gini_index</th>\n      <th>share_non_white</th>\n      <th>share_voters_voted_trump</th>\n      <th>hate_crimes_per_100k_splc</th>\n      <th>avg_hatecrimes_per_100k_fbi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8</th>\n      <td>District of Columbia</td>\n      <td>68277</td>\n      <td>0.067</td>\n      <td>1.00</td>\n      <td>0.871</td>\n      <td>0.11</td>\n      <td>0.04</td>\n      <td>0.532</td>\n      <td>0.63</td>\n      <td>0.04</td>\n      <td>1.52</td>\n      <td>10.95</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>Oregon</td>\n      <td>58875</td>\n      <td>0.062</td>\n      <td>0.87</td>\n      <td>0.891</td>\n      <td>0.07</td>\n      <td>0.10</td>\n      <td>0.449</td>\n      <td>0.26</td>\n      <td>0.41</td>\n      <td>0.83</td>\n      <td>3.39</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>Washington</td>\n      <td>59068</td>\n      <td>0.052</td>\n      <td>0.86</td>\n      <td>0.897</td>\n      <td>0.08</td>\n      <td>0.09</td>\n      <td>0.441</td>\n      <td>0.31</td>\n      <td>0.38</td>\n      <td>0.67</td>\n      <td>3.81</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\noutliers_df = df[df.hate_crimes_per_100k_splc > (np.std(df.hate_crimes_per_100k_splc) * 2.5)]\ndf.plot(x = 'hate_crimes_per_100k_splc', y = 'median_household_income', kind='scatter')\n\nplt.scatter(outliers_df.hate_crimes_per_100k_splc, outliers_df.median_household_income ,c='red')\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```\n<matplotlib.collections.PathCollection at 0x1683a3950>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab7-Part1_files/figure-pdf/cell-24-output-2.pdf){fig-pos='H'}\n:::\n:::\n\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\ndf_pivot = df.pivot_table(index=['NAME'], values=['hate_crimes_per_100k_splc', 'avg_hatecrimes_per_100k_fbi', 'median_household_income'])\ndf_pivot\n\n##sort by values\n#df_pivot = pd.pivot_table(df, index=['state'], columns = ['hate_crimes_per_100k_splc'], fill_value=0)\n#df_pivot\n#df2 = df_pivot.reindex(df_pivot['hate_crimes_per_100k_splc'].sort_values(by='hate_crimes_per_100k_splc', ascending=False).index)\n\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>avg_hatecrimes_per_100k_fbi</th>\n      <th>hate_crimes_per_100k_splc</th>\n      <th>median_household_income</th>\n    </tr>\n    <tr>\n      <th>NAME</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Alabama</th>\n      <td>1.80</td>\n      <td>0.12</td>\n      <td>42278</td>\n    </tr>\n    <tr>\n      <th>Alaska</th>\n      <td>1.65</td>\n      <td>0.14</td>\n      <td>67629</td>\n    </tr>\n    <tr>\n      <th>Arizona</th>\n      <td>3.41</td>\n      <td>0.22</td>\n      <td>49254</td>\n    </tr>\n    <tr>\n      <th>Arkansas</th>\n      <td>0.86</td>\n      <td>0.06</td>\n      <td>44922</td>\n    </tr>\n    <tr>\n      <th>California</th>\n      <td>2.39</td>\n      <td>0.25</td>\n      <td>60487</td>\n    </tr>\n    <tr>\n      <th>Colorado</th>\n      <td>2.80</td>\n      <td>0.39</td>\n      <td>60940</td>\n    </tr>\n    <tr>\n      <th>Connecticut</th>\n      <td>3.77</td>\n      <td>0.33</td>\n      <td>70161</td>\n    </tr>\n    <tr>\n      <th>Delaware</th>\n      <td>1.46</td>\n      <td>0.32</td>\n      <td>57522</td>\n    </tr>\n    <tr>\n      <th>District of Columbia</th>\n      <td>10.95</td>\n      <td>1.52</td>\n      <td>68277</td>\n    </tr>\n    <tr>\n      <th>Florida</th>\n      <td>0.69</td>\n      <td>0.18</td>\n      <td>46140</td>\n    </tr>\n    <tr>\n      <th>Georgia</th>\n      <td>0.41</td>\n      <td>0.12</td>\n      <td>49555</td>\n    </tr>\n    <tr>\n      <th>Idaho</th>\n      <td>1.89</td>\n      <td>0.12</td>\n      <td>53438</td>\n    </tr>\n    <tr>\n      <th>Illinois</th>\n      <td>1.04</td>\n      <td>0.19</td>\n      <td>54916</td>\n    </tr>\n    <tr>\n      <th>Indiana</th>\n      <td>1.75</td>\n      <td>0.24</td>\n      <td>48060</td>\n    </tr>\n    <tr>\n      <th>Iowa</th>\n      <td>0.56</td>\n      <td>0.45</td>\n      <td>57810</td>\n    </tr>\n    <tr>\n      <th>Kansas</th>\n      <td>2.14</td>\n      <td>0.10</td>\n      <td>53444</td>\n    </tr>\n    <tr>\n      <th>Kentucky</th>\n      <td>4.20</td>\n      <td>0.32</td>\n      <td>42786</td>\n    </tr>\n    <tr>\n      <th>Louisiana</th>\n      <td>1.34</td>\n      <td>0.10</td>\n      <td>42406</td>\n    </tr>\n    <tr>\n      <th>Maine</th>\n      <td>2.62</td>\n      <td>0.61</td>\n      <td>51710</td>\n    </tr>\n    <tr>\n      <th>Maryland</th>\n      <td>1.32</td>\n      <td>0.37</td>\n      <td>76165</td>\n    </tr>\n    <tr>\n      <th>Massachusetts</th>\n      <td>4.80</td>\n      <td>0.63</td>\n      <td>63151</td>\n    </tr>\n    <tr>\n      <th>Michigan</th>\n      <td>3.20</td>\n      <td>0.40</td>\n      <td>52005</td>\n    </tr>\n    <tr>\n      <th>Minnesota</th>\n      <td>3.61</td>\n      <td>0.62</td>\n      <td>67244</td>\n    </tr>\n    <tr>\n      <th>Mississippi</th>\n      <td>0.62</td>\n      <td>0.06</td>\n      <td>35521</td>\n    </tr>\n    <tr>\n      <th>Missouri</th>\n      <td>1.90</td>\n      <td>0.18</td>\n      <td>56630</td>\n    </tr>\n    <tr>\n      <th>Montana</th>\n      <td>2.95</td>\n      <td>0.49</td>\n      <td>51102</td>\n    </tr>\n    <tr>\n      <th>Nebraska</th>\n      <td>2.68</td>\n      <td>0.15</td>\n      <td>56870</td>\n    </tr>\n    <tr>\n      <th>Nevada</th>\n      <td>2.11</td>\n      <td>0.14</td>\n      <td>49875</td>\n    </tr>\n    <tr>\n      <th>New Hampshire</th>\n      <td>2.10</td>\n      <td>0.15</td>\n      <td>73397</td>\n    </tr>\n    <tr>\n      <th>New Jersey</th>\n      <td>4.41</td>\n      <td>0.07</td>\n      <td>65243</td>\n    </tr>\n    <tr>\n      <th>New Mexico</th>\n      <td>1.88</td>\n      <td>0.29</td>\n      <td>46686</td>\n    </tr>\n    <tr>\n      <th>New York</th>\n      <td>3.10</td>\n      <td>0.35</td>\n      <td>54310</td>\n    </tr>\n    <tr>\n      <th>North Carolina</th>\n      <td>1.26</td>\n      <td>0.24</td>\n      <td>46784</td>\n    </tr>\n    <tr>\n      <th>North Dakota</th>\n      <td>4.74</td>\n      <td>0.00</td>\n      <td>60730</td>\n    </tr>\n    <tr>\n      <th>Ohio</th>\n      <td>3.24</td>\n      <td>0.19</td>\n      <td>49644</td>\n    </tr>\n    <tr>\n      <th>Oklahoma</th>\n      <td>1.08</td>\n      <td>0.13</td>\n      <td>47199</td>\n    </tr>\n    <tr>\n      <th>Oregon</th>\n      <td>3.39</td>\n      <td>0.83</td>\n      <td>58875</td>\n    </tr>\n    <tr>\n      <th>Pennsylvania</th>\n      <td>0.43</td>\n      <td>0.28</td>\n      <td>55173</td>\n    </tr>\n    <tr>\n      <th>Rhode Island</th>\n      <td>1.28</td>\n      <td>0.09</td>\n      <td>58633</td>\n    </tr>\n    <tr>\n      <th>South Carolina</th>\n      <td>1.93</td>\n      <td>0.20</td>\n      <td>44929</td>\n    </tr>\n    <tr>\n      <th>South Dakota</th>\n      <td>3.30</td>\n      <td>0.00</td>\n      <td>53053</td>\n    </tr>\n    <tr>\n      <th>Tennessee</th>\n      <td>3.13</td>\n      <td>0.19</td>\n      <td>43716</td>\n    </tr>\n    <tr>\n      <th>Texas</th>\n      <td>0.75</td>\n      <td>0.21</td>\n      <td>53875</td>\n    </tr>\n    <tr>\n      <th>Utah</th>\n      <td>2.38</td>\n      <td>0.13</td>\n      <td>63383</td>\n    </tr>\n    <tr>\n      <th>Vermont</th>\n      <td>1.90</td>\n      <td>0.32</td>\n      <td>60708</td>\n    </tr>\n    <tr>\n      <th>Virginia</th>\n      <td>1.72</td>\n      <td>0.36</td>\n      <td>66155</td>\n    </tr>\n    <tr>\n      <th>Washington</th>\n      <td>3.81</td>\n      <td>0.67</td>\n      <td>59068</td>\n    </tr>\n    <tr>\n      <th>West Virginia</th>\n      <td>2.03</td>\n      <td>0.32</td>\n      <td>39552</td>\n    </tr>\n    <tr>\n      <th>Wisconsin</th>\n      <td>1.12</td>\n      <td>0.22</td>\n      <td>58080</td>\n    </tr>\n    <tr>\n      <th>Wyoming</th>\n      <td>0.26</td>\n      <td>0.00</td>\n      <td>55690</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\ndf_pivot.sort_values(by=['avg_hatecrimes_per_100k_fbi'], ascending=False)\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>avg_hatecrimes_per_100k_fbi</th>\n      <th>hate_crimes_per_100k_splc</th>\n      <th>median_household_income</th>\n    </tr>\n    <tr>\n      <th>NAME</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>District of Columbia</th>\n      <td>10.95</td>\n      <td>1.52</td>\n      <td>68277</td>\n    </tr>\n    <tr>\n      <th>Massachusetts</th>\n      <td>4.80</td>\n      <td>0.63</td>\n      <td>63151</td>\n    </tr>\n    <tr>\n      <th>North Dakota</th>\n      <td>4.74</td>\n      <td>0.00</td>\n      <td>60730</td>\n    </tr>\n    <tr>\n      <th>New Jersey</th>\n      <td>4.41</td>\n      <td>0.07</td>\n      <td>65243</td>\n    </tr>\n    <tr>\n      <th>Kentucky</th>\n      <td>4.20</td>\n      <td>0.32</td>\n      <td>42786</td>\n    </tr>\n    <tr>\n      <th>Washington</th>\n      <td>3.81</td>\n      <td>0.67</td>\n      <td>59068</td>\n    </tr>\n    <tr>\n      <th>Connecticut</th>\n      <td>3.77</td>\n      <td>0.33</td>\n      <td>70161</td>\n    </tr>\n    <tr>\n      <th>Minnesota</th>\n      <td>3.61</td>\n      <td>0.62</td>\n      <td>67244</td>\n    </tr>\n    <tr>\n      <th>Arizona</th>\n      <td>3.41</td>\n      <td>0.22</td>\n      <td>49254</td>\n    </tr>\n    <tr>\n      <th>Oregon</th>\n      <td>3.39</td>\n      <td>0.83</td>\n      <td>58875</td>\n    </tr>\n    <tr>\n      <th>South Dakota</th>\n      <td>3.30</td>\n      <td>0.00</td>\n      <td>53053</td>\n    </tr>\n    <tr>\n      <th>Ohio</th>\n      <td>3.24</td>\n      <td>0.19</td>\n      <td>49644</td>\n    </tr>\n    <tr>\n      <th>Michigan</th>\n      <td>3.20</td>\n      <td>0.40</td>\n      <td>52005</td>\n    </tr>\n    <tr>\n      <th>Tennessee</th>\n      <td>3.13</td>\n      <td>0.19</td>\n      <td>43716</td>\n    </tr>\n    <tr>\n      <th>New York</th>\n      <td>3.10</td>\n      <td>0.35</td>\n      <td>54310</td>\n    </tr>\n    <tr>\n      <th>Montana</th>\n      <td>2.95</td>\n      <td>0.49</td>\n      <td>51102</td>\n    </tr>\n    <tr>\n      <th>Colorado</th>\n      <td>2.80</td>\n      <td>0.39</td>\n      <td>60940</td>\n    </tr>\n    <tr>\n      <th>Nebraska</th>\n      <td>2.68</td>\n      <td>0.15</td>\n      <td>56870</td>\n    </tr>\n    <tr>\n      <th>Maine</th>\n      <td>2.62</td>\n      <td>0.61</td>\n      <td>51710</td>\n    </tr>\n    <tr>\n      <th>California</th>\n      <td>2.39</td>\n      <td>0.25</td>\n      <td>60487</td>\n    </tr>\n    <tr>\n      <th>Utah</th>\n      <td>2.38</td>\n      <td>0.13</td>\n      <td>63383</td>\n    </tr>\n    <tr>\n      <th>Kansas</th>\n      <td>2.14</td>\n      <td>0.10</td>\n      <td>53444</td>\n    </tr>\n    <tr>\n      <th>Nevada</th>\n      <td>2.11</td>\n      <td>0.14</td>\n      <td>49875</td>\n    </tr>\n    <tr>\n      <th>New Hampshire</th>\n      <td>2.10</td>\n      <td>0.15</td>\n      <td>73397</td>\n    </tr>\n    <tr>\n      <th>West Virginia</th>\n      <td>2.03</td>\n      <td>0.32</td>\n      <td>39552</td>\n    </tr>\n    <tr>\n      <th>South Carolina</th>\n      <td>1.93</td>\n      <td>0.20</td>\n      <td>44929</td>\n    </tr>\n    <tr>\n      <th>Vermont</th>\n      <td>1.90</td>\n      <td>0.32</td>\n      <td>60708</td>\n    </tr>\n    <tr>\n      <th>Missouri</th>\n      <td>1.90</td>\n      <td>0.18</td>\n      <td>56630</td>\n    </tr>\n    <tr>\n      <th>Idaho</th>\n      <td>1.89</td>\n      <td>0.12</td>\n      <td>53438</td>\n    </tr>\n    <tr>\n      <th>New Mexico</th>\n      <td>1.88</td>\n      <td>0.29</td>\n      <td>46686</td>\n    </tr>\n    <tr>\n      <th>Alabama</th>\n      <td>1.80</td>\n      <td>0.12</td>\n      <td>42278</td>\n    </tr>\n    <tr>\n      <th>Indiana</th>\n      <td>1.75</td>\n      <td>0.24</td>\n      <td>48060</td>\n    </tr>\n    <tr>\n      <th>Virginia</th>\n      <td>1.72</td>\n      <td>0.36</td>\n      <td>66155</td>\n    </tr>\n    <tr>\n      <th>Alaska</th>\n      <td>1.65</td>\n      <td>0.14</td>\n      <td>67629</td>\n    </tr>\n    <tr>\n      <th>Delaware</th>\n      <td>1.46</td>\n      <td>0.32</td>\n      <td>57522</td>\n    </tr>\n    <tr>\n      <th>Louisiana</th>\n      <td>1.34</td>\n      <td>0.10</td>\n      <td>42406</td>\n    </tr>\n    <tr>\n      <th>Maryland</th>\n      <td>1.32</td>\n      <td>0.37</td>\n      <td>76165</td>\n    </tr>\n    <tr>\n      <th>Rhode Island</th>\n      <td>1.28</td>\n      <td>0.09</td>\n      <td>58633</td>\n    </tr>\n    <tr>\n      <th>North Carolina</th>\n      <td>1.26</td>\n      <td>0.24</td>\n      <td>46784</td>\n    </tr>\n    <tr>\n      <th>Wisconsin</th>\n      <td>1.12</td>\n      <td>0.22</td>\n      <td>58080</td>\n    </tr>\n    <tr>\n      <th>Oklahoma</th>\n      <td>1.08</td>\n      <td>0.13</td>\n      <td>47199</td>\n    </tr>\n    <tr>\n      <th>Illinois</th>\n      <td>1.04</td>\n      <td>0.19</td>\n      <td>54916</td>\n    </tr>\n    <tr>\n      <th>Arkansas</th>\n      <td>0.86</td>\n      <td>0.06</td>\n      <td>44922</td>\n    </tr>\n    <tr>\n      <th>Texas</th>\n      <td>0.75</td>\n      <td>0.21</td>\n      <td>53875</td>\n    </tr>\n    <tr>\n      <th>Florida</th>\n      <td>0.69</td>\n      <td>0.18</td>\n      <td>46140</td>\n    </tr>\n    <tr>\n      <th>Mississippi</th>\n      <td>0.62</td>\n      <td>0.06</td>\n      <td>35521</td>\n    </tr>\n    <tr>\n      <th>Iowa</th>\n      <td>0.56</td>\n      <td>0.45</td>\n      <td>57810</td>\n    </tr>\n    <tr>\n      <th>Pennsylvania</th>\n      <td>0.43</td>\n      <td>0.28</td>\n      <td>55173</td>\n    </tr>\n    <tr>\n      <th>Georgia</th>\n      <td>0.41</td>\n      <td>0.12</td>\n      <td>49555</td>\n    </tr>\n    <tr>\n      <th>Wyoming</th>\n      <td>0.26</td>\n      <td>0.00</td>\n      <td>55690</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\n#This is code for standarization  \nfrom sklearn import preprocessing\nimport numpy as np\n\n#Get column names first\n#names = df.columns\n#df_stand = df[['median_household_income','share_unemployed_seasonal']]\ndf_stand = df[['median_household_income','share_unemployed_seasonal', 'share_population_in_metro_areas'\n               , 'share_population_with_high_school_degree', 'share_non_citizen', 'share_white_poverty', 'gini_index'\n               , 'share_non_white', 'share_voters_voted_trump', 'hate_crimes_per_100k_splc', 'avg_hatecrimes_per_100k_fbi']]\nnames = df_stand.columns\n#Create the Scaler object\nscaler = preprocessing.StandardScaler()\n#Fit your data on the scaler object\ndf2 = scaler.fit_transform(df_stand)\ndf2 = pd.DataFrame(df2, columns=names)\ndf2.tail()\n\n```\n\n::: {.cell-output .cell-output-display execution_count=26}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>median_household_income</th>\n      <th>share_unemployed_seasonal</th>\n      <th>share_population_in_metro_areas</th>\n      <th>share_population_with_high_school_degree</th>\n      <th>share_non_citizen</th>\n      <th>share_white_poverty</th>\n      <th>gini_index</th>\n      <th>share_non_white</th>\n      <th>share_voters_voted_trump</th>\n      <th>hate_crimes_per_100k_splc</th>\n      <th>avg_hatecrimes_per_100k_fbi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>45</th>\n      <td>1.261305</td>\n      <td>-0.657461</td>\n      <td>0.771002</td>\n      <td>-0.071795</td>\n      <td>0.193108</td>\n      <td>-0.905436</td>\n      <td>0.233085</td>\n      <td>0.497859</td>\n      <td>-0.379003</td>\n      <td>0.311206</td>\n      <td>-0.378961</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>0.466836</td>\n      <td>0.202590</td>\n      <td>0.605787</td>\n      <td>0.847894</td>\n      <td>0.841399</td>\n      <td>-0.089728</td>\n      <td>-0.637357</td>\n      <td>0.028181</td>\n      <td>-0.984716</td>\n      <td>1.535493</td>\n      <td>0.852428</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>-1.720951</td>\n      <td>2.209376</td>\n      <td>-1.101431</td>\n      <td>-1.199157</td>\n      <td>-1.427620</td>\n      <td>1.949543</td>\n      <td>-0.153778</td>\n      <td>-1.582146</td>\n      <td>1.697727</td>\n      <td>0.153233</td>\n      <td>-0.196315</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>0.356079</td>\n      <td>-0.657461</td>\n      <td>-0.330429</td>\n      <td>0.877562</td>\n      <td>-0.779329</td>\n      <td>-0.089728</td>\n      <td>-1.169293</td>\n      <td>-0.575692</td>\n      <td>-0.119412</td>\n      <td>-0.241698</td>\n      <td>-0.732470</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>0.088155</td>\n      <td>-0.944145</td>\n      <td>-2.423149</td>\n      <td>1.470910</td>\n      <td>-1.103475</td>\n      <td>-0.089728</td>\n      <td>-1.507798</td>\n      <td>-1.045370</td>\n      <td>1.784258</td>\n      <td>-1.110547</td>\n      <td>-1.239166</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\nax = sns.boxplot(data=df2, orient=\"h\")\n```\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab7-Part1_files/figure-pdf/cell-28-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\n#wanted to remove row with Hawaii (row nr 11) following https://chrisalbon.com/python/data_wrangling/pandas_dropping_column_and_rows/\n\ndf2 = df.copy()\ndf2\n#df2.drop('Hawaii')\n#df2.drop(11) #drop Hawaii row\ndf2.drop(df.index[11])\ndf2.tail()\n\n```\n\n::: {.cell-output .cell-output-display execution_count=28}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>NAME</th>\n      <th>median_household_income</th>\n      <th>share_unemployed_seasonal</th>\n      <th>share_population_in_metro_areas</th>\n      <th>share_population_with_high_school_degree</th>\n      <th>share_non_citizen</th>\n      <th>share_white_poverty</th>\n      <th>gini_index</th>\n      <th>share_non_white</th>\n      <th>share_voters_voted_trump</th>\n      <th>hate_crimes_per_100k_splc</th>\n      <th>avg_hatecrimes_per_100k_fbi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>46</th>\n      <td>Virginia</td>\n      <td>66155</td>\n      <td>0.043</td>\n      <td>0.89</td>\n      <td>0.866</td>\n      <td>0.06</td>\n      <td>0.07</td>\n      <td>0.459</td>\n      <td>0.38</td>\n      <td>0.45</td>\n      <td>0.36</td>\n      <td>1.72</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>Washington</td>\n      <td>59068</td>\n      <td>0.052</td>\n      <td>0.86</td>\n      <td>0.897</td>\n      <td>0.08</td>\n      <td>0.09</td>\n      <td>0.441</td>\n      <td>0.31</td>\n      <td>0.38</td>\n      <td>0.67</td>\n      <td>3.81</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>West Virginia</td>\n      <td>39552</td>\n      <td>0.073</td>\n      <td>0.55</td>\n      <td>0.828</td>\n      <td>0.01</td>\n      <td>0.14</td>\n      <td>0.451</td>\n      <td>0.07</td>\n      <td>0.69</td>\n      <td>0.32</td>\n      <td>2.03</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>Wisconsin</td>\n      <td>58080</td>\n      <td>0.043</td>\n      <td>0.69</td>\n      <td>0.898</td>\n      <td>0.03</td>\n      <td>0.09</td>\n      <td>0.430</td>\n      <td>0.22</td>\n      <td>0.48</td>\n      <td>0.22</td>\n      <td>1.12</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>Wyoming</td>\n      <td>55690</td>\n      <td>0.040</td>\n      <td>0.31</td>\n      <td>0.918</td>\n      <td>0.02</td>\n      <td>0.09</td>\n      <td>0.423</td>\n      <td>0.15</td>\n      <td>0.70</td>\n      <td>0.00</td>\n      <td>0.26</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\nimport scipy.stats\n#instead of running it one by one for every pair of variables, like:\n#scipy.stats.pearsonr(st_wine.quality.values, st_wine.alcohol.values) \n\ncorrMatrix = df2.corr(numeric_only=True).round(2)\nprint (corrMatrix)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                          median_household_income  \\\nmedian_household_income                                      1.00   \nshare_unemployed_seasonal                                   -0.34   \nshare_population_in_metro_areas                              0.29   \nshare_population_with_high_school_degree                     0.64   \nshare_non_citizen                                            0.28   \nshare_white_poverty                                         -0.82   \ngini_index                                                  -0.15   \nshare_non_white                                             -0.00   \nshare_voters_voted_trump                                    -0.57   \nhate_crimes_per_100k_splc                                    0.33   \navg_hatecrimes_per_100k_fbi                                  0.32   \n\n                                          share_unemployed_seasonal  \\\nmedian_household_income                                       -0.34   \nshare_unemployed_seasonal                                      1.00   \nshare_population_in_metro_areas                                0.37   \nshare_population_with_high_school_degree                      -0.61   \nshare_non_citizen                                              0.31   \nshare_white_poverty                                            0.19   \ngini_index                                                     0.53   \nshare_non_white                                                0.59   \nshare_voters_voted_trump                                      -0.21   \nhate_crimes_per_100k_splc                                      0.18   \navg_hatecrimes_per_100k_fbi                                    0.07   \n\n                                          share_population_in_metro_areas  \\\nmedian_household_income                                              0.29   \nshare_unemployed_seasonal                                            0.37   \nshare_population_in_metro_areas                                      1.00   \nshare_population_with_high_school_degree                            -0.27   \nshare_non_citizen                                                    0.75   \nshare_white_poverty                                                 -0.39   \ngini_index                                                           0.52   \nshare_non_white                                                      0.60   \nshare_voters_voted_trump                                            -0.58   \nhate_crimes_per_100k_splc                                            0.26   \navg_hatecrimes_per_100k_fbi                                          0.21   \n\n                                          share_population_with_high_school_degree  \\\nmedian_household_income                                                       0.64   \nshare_unemployed_seasonal                                                    -0.61   \nshare_population_in_metro_areas                                              -0.27   \nshare_population_with_high_school_degree                                      1.00   \nshare_non_citizen                                                            -0.30   \nshare_white_poverty                                                          -0.48   \ngini_index                                                                   -0.58   \nshare_non_white                                                              -0.56   \nshare_voters_voted_trump                                                     -0.13   \nhate_crimes_per_100k_splc                                                     0.21   \navg_hatecrimes_per_100k_fbi                                                   0.16   \n\n                                          share_non_citizen  \\\nmedian_household_income                                0.28   \nshare_unemployed_seasonal                              0.31   \nshare_population_in_metro_areas                        0.75   \nshare_population_with_high_school_degree              -0.30   \nshare_non_citizen                                      1.00   \nshare_white_poverty                                   -0.38   \ngini_index                                             0.51   \nshare_non_white                                        0.76   \nshare_voters_voted_trump                              -0.62   \nhate_crimes_per_100k_splc                              0.28   \navg_hatecrimes_per_100k_fbi                            0.30   \n\n                                          share_white_poverty  gini_index  \\\nmedian_household_income                                 -0.82       -0.15   \nshare_unemployed_seasonal                                0.19        0.53   \nshare_population_in_metro_areas                         -0.39        0.52   \nshare_population_with_high_school_degree                -0.48       -0.58   \nshare_non_citizen                                       -0.38        0.51   \nshare_white_poverty                                      1.00        0.01   \ngini_index                                               0.01        1.00   \nshare_non_white                                         -0.24        0.59   \nshare_voters_voted_trump                                 0.54       -0.46   \nhate_crimes_per_100k_splc                               -0.26        0.38   \navg_hatecrimes_per_100k_fbi                             -0.26        0.42   \n\n                                          share_non_white  \\\nmedian_household_income                             -0.00   \nshare_unemployed_seasonal                            0.59   \nshare_population_in_metro_areas                      0.60   \nshare_population_with_high_school_degree            -0.56   \nshare_non_citizen                                    0.76   \nshare_white_poverty                                 -0.24   \ngini_index                                           0.59   \nshare_non_white                                      1.00   \nshare_voters_voted_trump                            -0.44   \nhate_crimes_per_100k_splc                            0.12   \navg_hatecrimes_per_100k_fbi                          0.08   \n\n                                          share_voters_voted_trump  \\\nmedian_household_income                                      -0.57   \nshare_unemployed_seasonal                                    -0.21   \nshare_population_in_metro_areas                              -0.58   \nshare_population_with_high_school_degree                     -0.13   \nshare_non_citizen                                            -0.62   \nshare_white_poverty                                           0.54   \ngini_index                                                   -0.46   \nshare_non_white                                              -0.44   \nshare_voters_voted_trump                                      1.00   \nhate_crimes_per_100k_splc                                    -0.69   \navg_hatecrimes_per_100k_fbi                                  -0.50   \n\n                                          hate_crimes_per_100k_splc  \\\nmedian_household_income                                        0.33   \nshare_unemployed_seasonal                                      0.18   \nshare_population_in_metro_areas                                0.26   \nshare_population_with_high_school_degree                       0.21   \nshare_non_citizen                                              0.28   \nshare_white_poverty                                           -0.26   \ngini_index                                                     0.38   \nshare_non_white                                                0.12   \nshare_voters_voted_trump                                      -0.69   \nhate_crimes_per_100k_splc                                      1.00   \navg_hatecrimes_per_100k_fbi                                    0.68   \n\n                                          avg_hatecrimes_per_100k_fbi  \nmedian_household_income                                          0.32  \nshare_unemployed_seasonal                                        0.07  \nshare_population_in_metro_areas                                  0.21  \nshare_population_with_high_school_degree                         0.16  \nshare_non_citizen                                                0.30  \nshare_white_poverty                                             -0.26  \ngini_index                                                       0.42  \nshare_non_white                                                  0.08  \nshare_voters_voted_trump                                        -0.50  \nhate_crimes_per_100k_splc                                        0.68  \navg_hatecrimes_per_100k_fbi                                      1.00  \n```\n:::\n:::\n\n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\nimport pandas as pd\nimport seaborn as sn\nimport matplotlib.pyplot as plt\n\ncorrMatrix = df2.corr(numeric_only=True).round(1)  #I added here \".round(1)\" so that's easier to read given number of variables\nsn.heatmap(corrMatrix, annot=True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab7-Part1_files/figure-pdf/cell-31-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n::: {.cell execution_count=31}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\n\nx = df2[['median_household_income', 'share_population_with_high_school_degree', 'share_voters_voted_trump']]\ny = df2[['avg_hatecrimes_per_100k_fbi']]\n#what if we change the y variable\n#y = df2[['hate_crimes_per_100k_splc']]\n\nest = LinearRegression(fit_intercept = True) \nest.fit(x, y)\n\nprint(\"Coefficients:\", est.coef_)\nprint (\"Intercept:\", est.intercept_)\n\nmodel = LinearRegression()\nmodel.fit(x, y)\ny_hat = model.predict(x)\nprint (\"MSE:\", metrics.mean_squared_error(y, y_hat))\nprint (\"R^2:\", metrics.r2_score(y, y_hat))\nprint (\"var:\", y.var())\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCoefficients: [[-1.63935828e-05  7.65352737e+00 -7.85302986e+00]]\nIntercept: [0.49461694]\nMSE: 2.1105276140605045\nR^2: 0.26736253642536767\nvar: avg_hatecrimes_per_100k_fbi    2.939516\ndtype: float64\n```\n:::\n:::\n\n\n",
    "supporting": [
      "IM939_Lab7-Part1_files/figure-pdf"
    ],
    "filters": []
  }
}