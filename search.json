[
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Data Science Across Disciplines",
    "section": "Welcome",
    "text": "Welcome\nThis is the handbook for IM939: Data Science Across Disciplines, taught at the Centre for Interdisciplinary Methodologies at the University of Warwick and taught1 by Cagatay Turkay (Module leader), Kavin Narasimhan and Carlos Cámara-Menoyo1 For a comprehensive list of past and current staff members, refer to Section 1\n\n\n\nIM939 Logo"
  },
  {
    "objectID": "index.html#what-this-module-is-about",
    "href": "index.html#what-this-module-is-about",
    "title": "Data Science Across Disciplines",
    "section": "What this module is about?",
    "text": "What this module is about?\nThis module introduces students to the fundamental techniques, concepts and contemporary discussions across the broad field of data science. With data and data related artefacts becoming ubiquitous in all aspects of social life, data science gains access to new sources of data, is taken up across an expanding range of research fields and disciplines, and increasingly engages with societal challenges. The module provides an advanced introduction to the theoretical and scientific frameworks of data science, and to the fundamental techniques for working with data using appropriate procedures, algorithms and visualisation.\nStudents learn how to critically approach data and data-driven artefacts, and engage with and critically reflect on contemporary discussions around the practice of data science, its compatibility with different analytics frameworks and disciplinary, and its relation to on-going digital transformations of society. As well as lectures discussing the theoretical, scientific and ethical frameworks of data science, the module features coding labs and workshops that expose students to the practice of working effectively with data, algorithms, and analytical techniques, as well as providing a platform for reflective and critical discussions on data science practices, resulting data artefacts and how they can be interpreted, actioned and influence society."
  },
  {
    "objectID": "index.html#course-contents",
    "href": "index.html#course-contents",
    "title": "Data Science Across Disciplines",
    "section": "Course contents",
    "text": "Course contents\n\nIntroduction and historical perspectives (Chapter 1)\nThis week discusses data science as a field that cuts across disciplines and provides a historical perspective on the subject. We discuss the terms Data Science and Data Scientists, reflect on examples of Data Science projects, and discuss the research process at a methodological level.\n\n\nThinking Data: Theoretical and Practical Concerns\nThis week explores the cultural, ethical, and critical challenges posed by data artefacts and data-intensive scientific processes. Engaging with Critical Data Studies, we discuss issues around data capture, curation, data quality, inclusion/exclusion and representativeness. The session also discusses the different kinds of data that one can encounter across disciplines, the underlying characteristics of data and how we can analytically and practically approach data quality issues and the challenge of identifying and curating appropriate data sets.\n\n\nAbstractions and Models\nThis week discusses ways of abstracting data. We start by visiting statistics as a means of representing data and its inherent characteristics. The session moves on to discuss the notion of a “model” and visit the different schools of thought within model-ing, as well as a tour of fundamental statistical models that help abstract data and its inherent relations.\n\n\nStructures and Spaces\nThis week explores the notion of structures and how data science can enable the extraction of “hidden” underlying groups – clusters – and hierarchical structures from data. We discuss the different techniques to surface and generate artificial boundaries and how the resulting artefacts can be interpreted. This session then investigates how artificial and abstract spaces can be constructed through different “projection” techniques, and how these spaces help us navigate data that are high-dimensional in nature and apply analytic frameworks to them.\n\n\nMulti-model Thinking and Rigour in Data Science\nThis week we focus on multi-model approaches as a way of thinking and how critical, pluralistic thinking can improve our understanding of the underlying phenomena implicit in data. We also discuss how to adopt a comprehensive approach to the data science process, and investigate indicators of rigour in data science.\n\n\nRecognising and Avoiding Traps\nData analysis and statistical routines and procedures are ingrained with several pitfalls and limitations – these range from methodological pitfalls in the processes and data that once can use, to cognitive and behavioural pitfalls that one can come across in making inferences from data and data artefacts. This week we discuss such theoretical and practical traps and pitfalls, how we can be aware of them and what approaches we have to avoid them.\n\n\nData Science and Society\nWe will engage with academic and practices discourse on the social, cultural and ethical aspects of data science, and discuss around how one can responsibly carry out data science research on social phenomena, whether data science can be a transformative power in society, and what ethical and social frameworks can help us to critically approach data science practices and its effects on society, and what are ethical practices for data scientists.\n\n\nData Science Workshop 1 (Design Thinking in Data Science)\nThis week explores the question “Can we approach data science as a design problem?” and discusses how one can embrace a user-centred approach to design appropriate data science processes. We will do this through hands-on practical where we go through the data science process over an applied case.\n\n\nData Science Workshop 2\nThis workshop week will involve you working hands-on towards your final assessments individually but in small coding groups. In the workshop session, we would like to hear from you on your ideas for your second coursework. You will also be able to use this session to start working on the dataset(s) that you have decided (or still considering) to analyse as part of your second assessment and discuss with your peers and with the staff members."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Data Science Across Disciplines",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis handbook has been created by Carlos Cámara-Menoyo and Cagatay Turkay, based on the materials from previous years created by Cagatay Turkay and James Tripp."
  },
  {
    "objectID": "index.html#licence",
    "href": "index.html#licence",
    "title": "Data Science Across Disciplines",
    "section": "Licence",
    "text": "Licence"
  },
  {
    "objectID": "content/about/teaching_staff.html#sec-staff-all",
    "href": "content/about/teaching_staff.html#sec-staff-all",
    "title": "Teaching Staff",
    "section": "Present and former staff",
    "text": "Present and former staff\nThis module has also been taught by the following people in the past (in alphabetical order):\n\nCagatay Turkay, Professor (202X-Present)\nCarlos Cámara-Menoyo, Senior Research Software Engineer (2022-Present)\nJames Tripp, Senior Research Software Engineer (2020-2022)\nKavin Narasimhan, Assistant Professor (2023-Present)\nMaria Petrescu, Teaching Assistant (XX-XXX)\nYulu Pi, Teaching Assistant (2022-2023)\nZofia Bednarowska-Michaiel, Teaching Fellow (2021-2022)\n\nWe would like to thank them all for their contributions to the module, some of which are also reflected in these materials."
  },
  {
    "objectID": "content/about/im939.html",
    "href": "content/about/im939.html",
    "title": "About the module",
    "section": "",
    "text": "Warning\n\n\n\nThis is a placeholder for https://cagatayturkay.github.io/data-science-across-disciplines/furtherInformation.html"
  },
  {
    "objectID": "content/about/teaching_materials.html#handbook",
    "href": "content/about/teaching_materials.html#handbook",
    "title": "Teaching materials",
    "section": "Handbook",
    "text": "Handbook\nExplain that it uses quarto and uses a lot of jupyter notebooks.\nExplain what jupyter notebooks are. Probably reference to skills programme?"
  },
  {
    "objectID": "content/about/teaching_materials.html#moodle",
    "href": "content/about/teaching_materials.html#moodle",
    "title": "Teaching materials",
    "section": "Moodle",
    "text": "Moodle"
  },
  {
    "objectID": "content/about/teaching_materials.html#git-repo",
    "href": "content/about/teaching_materials.html#git-repo",
    "title": "Teaching materials",
    "section": "Git repo?",
    "text": "Git repo?"
  },
  {
    "objectID": "content/sessions/session-01.html",
    "href": "content/sessions/session-01.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Warning\n\n\n\nThis page is still a stub\n\n\nThis is a placeholder for this content: https://cagatayturkay.github.io/data-science-across-disciplines/Sessions/session-01.html"
  },
  {
    "objectID": "content/labs/Lab_1/IM939_Lab_1_1.html#technological-stack",
    "href": "content/labs/Lab_1/IM939_Lab_1_1.html#technological-stack",
    "title": "2  Lab: Introduction to Python",
    "section": "2.1 Technological stack",
    "text": "2.1 Technological stack\nThis module uses a combination of different technologies (technological stack) besides python. You will need to be familiar with all their components to follow the labs as well as the assignments. You can find a brief description of the main components below, and you can refer to Appendix A for a more technical explanation of the setup.\n\n\n\n\n\n\nImportant\n\n\n\nPlease make sure you have Anaconda Python installed and running on your computer. Instructions for doing this can be found on Appendix A.\n\n\n\n2.1.1 Python\nPython is a very popular general purpose programming language. Data scientists use it to clean up, analyse and visualise data. A major strength of python is that the core Python functionality can be extended using libraries. In future labs, you will learn about popular data science libraries such as pandas and numpy.\nIt is useful to think of programming languages as a structured way to tell the computer what to do. We cover some of the basic features of Python in this lab.\n\n\n2.1.2 Anaconda\nAnaconda1 is a distribution platform that manages and installs many tools used in data science, such as programming languages (python), libraries, IDEs2… as well as a GUI (Anaconda navigator) and a CLI (Anaconda interpreter). Anaconda does some other useful things, such as creating isolated virtual environments, which we will be using for this module.1 If you want to know more about Anaconda, these tutorials can be a good start: https://www.tangolearn.com/what-is-anaconda/, https://www.upgrad.com/blog/python-anaconda-tutorial/2 Some specific Integrated Development Environments (IDEs) for Python included in Anaconda are VS Code, Jupyterlab and Spyder. In this module, there’s no preferred IDE (actually, different members of the staff use different IDEs) and you can use the one you are more familiar with.\n\n\n\n\n\n\nVirtual environments\n\n\n\nVirtual environments are a way to install all the dependencies (and their right version) required for a certain project by isolating python and libraries’ specific versions. Every person who recreatesthe virtual environment will be using the same packages and versions, reducing errors and increasing reproducibility. While they are considered an advanced practice, and are therefore out of scope of this course, you may want to learn about Python’s environments here: https://realpython.com/python-virtual-environments-a-primer/\n\n\n\n\n2.1.3 Jupter Notebooks\nJupyter notebooks, such as this one, allow you to combine text and code into documents you can edit in the browser. The power of these notebooks is in documenting or describing what you are doing with the code alongside that code. For example, you could detail why you chose a particular clustering algorithm above the clustering code itself. In other words, it add narrative and helps clarify your workflow.\n\n\n\nThis same notebook, as displayed within Jupyterlab"
  },
  {
    "objectID": "content/labs/Lab_1/IM939_Lab_1_1.html#getting-started",
    "href": "content/labs/Lab_1/IM939_Lab_1_1.html#getting-started",
    "title": "2  Lab: Introduction to Python",
    "section": "2.2 Getting started",
    "text": "2.2 Getting started\nIf you send Python a number then it will print that number for you.\n\n45\n\n45\n\n\nYou will see both the input and output displayed. The input will have a label next to it like ‘In [1]’ where the number tells you how much code has already been sent to the Python interpreter (the programming interpreting the Python code and returnign the result). A line such as ‘In [100]’ tells you that 99 code cells have been passed to the Python interpreter in the current session.\nPython can carry out simple arithetic.\n\n44 + 87\n\n131\n\n\n\n188 / 12\n\n15.666666666666666\n\n\n\n46 - 128\n\n-82\n\n\nEach time the code in the cell is run and the result from the Python interpreter is displayed."
  },
  {
    "objectID": "content/labs/Lab_1/IM939_Lab_1_1.html#data-types",
    "href": "content/labs/Lab_1/IM939_Lab_1_1.html#data-types",
    "title": "2  Lab: Introduction to Python",
    "section": "2.3 Data Types",
    "text": "2.3 Data Types\nAs we saw at this unit in the Skills Programme, programming languages use types to help them understand what a piece of data might represent. Knowing how data types work is important because they define what can be done and cannot be done with them. Each programming language has different data types which can be extended by other libraries such as Pandas, but these are some of the most frequent ones (and the ones we will be facing more frequently).\n\n2.3.1 int, floats, strings\nIntegers are whole numbers, like the ones we used above. We can check an object’s data type using type():\n\ntype(33)\n\nint\n\n\nYou can also have floats (numbers with decimal points)\n\n33.4\n\n33.4\n\n\n\ntype(33.4)\n\nfloat\n\n\nand a series of characters (strings).\n\n'I have a plan, sir.'\n\n'I have a plan, sir.'\n\n\n\ntype('I have a plan, sir.')\n\nstr\n\n\nData types are great and operators such as * do different things depending on the data type. For instance,\n\n33 * 3\n\n99\n\n\nThat seems quite sensible. What about if we had a string? Run teh below line. What is the * doing?\n\n'I have a plan, sir' * 3\n\n'I have a plan, sirI have a plan, sirI have a plan, sir'\n\n\nThere are also operators which only work with particular data types.\n\n'I have a plan, sir.' / 2\n\nTypeError: unsupported operand type(s) for /: 'str' and 'int'\n\n\nThis error message is very informative indeed. It tells us the line which caused the problem and that we have an error. Specifically, our error is a TypeError.\n\n\n\n\n\n\nUnderstanding the error\n\n\n\nIn this case, it says that the line 'I have a cunning plan' / 2 consists of string / int. We are trying to divide a string and int. The / operand is not able to divide a string by an int.\n\n\n\n\n2.3.2 lists and dictionaries\nYou can collect multiple values in a list. This is how they look like:\n\n[35, 'brown', 'yes']\n\n[35, 'brown', 'yes']\n\n\nAnd we can check theyr type:\n\ntype([35, 'brown', 'yes'])\n\nlist\n\n\nOr add keys to the values as a dictionary.\n\n{'age':35, 'hair colour': 'brown', 'Glasses': 'yes'}\n\n{'age': 35, 'hair colour': 'brown', 'Glasses': 'yes'}\n\n\n\ntype({'age':35, 'hair colour': 'brown', 'Glasses': 'yes'})\n\ndict"
  },
  {
    "objectID": "content/labs/Lab_1/IM939_Lab_1_1.html#variables",
    "href": "content/labs/Lab_1/IM939_Lab_1_1.html#variables",
    "title": "2  Lab: Introduction to Python",
    "section": "2.4 Variables",
    "text": "2.4 Variables\nVariables are bins for storing things in. These things can be data types. For example, the below creates a variable called my_height and stores the in 140 there.\n\nmy_height = 140\n\nThe Python interpreter is now storing the int 140 in a bin called my_height. If you pass the variable name to the interpreter then it will behave just like if you typed in 140 instead.\n\nmy_height\n\n140\n\n\n140\nVariables are neat when it comes to lists.\n\nmy_heights = [231, 234, 43]\nmy_heights\n\n[231, 234, 43]\n\n\n\nmy_heights[1]\n\n234\n\n\nWait, what happened above? What do you think the [1] does?\nYou can index multiple values from a list.\n\nmy_heights[0:2]\n\n[231, 234]"
  },
  {
    "objectID": "content/labs/Lab_1/IM939_Lab_1_1.html#bringing-it-all-together",
    "href": "content/labs/Lab_1/IM939_Lab_1_1.html#bringing-it-all-together",
    "title": "2  Lab: Introduction to Python",
    "section": "2.5 Bringing it all together",
    "text": "2.5 Bringing it all together\nWhat does the below do?\n\nradius = 40\npi = 3.1415\ncircle_area = pi * (radius * radius)\n\nlength = 12\nsquare_area = length * length\n\nmy_areas = [circle_area, square_area]\n\n\nmy_areas\n\n[5026.400000000001, 144]\n\n\nAs an aside, you can include comments which are not evaluated by the Python interpreter.\n\n# this is a comment. Python will ignore it.\n# another comment. \nn = 33"
  },
  {
    "objectID": "content/labs/Lab_1/IM939_Lab_1_1.html#congratulations",
    "href": "content/labs/Lab_1/IM939_Lab_1_1.html#congratulations",
    "title": "2  Lab: Introduction to Python",
    "section": "2.6 Congratulations",
    "text": "2.6 Congratulations\nYou’ve reached the end of the first notebook. We’ve looked at basic data type and variables. These are key components of all programming languages and a key part of working with data.\nIn the next notebook we will examine using libraries, loading in data, loops and functions."
  },
  {
    "objectID": "content/labs/Lab_1/IM939_Lab_1_2.html#functions",
    "href": "content/labs/Lab_1/IM939_Lab_1_2.html#functions",
    "title": "3  Lab: Repetition",
    "section": "3.1 Functions",
    "text": "3.1 Functions\nHow do we get the length of a list?\n\nlunch_ingredients = ['gravy', 'potatoes', 'meat', 'veg']\nlen(lunch_ingredients)\n\n4\n\n\nWe used a function. The syntax for a function is function_name(argument1, argument2). In our above example we passed the list lunch_ingrediants as an argument to the len function.\n\njames = {'age':35, 'hair colour': 'brown', 'Glasses': 'yes'}\n\n\nlen(james)\n\n3\n\n\nOne very useful function is to check a data type. In base R it may be obvious but when you move to libraries, with different data types, it may be worth checking.\n\ntype(james)\n\ndict\n\n\n\ntype(lunch_ingredients)\n\nlist\n\n\n\ntype(55)\n\nint"
  },
  {
    "objectID": "content/labs/Lab_1/IM939_Lab_1_2.html#methods",
    "href": "content/labs/Lab_1/IM939_Lab_1_2.html#methods",
    "title": "3  Lab: Repetition",
    "section": "3.2 Methods",
    "text": "3.2 Methods\nWhat is really happening? Well all data types in Python have functions built in. A variable is an object (a string, a list, an int, etc.). Each object has a set of methods do stuff with that object.\nFor example, I can add a value to the lunch_ingredients list liks so\n\nlunch_ingredients.append('yorkshire pudding')\nlunch_ingredients\n\n['gravy', 'potatoes', 'meat', 'veg', 'yorkshire pudding']\n\n\nNote the objectname.method notation. By adding a yorkshire pudding, a fine addition to any meal, my lunch ingredients now number 5.\nI can count these easily.\n\nlen(lunch_ingredients)\n\n5\n\n\nWhich is the same as the following.\n\nlunch_ingredients.__len__()\n\n5\n\n\nYou can tab complete to see what methods are available for a given object. There are quite a few built in python functions.\nHowever, do be aware that different objects will likely have different methods. For instance, can we get a length of an int?\n\nlen(4)\n\nTypeError: object of type 'int' has no len()\n\n\nThe error “TypeError: object of type ‘int’ has no len()” essentially means an int object has no len dunder method.\nTo see the methods for a given object, type in the object name in a code cell, add a period ‘.’ and press tab."
  },
  {
    "objectID": "content/labs/Lab_1/IM939_Lab_1_2.html#flow-control",
    "href": "content/labs/Lab_1/IM939_Lab_1_2.html#flow-control",
    "title": "3  Lab: Repetition",
    "section": "3.3 Flow control",
    "text": "3.3 Flow control\n\n3.3.1 For loops\nYou may need to repeat some code (such as reading lines in a csv file). For loops allow you to repeat code but with a variable changing each loop.\nFor instance,\n\nfor ingredient in lunch_ingredients:\n    print(ingredient)\n    \n\ngravy\npotatoes\nmeat\nveg\nyorkshire pudding\n\n\nThe above loop went through each ingrediant and printed it.\nNote the syntax and spacing. The for statement finishes with a : and the next line has a tab. For loops have to be defined like that. If you miss out the tab then you will be shown an error. It does make the code more readable though.\n\nfor ingredient in lunch_ingredients:\n    print(ingredient)\n\ngravy\npotatoes\nmeat\nveg\nyorkshire pudding\n\n\nAt least the error is informative.\nWe can have more than one line in the loop.\n\nfor ingredient in lunch_ingredients:\n    print('I love to eat ' + ingredient)\n    print('food is great')\n    \n\nI love to eat gravy\nfood is great\nI love to eat potatoes\nfood is great\nI love to eat meat\nfood is great\nI love to eat veg\nfood is great\nI love to eat yorkshire pudding\nfood is great\n\n\n\n\n3.3.2 If statements\nAn if statement allows us to run different code depending on a condition.\n\nmy_num = 3\n\nif my_num &gt; 5:\n    print('Number is bigger than 5')\n    \nif my_num &lt; 5:\n    print('Number is less than 5')\n\nNumber is less than 5\n\n\nWe can include this in a for loop.\n\nfor i in range(10): #go through each number 0:10\n    print(i)\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\nfor i in range(10):\n    print(i)\n    if i == 5:\n        print('We found a 5!')\n\n0\n1\n2\n3\n4\n5\nWe found a 5!\n6\n7\n8\n9\n\n\nYou can also include an else part of your if statement.\n\nfor i in range(10):\n    print(i)\n    if i == 5:\n        print('We found a 5! :)')\n    else:\n        print('Not a 5. Boo hoo :(')\n\n0\nNot a 5. Boo hoo :(\n1\nNot a 5. Boo hoo :(\n2\nNot a 5. Boo hoo :(\n3\nNot a 5. Boo hoo :(\n4\nNot a 5. Boo hoo :(\n5\nWe found a 5! :)\n6\nNot a 5. Boo hoo :(\n7\nNot a 5. Boo hoo :(\n8\nNot a 5. Boo hoo :(\n9\nNot a 5. Boo hoo :("
  },
  {
    "objectID": "content/labs/Lab_1/IM939_Lab_1_3.html#modules",
    "href": "content/labs/Lab_1/IM939_Lab_1_3.html#modules",
    "title": "4  Lab: Libraries",
    "section": "4.1 Modules",
    "text": "4.1 Modules\nA module is a collection of code someone else has written (objects, methods, etc.). A library is a group of modules. In data science work you will use libraries such as numpy and scikit-learn. As we mentioned, Anaconda has many of these libraries built in and we do not need to install them separately.\nSometimes there are multiple ways to do something. Different libraries may have modules which have similiar functionality but slight differences.\nFor example, lets go through a comma seperated value file to make some data accessible in Python.\n\n4.1.1 CSV module\n\nimport csv\n\nThe import keyword loads in a module. Here it loads in the csv module from the inbuilt Python library (see the Python docs on the csv module).\nPlease put the Facebook.csv file in the same folder as this notebook for this bit.\nThe code for going through a csv file is a bit lengthy.\n\nwith open('data/Facebook.csv', mode = 'r', encoding = 'UTF-8') as csvfile:    # open up our csv file\n    reader = csv.reader(csvfile)                                         # create a reader object which looks at our csv file\n    for row in reader:                                                   # for each row the reader finds\n        print(row)                                                       # print out the current row\n\n['\\ufefftype', 'post_message', 'link', 'link_domain', 'post_published', 'likes_count_fb', 'comments_count_fb', 'reactions_count_fb', 'shares_count_fb', 'engagement_fb']\n['status', 'Don t forget to post your photographs from heritage weekend  of you having a good time on here please.', '', '', '2017-09-09T13:57:14+0000', '5', '1', '5', '0', '6']\n['status', 'If you want to learn about Historic Coventry this Saturday we will be carrying out a guided tour around the Medieval Wall of Coventry. Booking details below', '', '', '2017-08-31T17:36:47+0000', '7', '12', '7', '0', '19']\n['event', '3 more dates for September Tours of The Medieval City Wall Below. Saturday 16th  23  rd and 30th https://www.facebook.com/events/1935828209971181/?ti=icl', 'https://www.facebook.com/events/1935828209971181/', 'facebook.com', '2017-09-05T10:46:45+0000', '2', '0', '2', '0', '2']\n['link', 'http://www.coventrytelegraph.net/whats-on/arts-culture-news/majestic-medieval-hall-coventrys-best-13448249', 'http://www.coventrytelegraph.net/whats-on/arts-culture-news/majestic-medieval-hall-coventrys-best-13448249', 'coventrytelegraph.net', '2017-08-14T08:56:04+0000', '9', '2', '10', '4', '16']\n['status', 'Different angle of St Michaels', '', '', '2017-08-26T09:04:35+0000', '11', '0', '12', '0', '12']\n['link', 'The Heritage Open Days are taking place from 7-10 September  with venues opening their doors and giving access to areas not normally open to the public - and now is the time to book a tour. As in previous years  there are some very interesting and unique tours taking place.   Make sure you don‚Äôt miss out on these by checking booking details online and booking your place.  Tours that need pre booking include: Medieval City Wall; BBC Coventry and Warwickshire Radio; Priory Undercroft and Visitor Centre; King Henry VIII School; Coventry Solihull Waste Disposal Ltd; and Coventry Central Library behind the scenes tour.  A Heritage Open Days brochure will be available from Council buildings  The Herbert  Coventry Cathedral and libraries from Wednesday (23 August).  For more details  visit the website www.coventry.gov.uk/hod.', 'http://www.coventry.gov.uk/hod', 'coventry.gov.uk', '2017-08-21T10:26:52+0000', '10', '0', '10', '13', '23']\n['status', 'Hi thanks to be able to share here & be part of tnis group ; Good to kbow iof yt area as going down soon with Natiomal Trust centre from Glasgow', '', '', '2017-08-17T08:36:07+0000', '1', '8', '1', '0', '9']\n['link', 'http://www.coventrytelegraph.net/whats-on/whats-on-news/future-coventrys-albany-theatre-been-13477790', 'http://www.coventrytelegraph.net/whats-on/whats-on-news/future-coventrys-albany-theatre-been-13477790', 'coventrytelegraph.net', '2017-08-17T09:14:59+0000', '8', '1', '9', '1', '11']\n['photo', 'https://www.facebook.com/peter.garbett.9/posts/1769150796433812  Sad we are loosing this museum visit it whilst you can', 'https://www.facebook.com/photo.php?fbid=1280170205424760&set=gm.1921178978100268&type=3', 'facebook.com', '2017-08-10T11:49:47+0000', '2', '1', '2', '0', '3']\n['link', 'https://www.facebook.com/CoventryPVC/posts/859986554155927', 'http://www.crowdfunder.co.uk/coventry-priory', 'crowdfunder.co.uk', '2017-08-07T21:13:46+0000', '6', '1', '6', '0', '7']\n['photo', 'Looking good in the sunshine.. https://www.facebook.com/visitcoventry.net/posts/501006916915750', 'https://www.facebook.com/visitcoventry.net/photos/a.373019843047792.1073741829.361929627490147/501006773582431/?type=3', 'facebook.com', '2017-08-06T21:25:32+0000', '8', '0', '8', '1', '9']\n['status', 'Best unique Pubs Whats your favourite pubs and why? in Coventry and surrounding areas.', '', '', '2017-07-19T08:35:49+0000', '0', '7', '0', '0', '7']\n['status', 'Thank you for letting me join your group', '', '', '2017-07-19T14:30:58+0000', '1', '1', '1', '0', '2']\n['link', 'https://www.facebook.com/theprideofwillenhall/posts/1378187435629358', 'http://www.coventrytelegraph.net/whats-on/whats-on-news/worlds-longest-inflatable-obstacle-course-13346712', 'coventrytelegraph.net', '2017-07-19T08:23:55+0000', '0', '0', '0', '0', '0']\n['link', 'https://www.facebook.com/peter.garbett.9/posts/1740635729285319  Coventry shortlisted!', 'https://coventry2021.co.uk/news/?is_mobile=1', 'coventry2021.co.uk', '2017-07-15T18:04:34+0000', '2', '0', '2', '0', '2']\n['photo', 'Nice. https://www.facebook.com/visitcoventry.net/photos/a.363722627310847.1073741828.361929627490147/489764101373365/?type=3', 'https://www.facebook.com/visitcoventry.net/photos/a.363722627310847.1073741828.361929627490147/489764101373365/?type=3', 'facebook.com', '2017-07-13T22:06:20+0000', '3', '0', '3', '0', '3']\n['photo', 'I ve always wanted to have a nose down this passageway..  https://www.facebook.com/visitcoventry.net/posts/487218148294627', 'https://www.facebook.com/visitcoventry.net/photos/a.373019843047792.1073741829.361929627490147/487215948294847/?type=3', 'facebook.com', '2017-07-09T12:21:01+0000', '11', '1', '11', '0', '12']\n['link', 'http://www.coventrytelegraph.net/whats-on/music-nightlife-news/what-godiva-festival-weather-forecast-13275038', 'http://www.coventrytelegraph.net/whats-on/music-nightlife-news/what-godiva-festival-weather-forecast-13275038', 'coventrytelegraph.net', '2017-07-08T08:39:38+0000', '1', '0', '1', '0', '1']\n['video', 'Looks like a great idea.', 'https://www.facebook.com/stmarysguildhall/videos/10156233759843475/', 'facebook.com', '2017-07-07T15:28:24+0000', '7', '0', '9', '2', '11']\n['photo', 'Lovely pic.. https://www.facebook.com/visitcoventry.net/photos/a.363722627310847.1073741828.361929627490147/483673398649102/?type=3', 'https://www.facebook.com/visitcoventry.net/photos/a.363722627310847.1073741828.361929627490147/483673398649102/?type=3', 'facebook.com', '2017-07-02T08:41:08+0000', '15', '2', '16', '1', '19']\n['status', 'What is your favourite restaurant or eatery in Coventry area?', '', '', '2017-06-29T08:55:29+0000', '3', '35', '3', '0', '38']\n['photo', 'Interesting.. https://www.facebook.com/visitcoventry.net/photos/a.363722627310847.1073741828.361929627490147/481529385530170/?type=3', 'https://www.facebook.com/visitcoventry.net/photos/a.363722627310847.1073741828.361929627490147/481529385530170/?type=3', 'facebook.com', '2017-06-29T05:32:53+0000', '9', '0', '9', '0', '9']\n['link', 'https://www.facebook.com/BerkswellWindmill/posts/1322847697832828', 'https://www.facebook.com/BerkswellWindmill/posts/1322847697832828', 'facebook.com', '2017-06-26T18:24:47+0000', '0', '0', '0', '0', '0']\n['status', 'Just saw a post about people that made me think would almost be a great motto for our fabulous city.   Let go of what s gone. Be grateful for (and save) what remains. Look forward to what is coming. ', '', '', '2017-06-25T17:17:12+0000', '4', '2', '5', '0', '7']\n['event', 'Why not make a weekend of it? It s Jaguar Super Saturday! In aid of Air Ambulance  details on joining below  if you own a Jaguar! This year there will be a  timeline  of cars from oldest to newest  many built at Browns Lane!', 'https://www.facebook.com/events/201361447045208/', 'facebook.com', '2017-06-22T19:27:54+0000', '2', '0', '2', '0', '2']\n['event', ':) we have a wonderful collection of exciting Citroen motorcars at the Museum on Sunday  if you own a Citroen you re welcome to come along with your car 10-4pm it s FREE and you may be given a leaflet about the car club! Any Citroen can come along  old or new  thank you :)', 'https://www.facebook.com/events/362084697478113/', 'facebook.com', '2017-06-22T19:23:49+0000', '1', '0', '1', '0', '1']\n['event', 'City Wall Guided Costume Tour this Saturday meeting at 2pm at Priory Visitors Centre ( behind Nando s  Trinity Street) with the Deep Fat Friar https://www.facebook.com/events/313502545772268/?ti=icl', 'https://www.facebook.com/events/313502545772268/', 'facebook.com', '2017-06-22T19:20:55+0000', '6', '0', '6', '1', '7']\n['link', 'https://www.facebook.com/BerkswellWindmill/posts/1322847697832828', 'https://www.facebook.com/BerkswellWindmill/posts/1322847697832828', 'facebook.com', '2017-06-22T16:59:56+0000', '1', '0', '1', '0', '1']\n\n\nOk, each row looks like a list. But printing it out is pretty messy. We should dump this into a list of dictionaries (so we can pick out particular values).\n\ncsv_content = []                                   # create list to put our data into\n\nwith open('data/Facebook.csv', mode = 'r', encoding = 'UTF-8') as csvfile:  # open up csv file\n    reader = csv.DictReader(csvfile)                                   # create a reader which will read in each row and turn it into a dictionary\n    for row in reader:                                                 # for each row\n        csv_content.append(row) \n        print(row)                                                     # put the created dictionary into our list\n\n{'\\ufefftype': 'status', 'post_message': 'Don t forget to post your photographs from heritage weekend  of you having a good time on here please.', 'link': '', 'link_domain': '', 'post_published': '2017-09-09T13:57:14+0000', 'likes_count_fb': '5', 'comments_count_fb': '1', 'reactions_count_fb': '5', 'shares_count_fb': '0', 'engagement_fb': '6'}\n{'\\ufefftype': 'status', 'post_message': 'If you want to learn about Historic Coventry this Saturday we will be carrying out a guided tour around the Medieval Wall of Coventry. Booking details below', 'link': '', 'link_domain': '', 'post_published': '2017-08-31T17:36:47+0000', 'likes_count_fb': '7', 'comments_count_fb': '12', 'reactions_count_fb': '7', 'shares_count_fb': '0', 'engagement_fb': '19'}\n{'\\ufefftype': 'event', 'post_message': '3 more dates for September Tours of The Medieval City Wall Below. Saturday 16th  23  rd and 30th https://www.facebook.com/events/1935828209971181/?ti=icl', 'link': 'https://www.facebook.com/events/1935828209971181/', 'link_domain': 'facebook.com', 'post_published': '2017-09-05T10:46:45+0000', 'likes_count_fb': '2', 'comments_count_fb': '0', 'reactions_count_fb': '2', 'shares_count_fb': '0', 'engagement_fb': '2'}\n{'\\ufefftype': 'link', 'post_message': 'http://www.coventrytelegraph.net/whats-on/arts-culture-news/majestic-medieval-hall-coventrys-best-13448249', 'link': 'http://www.coventrytelegraph.net/whats-on/arts-culture-news/majestic-medieval-hall-coventrys-best-13448249', 'link_domain': 'coventrytelegraph.net', 'post_published': '2017-08-14T08:56:04+0000', 'likes_count_fb': '9', 'comments_count_fb': '2', 'reactions_count_fb': '10', 'shares_count_fb': '4', 'engagement_fb': '16'}\n{'\\ufefftype': 'status', 'post_message': 'Different angle of St Michaels', 'link': '', 'link_domain': '', 'post_published': '2017-08-26T09:04:35+0000', 'likes_count_fb': '11', 'comments_count_fb': '0', 'reactions_count_fb': '12', 'shares_count_fb': '0', 'engagement_fb': '12'}\n{'\\ufefftype': 'link', 'post_message': 'The Heritage Open Days are taking place from 7-10 September  with venues opening their doors and giving access to areas not normally open to the public - and now is the time to book a tour. As in previous years  there are some very interesting and unique tours taking place.   Make sure you don‚Äôt miss out on these by checking booking details online and booking your place.  Tours that need pre booking include: Medieval City Wall; BBC Coventry and Warwickshire Radio; Priory Undercroft and Visitor Centre; King Henry VIII School; Coventry Solihull Waste Disposal Ltd; and Coventry Central Library behind the scenes tour.  A Heritage Open Days brochure will be available from Council buildings  The Herbert  Coventry Cathedral and libraries from Wednesday (23 August).  For more details  visit the website www.coventry.gov.uk/hod.', 'link': 'http://www.coventry.gov.uk/hod', 'link_domain': 'coventry.gov.uk', 'post_published': '2017-08-21T10:26:52+0000', 'likes_count_fb': '10', 'comments_count_fb': '0', 'reactions_count_fb': '10', 'shares_count_fb': '13', 'engagement_fb': '23'}\n{'\\ufefftype': 'status', 'post_message': 'Hi thanks to be able to share here & be part of tnis group ; Good to kbow iof yt area as going down soon with Natiomal Trust centre from Glasgow', 'link': '', 'link_domain': '', 'post_published': '2017-08-17T08:36:07+0000', 'likes_count_fb': '1', 'comments_count_fb': '8', 'reactions_count_fb': '1', 'shares_count_fb': '0', 'engagement_fb': '9'}\n{'\\ufefftype': 'link', 'post_message': 'http://www.coventrytelegraph.net/whats-on/whats-on-news/future-coventrys-albany-theatre-been-13477790', 'link': 'http://www.coventrytelegraph.net/whats-on/whats-on-news/future-coventrys-albany-theatre-been-13477790', 'link_domain': 'coventrytelegraph.net', 'post_published': '2017-08-17T09:14:59+0000', 'likes_count_fb': '8', 'comments_count_fb': '1', 'reactions_count_fb': '9', 'shares_count_fb': '1', 'engagement_fb': '11'}\n{'\\ufefftype': 'photo', 'post_message': 'https://www.facebook.com/peter.garbett.9/posts/1769150796433812  Sad we are loosing this museum visit it whilst you can', 'link': 'https://www.facebook.com/photo.php?fbid=1280170205424760&set=gm.1921178978100268&type=3', 'link_domain': 'facebook.com', 'post_published': '2017-08-10T11:49:47+0000', 'likes_count_fb': '2', 'comments_count_fb': '1', 'reactions_count_fb': '2', 'shares_count_fb': '0', 'engagement_fb': '3'}\n{'\\ufefftype': 'link', 'post_message': 'https://www.facebook.com/CoventryPVC/posts/859986554155927', 'link': 'http://www.crowdfunder.co.uk/coventry-priory', 'link_domain': 'crowdfunder.co.uk', 'post_published': '2017-08-07T21:13:46+0000', 'likes_count_fb': '6', 'comments_count_fb': '1', 'reactions_count_fb': '6', 'shares_count_fb': '0', 'engagement_fb': '7'}\n{'\\ufefftype': 'photo', 'post_message': 'Looking good in the sunshine.. https://www.facebook.com/visitcoventry.net/posts/501006916915750', 'link': 'https://www.facebook.com/visitcoventry.net/photos/a.373019843047792.1073741829.361929627490147/501006773582431/?type=3', 'link_domain': 'facebook.com', 'post_published': '2017-08-06T21:25:32+0000', 'likes_count_fb': '8', 'comments_count_fb': '0', 'reactions_count_fb': '8', 'shares_count_fb': '1', 'engagement_fb': '9'}\n{'\\ufefftype': 'status', 'post_message': 'Best unique Pubs Whats your favourite pubs and why? in Coventry and surrounding areas.', 'link': '', 'link_domain': '', 'post_published': '2017-07-19T08:35:49+0000', 'likes_count_fb': '0', 'comments_count_fb': '7', 'reactions_count_fb': '0', 'shares_count_fb': '0', 'engagement_fb': '7'}\n{'\\ufefftype': 'status', 'post_message': 'Thank you for letting me join your group', 'link': '', 'link_domain': '', 'post_published': '2017-07-19T14:30:58+0000', 'likes_count_fb': '1', 'comments_count_fb': '1', 'reactions_count_fb': '1', 'shares_count_fb': '0', 'engagement_fb': '2'}\n{'\\ufefftype': 'link', 'post_message': 'https://www.facebook.com/theprideofwillenhall/posts/1378187435629358', 'link': 'http://www.coventrytelegraph.net/whats-on/whats-on-news/worlds-longest-inflatable-obstacle-course-13346712', 'link_domain': 'coventrytelegraph.net', 'post_published': '2017-07-19T08:23:55+0000', 'likes_count_fb': '0', 'comments_count_fb': '0', 'reactions_count_fb': '0', 'shares_count_fb': '0', 'engagement_fb': '0'}\n{'\\ufefftype': 'link', 'post_message': 'https://www.facebook.com/peter.garbett.9/posts/1740635729285319  Coventry shortlisted!', 'link': 'https://coventry2021.co.uk/news/?is_mobile=1', 'link_domain': 'coventry2021.co.uk', 'post_published': '2017-07-15T18:04:34+0000', 'likes_count_fb': '2', 'comments_count_fb': '0', 'reactions_count_fb': '2', 'shares_count_fb': '0', 'engagement_fb': '2'}\n{'\\ufefftype': 'photo', 'post_message': 'Nice. https://www.facebook.com/visitcoventry.net/photos/a.363722627310847.1073741828.361929627490147/489764101373365/?type=3', 'link': 'https://www.facebook.com/visitcoventry.net/photos/a.363722627310847.1073741828.361929627490147/489764101373365/?type=3', 'link_domain': 'facebook.com', 'post_published': '2017-07-13T22:06:20+0000', 'likes_count_fb': '3', 'comments_count_fb': '0', 'reactions_count_fb': '3', 'shares_count_fb': '0', 'engagement_fb': '3'}\n{'\\ufefftype': 'photo', 'post_message': 'I ve always wanted to have a nose down this passageway..  https://www.facebook.com/visitcoventry.net/posts/487218148294627', 'link': 'https://www.facebook.com/visitcoventry.net/photos/a.373019843047792.1073741829.361929627490147/487215948294847/?type=3', 'link_domain': 'facebook.com', 'post_published': '2017-07-09T12:21:01+0000', 'likes_count_fb': '11', 'comments_count_fb': '1', 'reactions_count_fb': '11', 'shares_count_fb': '0', 'engagement_fb': '12'}\n{'\\ufefftype': 'link', 'post_message': 'http://www.coventrytelegraph.net/whats-on/music-nightlife-news/what-godiva-festival-weather-forecast-13275038', 'link': 'http://www.coventrytelegraph.net/whats-on/music-nightlife-news/what-godiva-festival-weather-forecast-13275038', 'link_domain': 'coventrytelegraph.net', 'post_published': '2017-07-08T08:39:38+0000', 'likes_count_fb': '1', 'comments_count_fb': '0', 'reactions_count_fb': '1', 'shares_count_fb': '0', 'engagement_fb': '1'}\n{'\\ufefftype': 'video', 'post_message': 'Looks like a great idea.', 'link': 'https://www.facebook.com/stmarysguildhall/videos/10156233759843475/', 'link_domain': 'facebook.com', 'post_published': '2017-07-07T15:28:24+0000', 'likes_count_fb': '7', 'comments_count_fb': '0', 'reactions_count_fb': '9', 'shares_count_fb': '2', 'engagement_fb': '11'}\n{'\\ufefftype': 'photo', 'post_message': 'Lovely pic.. https://www.facebook.com/visitcoventry.net/photos/a.363722627310847.1073741828.361929627490147/483673398649102/?type=3', 'link': 'https://www.facebook.com/visitcoventry.net/photos/a.363722627310847.1073741828.361929627490147/483673398649102/?type=3', 'link_domain': 'facebook.com', 'post_published': '2017-07-02T08:41:08+0000', 'likes_count_fb': '15', 'comments_count_fb': '2', 'reactions_count_fb': '16', 'shares_count_fb': '1', 'engagement_fb': '19'}\n{'\\ufefftype': 'status', 'post_message': 'What is your favourite restaurant or eatery in Coventry area?', 'link': '', 'link_domain': '', 'post_published': '2017-06-29T08:55:29+0000', 'likes_count_fb': '3', 'comments_count_fb': '35', 'reactions_count_fb': '3', 'shares_count_fb': '0', 'engagement_fb': '38'}\n{'\\ufefftype': 'photo', 'post_message': 'Interesting.. https://www.facebook.com/visitcoventry.net/photos/a.363722627310847.1073741828.361929627490147/481529385530170/?type=3', 'link': 'https://www.facebook.com/visitcoventry.net/photos/a.363722627310847.1073741828.361929627490147/481529385530170/?type=3', 'link_domain': 'facebook.com', 'post_published': '2017-06-29T05:32:53+0000', 'likes_count_fb': '9', 'comments_count_fb': '0', 'reactions_count_fb': '9', 'shares_count_fb': '0', 'engagement_fb': '9'}\n{'\\ufefftype': 'link', 'post_message': 'https://www.facebook.com/BerkswellWindmill/posts/1322847697832828', 'link': 'https://www.facebook.com/BerkswellWindmill/posts/1322847697832828', 'link_domain': 'facebook.com', 'post_published': '2017-06-26T18:24:47+0000', 'likes_count_fb': '0', 'comments_count_fb': '0', 'reactions_count_fb': '0', 'shares_count_fb': '0', 'engagement_fb': '0'}\n{'\\ufefftype': 'status', 'post_message': 'Just saw a post about people that made me think would almost be a great motto for our fabulous city.   Let go of what s gone. Be grateful for (and save) what remains. Look forward to what is coming. ', 'link': '', 'link_domain': '', 'post_published': '2017-06-25T17:17:12+0000', 'likes_count_fb': '4', 'comments_count_fb': '2', 'reactions_count_fb': '5', 'shares_count_fb': '0', 'engagement_fb': '7'}\n{'\\ufefftype': 'event', 'post_message': 'Why not make a weekend of it? It s Jaguar Super Saturday! In aid of Air Ambulance  details on joining below  if you own a Jaguar! This year there will be a  timeline  of cars from oldest to newest  many built at Browns Lane!', 'link': 'https://www.facebook.com/events/201361447045208/', 'link_domain': 'facebook.com', 'post_published': '2017-06-22T19:27:54+0000', 'likes_count_fb': '2', 'comments_count_fb': '0', 'reactions_count_fb': '2', 'shares_count_fb': '0', 'engagement_fb': '2'}\n{'\\ufefftype': 'event', 'post_message': ':) we have a wonderful collection of exciting Citroen motorcars at the Museum on Sunday  if you own a Citroen you re welcome to come along with your car 10-4pm it s FREE and you may be given a leaflet about the car club! Any Citroen can come along  old or new  thank you :)', 'link': 'https://www.facebook.com/events/362084697478113/', 'link_domain': 'facebook.com', 'post_published': '2017-06-22T19:23:49+0000', 'likes_count_fb': '1', 'comments_count_fb': '0', 'reactions_count_fb': '1', 'shares_count_fb': '0', 'engagement_fb': '1'}\n{'\\ufefftype': 'event', 'post_message': 'City Wall Guided Costume Tour this Saturday meeting at 2pm at Priory Visitors Centre ( behind Nando s  Trinity Street) with the Deep Fat Friar https://www.facebook.com/events/313502545772268/?ti=icl', 'link': 'https://www.facebook.com/events/313502545772268/', 'link_domain': 'facebook.com', 'post_published': '2017-06-22T19:20:55+0000', 'likes_count_fb': '6', 'comments_count_fb': '0', 'reactions_count_fb': '6', 'shares_count_fb': '1', 'engagement_fb': '7'}\n{'\\ufefftype': 'link', 'post_message': 'https://www.facebook.com/BerkswellWindmill/posts/1322847697832828', 'link': 'https://www.facebook.com/BerkswellWindmill/posts/1322847697832828', 'link_domain': 'facebook.com', 'post_published': '2017-06-22T16:59:56+0000', 'likes_count_fb': '1', 'comments_count_fb': '0', 'reactions_count_fb': '1', 'shares_count_fb': '0', 'engagement_fb': '1'}\n\n\nLook at the first entry in csv_content.\n\ncsv_content[0]\n\n{'\\ufefftype': 'status',\n 'post_message': 'Don t forget to post your photographs from heritage weekend  of you having a good time on here please.',\n 'link': '',\n 'link_domain': '',\n 'post_published': '2017-09-09T13:57:14+0000',\n 'likes_count_fb': '5',\n 'comments_count_fb': '1',\n 'reactions_count_fb': '5',\n 'shares_count_fb': '0',\n 'engagement_fb': '6'}\n\n\nDictionaries have keys. The keys method of the dictionary object will show them to us (though they are obvious from printing the first line aboe).\n\ncsv_content[0].keys()\n\ndict_keys(['\\ufefftype', 'post_message', 'link', 'link_domain', 'post_published', 'likes_count_fb', 'comments_count_fb', 'reactions_count_fb', 'shares_count_fb', 'engagement_fb'])\n\n\n\ncsv_content[0]['post_message']\n\n'Don t forget to post your photographs from heritage weekend  of you having a good time on here please.'\n\n\nWe have a list where each list element is a dictionary. So we need to index our list each time, hence the csv_content[0].\nTo go through our list we need to use a for loop. So, in order to get each post message.\n\nfor post in csv_content:\n    print(post['post_message'])\n\nDon t forget to post your photographs from heritage weekend  of you having a good time on here please.\nIf you want to learn about Historic Coventry this Saturday we will be carrying out a guided tour around the Medieval Wall of Coventry. Booking details below\n3 more dates for September Tours of The Medieval City Wall Below. Saturday 16th  23  rd and 30th https://www.facebook.com/events/1935828209971181/?ti=icl\nhttp://www.coventrytelegraph.net/whats-on/arts-culture-news/majestic-medieval-hall-coventrys-best-13448249\nDifferent angle of St Michaels\nThe Heritage Open Days are taking place from 7-10 September  with venues opening their doors and giving access to areas not normally open to the public - and now is the time to book a tour. As in previous years  there are some very interesting and unique tours taking place.   Make sure you don‚Äôt miss out on these by checking booking details online and booking your place.  Tours that need pre booking include: Medieval City Wall; BBC Coventry and Warwickshire Radio; Priory Undercroft and Visitor Centre; King Henry VIII School; Coventry Solihull Waste Disposal Ltd; and Coventry Central Library behind the scenes tour.  A Heritage Open Days brochure will be available from Council buildings  The Herbert  Coventry Cathedral and libraries from Wednesday (23 August).  For more details  visit the website www.coventry.gov.uk/hod.\nHi thanks to be able to share here & be part of tnis group ; Good to kbow iof yt area as going down soon with Natiomal Trust centre from Glasgow\nhttp://www.coventrytelegraph.net/whats-on/whats-on-news/future-coventrys-albany-theatre-been-13477790\nhttps://www.facebook.com/peter.garbett.9/posts/1769150796433812  Sad we are loosing this museum visit it whilst you can\nhttps://www.facebook.com/CoventryPVC/posts/859986554155927\nLooking good in the sunshine.. https://www.facebook.com/visitcoventry.net/posts/501006916915750\nBest unique Pubs Whats your favourite pubs and why? in Coventry and surrounding areas.\nThank you for letting me join your group\nhttps://www.facebook.com/theprideofwillenhall/posts/1378187435629358\nhttps://www.facebook.com/peter.garbett.9/posts/1740635729285319  Coventry shortlisted!\nNice. https://www.facebook.com/visitcoventry.net/photos/a.363722627310847.1073741828.361929627490147/489764101373365/?type=3\nI ve always wanted to have a nose down this passageway..  https://www.facebook.com/visitcoventry.net/posts/487218148294627\nhttp://www.coventrytelegraph.net/whats-on/music-nightlife-news/what-godiva-festival-weather-forecast-13275038\nLooks like a great idea.\nLovely pic.. https://www.facebook.com/visitcoventry.net/photos/a.363722627310847.1073741828.361929627490147/483673398649102/?type=3\nWhat is your favourite restaurant or eatery in Coventry area?\nInteresting.. https://www.facebook.com/visitcoventry.net/photos/a.363722627310847.1073741828.361929627490147/481529385530170/?type=3\nhttps://www.facebook.com/BerkswellWindmill/posts/1322847697832828\nJust saw a post about people that made me think would almost be a great motto for our fabulous city.   Let go of what s gone. Be grateful for (and save) what remains. Look forward to what is coming. \nWhy not make a weekend of it? It s Jaguar Super Saturday! In aid of Air Ambulance  details on joining below  if you own a Jaguar! This year there will be a  timeline  of cars from oldest to newest  many built at Browns Lane!\n:) we have a wonderful collection of exciting Citroen motorcars at the Museum on Sunday  if you own a Citroen you re welcome to come along with your car 10-4pm it s FREE and you may be given a leaflet about the car club! Any Citroen can come along  old or new  thank you :)\nCity Wall Guided Costume Tour this Saturday meeting at 2pm at Priory Visitors Centre ( behind Nando s  Trinity Street) with the Deep Fat Friar https://www.facebook.com/events/313502545772268/?ti=icl\nhttps://www.facebook.com/BerkswellWindmill/posts/1322847697832828\n\n\nIf I want to do data science, where accessing data in a sensible way is key, then there must be a better way! There is.\n\n\n4.1.2 Pandas\nThe Pandas library is designed with data science in mind. You will examine it more in the coming weeks. Reading CSV files with pandas is very easy.\n\nimport pandas as pd\n\nThe import x as y is good practice. In the below code, anything with a pd. prefix comes from pandas. This is particulary useful for preventing a module from overwriting inbuilt Python functionality.\n\ndf = pd.read_csv('data/Facebook.csv', encoding = 'UTF-8')\n\nThat was easy. We can even pick out specific rows.\n\ndf['post_message']\n\n0     Don t forget to post your photographs from her...\n1     If you want to learn about Historic Coventry t...\n2     3 more dates for September Tours of The Mediev...\n3     http://www.coventrytelegraph.net/whats-on/arts...\n4                        Different angle of St Michaels\n5     The Heritage Open Days are taking place from 7...\n6     Hi thanks to be able to share here & be part o...\n7     http://www.coventrytelegraph.net/whats-on/what...\n8     https://www.facebook.com/peter.garbett.9/posts...\n9     https://www.facebook.com/CoventryPVC/posts/859...\n10    Looking good in the sunshine.. https://www.fac...\n11    Best unique Pubs Whats your favourite pubs and...\n12             Thank you for letting me join your group\n13    https://www.facebook.com/theprideofwillenhall/...\n14    https://www.facebook.com/peter.garbett.9/posts...\n15    Nice. https://www.facebook.com/visitcoventry.n...\n16    I ve always wanted to have a nose down this pa...\n17    http://www.coventrytelegraph.net/whats-on/musi...\n18                             Looks like a great idea.\n19    Lovely pic.. https://www.facebook.com/visitcov...\n20    What is your favourite restaurant or eatery in...\n21    Interesting.. https://www.facebook.com/visitco...\n22    https://www.facebook.com/BerkswellWindmill/pos...\n23    Just saw a post about people that made me thin...\n24    Why not make a weekend of it? It s Jaguar Supe...\n25    :) we have a wonderful collection of exciting ...\n26    City Wall Guided Costume Tour this Saturday me...\n27    https://www.facebook.com/BerkswellWindmill/pos...\nName: post_message, dtype: object\n\n\nAnd even look at the dataset in a pretty way.\n\ndf\n\n\n\n\n\n\n\n\ntype\npost_message\nlink\nlink_domain\npost_published\nlikes_count_fb\ncomments_count_fb\nreactions_count_fb\nshares_count_fb\nengagement_fb\n\n\n\n\n0\nstatus\nDon t forget to post your photographs from her...\nNaN\nNaN\n2017-09-09T13:57:14+0000\n5\n1\n5\n0\n6\n\n\n1\nstatus\nIf you want to learn about Historic Coventry t...\nNaN\nNaN\n2017-08-31T17:36:47+0000\n7\n12\n7\n0\n19\n\n\n2\nevent\n3 more dates for September Tours of The Mediev...\nhttps://www.facebook.com/events/1935828209971181/\nfacebook.com\n2017-09-05T10:46:45+0000\n2\n0\n2\n0\n2\n\n\n3\nlink\nhttp://www.coventrytelegraph.net/whats-on/arts...\nhttp://www.coventrytelegraph.net/whats-on/arts...\ncoventrytelegraph.net\n2017-08-14T08:56:04+0000\n9\n2\n10\n4\n16\n\n\n4\nstatus\nDifferent angle of St Michaels\nNaN\nNaN\n2017-08-26T09:04:35+0000\n11\n0\n12\n0\n12\n\n\n5\nlink\nThe Heritage Open Days are taking place from 7...\nhttp://www.coventry.gov.uk/hod\ncoventry.gov.uk\n2017-08-21T10:26:52+0000\n10\n0\n10\n13\n23\n\n\n6\nstatus\nHi thanks to be able to share here & be part o...\nNaN\nNaN\n2017-08-17T08:36:07+0000\n1\n8\n1\n0\n9\n\n\n7\nlink\nhttp://www.coventrytelegraph.net/whats-on/what...\nhttp://www.coventrytelegraph.net/whats-on/what...\ncoventrytelegraph.net\n2017-08-17T09:14:59+0000\n8\n1\n9\n1\n11\n\n\n8\nphoto\nhttps://www.facebook.com/peter.garbett.9/posts...\nhttps://www.facebook.com/photo.php?fbid=128017...\nfacebook.com\n2017-08-10T11:49:47+0000\n2\n1\n2\n0\n3\n\n\n9\nlink\nhttps://www.facebook.com/CoventryPVC/posts/859...\nhttp://www.crowdfunder.co.uk/coventry-priory\ncrowdfunder.co.uk\n2017-08-07T21:13:46+0000\n6\n1\n6\n0\n7\n\n\n10\nphoto\nLooking good in the sunshine.. https://www.fac...\nhttps://www.facebook.com/visitcoventry.net/pho...\nfacebook.com\n2017-08-06T21:25:32+0000\n8\n0\n8\n1\n9\n\n\n11\nstatus\nBest unique Pubs Whats your favourite pubs and...\nNaN\nNaN\n2017-07-19T08:35:49+0000\n0\n7\n0\n0\n7\n\n\n12\nstatus\nThank you for letting me join your group\nNaN\nNaN\n2017-07-19T14:30:58+0000\n1\n1\n1\n0\n2\n\n\n13\nlink\nhttps://www.facebook.com/theprideofwillenhall/...\nhttp://www.coventrytelegraph.net/whats-on/what...\ncoventrytelegraph.net\n2017-07-19T08:23:55+0000\n0\n0\n0\n0\n0\n\n\n14\nlink\nhttps://www.facebook.com/peter.garbett.9/posts...\nhttps://coventry2021.co.uk/news/?is_mobile=1\ncoventry2021.co.uk\n2017-07-15T18:04:34+0000\n2\n0\n2\n0\n2\n\n\n15\nphoto\nNice. https://www.facebook.com/visitcoventry.n...\nhttps://www.facebook.com/visitcoventry.net/pho...\nfacebook.com\n2017-07-13T22:06:20+0000\n3\n0\n3\n0\n3\n\n\n16\nphoto\nI ve always wanted to have a nose down this pa...\nhttps://www.facebook.com/visitcoventry.net/pho...\nfacebook.com\n2017-07-09T12:21:01+0000\n11\n1\n11\n0\n12\n\n\n17\nlink\nhttp://www.coventrytelegraph.net/whats-on/musi...\nhttp://www.coventrytelegraph.net/whats-on/musi...\ncoventrytelegraph.net\n2017-07-08T08:39:38+0000\n1\n0\n1\n0\n1\n\n\n18\nvideo\nLooks like a great idea.\nhttps://www.facebook.com/stmarysguildhall/vide...\nfacebook.com\n2017-07-07T15:28:24+0000\n7\n0\n9\n2\n11\n\n\n19\nphoto\nLovely pic.. https://www.facebook.com/visitcov...\nhttps://www.facebook.com/visitcoventry.net/pho...\nfacebook.com\n2017-07-02T08:41:08+0000\n15\n2\n16\n1\n19\n\n\n20\nstatus\nWhat is your favourite restaurant or eatery in...\nNaN\nNaN\n2017-06-29T08:55:29+0000\n3\n35\n3\n0\n38\n\n\n21\nphoto\nInteresting.. https://www.facebook.com/visitco...\nhttps://www.facebook.com/visitcoventry.net/pho...\nfacebook.com\n2017-06-29T05:32:53+0000\n9\n0\n9\n0\n9\n\n\n22\nlink\nhttps://www.facebook.com/BerkswellWindmill/pos...\nhttps://www.facebook.com/BerkswellWindmill/pos...\nfacebook.com\n2017-06-26T18:24:47+0000\n0\n0\n0\n0\n0\n\n\n23\nstatus\nJust saw a post about people that made me thin...\nNaN\nNaN\n2017-06-25T17:17:12+0000\n4\n2\n5\n0\n7\n\n\n24\nevent\nWhy not make a weekend of it? It s Jaguar Supe...\nhttps://www.facebook.com/events/201361447045208/\nfacebook.com\n2017-06-22T19:27:54+0000\n2\n0\n2\n0\n2\n\n\n25\nevent\n:) we have a wonderful collection of exciting ...\nhttps://www.facebook.com/events/362084697478113/\nfacebook.com\n2017-06-22T19:23:49+0000\n1\n0\n1\n0\n1\n\n\n26\nevent\nCity Wall Guided Costume Tour this Saturday me...\nhttps://www.facebook.com/events/313502545772268/\nfacebook.com\n2017-06-22T19:20:55+0000\n6\n0\n6\n1\n7\n\n\n27\nlink\nhttps://www.facebook.com/BerkswellWindmill/pos...\nhttps://www.facebook.com/BerkswellWindmill/pos...\nfacebook.com\n2017-06-22T16:59:56+0000\n1\n0\n1\n0\n1\n\n\n\n\n\n\n\nHuzzah!\nA final point, pd.read_csv returns a pandas specific object with associated methods.\n\ntype(df)\n\npandas.core.frame.DataFrame\n\n\n\ndf.dtypes\n\ntype                  object\npost_message          object\nlink                  object\nlink_domain           object\npost_published        object\nlikes_count_fb         int64\ncomments_count_fb      int64\nreactions_count_fb     int64\nshares_count_fb        int64\nengagement_fb          int64\ndtype: object\n\n\nWhere int64 are integers and object here refers to strings."
  },
  {
    "objectID": "content/labs/Lab_2/IM939_Lab_2_1.html#help",
    "href": "content/labs/Lab_2/IM939_Lab_2_1.html#help",
    "title": "5  Lab: Pandas",
    "section": "5.1 Help!",
    "text": "5.1 Help!\nPython has inbuilt documentation. To access this add a ? before an object or method.\nFor example, our dataframe\n\n?df.info\n\nThe dtypes property (properties of obejct are values associated with the object and are not called with a () at the end).\n\n?df.dtypes\n\nThe info method for dataframes.\n\n?df.info\n\nThe below is quite long. But goes give you the various arguments (options) you can use with the method.\n\n?pd.read_csv\n\nThe Pandas documentation is rather good. Relevent to our below work is:\n\nWhat kind of data does pandas handle?\nHow to calculate summary statistics?\nHow to create plots in pandas?\nHow to handle time series data with ease?\n\nI also found a rather nice series of lessons a kind person put together. There are lots of online tutorials which will help you."
  },
  {
    "objectID": "content/labs/Lab_2/IM939_Lab_2_1.html#structure",
    "href": "content/labs/Lab_2/IM939_Lab_2_1.html#structure",
    "title": "5  Lab: Pandas",
    "section": "5.2 Structure",
    "text": "5.2 Structure\nHow do we find out the structure of our data?\nWell, the variable df is now a pandas DataFrame object.\n\ntype(df)\n\npandas.core.frame.DataFrame\n\n\nThe DataFrame object has lots of built in methods and attributes.\nThe info method gives us information about datatypes, dimensions and the presence of null values in our dataframe.\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 188 entries, 0 to 187\nData columns (total 6 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   season       188 non-null    int64  \n 1   episode      188 non-null    int64  \n 2   title        188 non-null    object \n 3   imdb_rating  188 non-null    float64\n 4   total_votes  188 non-null    int64  \n 5   air_date     188 non-null    object \ndtypes: float64(1), int64(3), object(2)\nmemory usage: 8.9+ KB\n\n\nWe can just look at the datatypes if we want.\n\ndf.dtypes\n\nseason           int64\nepisode          int64\ntitle           object\nimdb_rating    float64\ntotal_votes      int64\nair_date        object\ndtype: object\n\n\nOr just the dimensions.\n\ndf.shape\n\n(188, 6)\n\n\nIn this case, there are only 188 rows. But for larger datasets we might want to look at the head (top 5) and tail (bottom 5) rows.\n\ndf.head()\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\nimdb_rating\ntotal_votes\nair_date\n\n\n\n\n0\n1\n1\nPilot\n7.6\n3706\n2005-03-24\n\n\n1\n1\n2\nDiversity Day\n8.3\n3566\n2005-03-29\n\n\n2\n1\n3\nHealth Care\n7.9\n2983\n2005-04-05\n\n\n3\n1\n4\nThe Alliance\n8.1\n2886\n2005-04-12\n\n\n4\n1\n5\nBasketball\n8.4\n3179\n2005-04-19\n\n\n\n\n\n\n\n\ndf.tail()\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\nimdb_rating\ntotal_votes\nair_date\n\n\n\n\n183\n9\n19\nStairmageddon\n8.0\n1484\n2013-04-11\n\n\n184\n9\n20\nPaper Airplane\n8.0\n1482\n2013-04-25\n\n\n185\n9\n21\nLivin' the Dream\n8.9\n2041\n2013-05-02\n\n\n186\n9\n22\nA.A.R.M.\n9.3\n2860\n2013-05-09\n\n\n187\n9\n23\nFinale\n9.7\n7934\n2013-05-16\n\n\n\n\n\n\n\nWe may need to put together a dataset.\nConsider the two dataframe below\n\ndf_1 = pd.read_csv('data/raw/office1.csv', encoding='UTF-8')\ndf_2 = pd.read_csv('data/raw/office2.csv', encoding='UTF-8')\n\n\ndf_1.head()\n\n\n\n\n\n\n\n\nid\nseason\nepisode\nimdb_rating\n\n\n\n\n0\n5-1\n5\n1\n8.8\n\n\n1\n9-13\n9\n13\n7.7\n\n\n2\n5-6\n5\n6\n8.5\n\n\n3\n3-23\n3\n23\n9.3\n\n\n4\n9-16\n9\n16\n8.2\n\n\n\n\n\n\n\n\ndf_2.head()\n\n\n\n\n\n\n\n\nid\ntotal_votes\n\n\n\n\n0\n4-10\n2095\n\n\n1\n3-21\n2403\n\n\n2\n7-24\n2040\n\n\n3\n6-18\n1769\n\n\n4\n8-8\n1584\n\n\n\n\n\n\n\nThe total votes and imdb ratings data are split between files. There is a common column called id. We can join the two dataframes together using the common column.\n\ninner_join_office_df = pd.merge(df_1, df_2, on='id', how='inner')\ninner_join_office_df\n\n\n\n\n\n\n\n\nid\nseason\nepisode\nimdb_rating\ntotal_votes\n\n\n\n\n0\n5-1\n5\n1\n8.8\n2501\n\n\n1\n9-13\n9\n13\n7.7\n1394\n\n\n2\n5-6\n5\n6\n8.5\n2018\n\n\n3\n3-23\n3\n23\n9.3\n3010\n\n\n4\n9-16\n9\n16\n8.2\n1572\n\n\n...\n...\n...\n...\n...\n...\n\n\n183\n5-21\n5\n21\n8.7\n2032\n\n\n184\n2-13\n2\n13\n8.3\n2363\n\n\n185\n9-6\n9\n6\n7.8\n1455\n\n\n186\n2-2\n2\n2\n8.2\n2736\n\n\n187\n3-4\n3\n4\n8.0\n2311\n\n\n\n\n188 rows × 5 columns\n\n\n\nIn this way you can combine datasets using common columns. We will leave that for the moment. If you want more information about merging data then see this page and the pandas documentation."
  },
  {
    "objectID": "content/labs/Lab_2/IM939_Lab_2_1.html#summary",
    "href": "content/labs/Lab_2/IM939_Lab_2_1.html#summary",
    "title": "5  Lab: Pandas",
    "section": "5.3 Summary",
    "text": "5.3 Summary\nTo get an overview of our data we can ask Python to ‘describe our data’\n\ndf.describe()\n\n\n\n\n\n\n\n\nseason\nepisode\nimdb_rating\ntotal_votes\n\n\n\n\ncount\n188.000000\n188.000000\n188.000000\n188.000000\n\n\nmean\n5.468085\n11.877660\n8.257447\n2126.648936\n\n\nstd\n2.386245\n7.024855\n0.538067\n787.098275\n\n\nmin\n1.000000\n1.000000\n6.700000\n1393.000000\n\n\n25%\n3.000000\n6.000000\n7.900000\n1631.500000\n\n\n50%\n6.000000\n11.500000\n8.200000\n1952.500000\n\n\n75%\n7.250000\n18.000000\n8.600000\n2379.000000\n\n\nmax\n9.000000\n26.000000\n9.700000\n7934.000000\n\n\n\n\n\n\n\nor we can pull out specific statistics.\n\ndf.mean(numeric_only=True)\n\nseason            5.468085\nepisode          11.877660\nimdb_rating       8.257447\ntotal_votes    2126.648936\ndtype: float64\n\n\nNote the error triggered above due to pandas attempting to calculate the mean of the wrong type (i.e. non-numeric values). We can address that by only computing the mean of numeric values (see below):\n\ndf.mean(numeric_only=True)\n\nseason            5.468085\nepisode          11.877660\nimdb_rating       8.257447\ntotal_votes    2126.648936\ndtype: float64\n\n\nor the sum.\n\ndf.sum()\n\nseason                                                      1028\nepisode                                                     2233\ntitle          PilotDiversity DayHealth CareThe AllianceBaske...\nimdb_rating                                               1552.4\ntotal_votes                                               399810\nair_date       2005-03-242005-03-292005-04-052005-04-122005-0...\ndtype: object\n\n\nSimilarly to what happened with mean(), sum() is adding all values in every observation of every attribute, regardless of their type. Can you see what happens with strings? And with dates?\nAgain, we can only force to use numeric values only:\n\ndf.sum(numeric_only=True)\n\nseason           1028.0\nepisode          2233.0\nimdb_rating      1552.4\ntotal_votes    399810.0\ndtype: float64\n\n\nCalculating these statistics for specific columns is straight forward.\n\ndf['imdb_rating'].mean()\n\n8.25744680851064\n\n\n\ndf['total_votes'].sum()\n\n399810"
  },
  {
    "objectID": "content/labs/Lab_2/IM939_Lab_2_1.html#selecting-subsets",
    "href": "content/labs/Lab_2/IM939_Lab_2_1.html#selecting-subsets",
    "title": "5  Lab: Pandas",
    "section": "5.4 Selecting subsets",
    "text": "5.4 Selecting subsets\nWe can even select more than one column!\n\ndf[['imdb_rating', 'total_votes']].mean()\n\nimdb_rating       8.257447\ntotal_votes    2126.648936\ndtype: float64\n\n\nTwo sets of squared brackets is needed because you are passing a list of the column names to the getitem dunder method of the pandas dataframe object (thank this stackoverflow question). You can also check out the pandas documentation on indexing and selecting data.\nYou can also select by row and column name using the iloc method. You can specify the [row, column]. So to choose the value in the 2nd row and 4th column.\n\ndf.iloc[2,4]\n\n2983\n\n\nAll the rows or all the columns are indicated by :. Such as,\n\ndf.iloc[:,2]\n\n0                 Pilot\n1         Diversity Day\n2           Health Care\n3          The Alliance\n4            Basketball\n             ...       \n183       Stairmageddon\n184      Paper Airplane\n185    Livin' the Dream\n186            A.A.R.M.\n187              Finale\nName: title, Length: 188, dtype: object\n\n\n\ndf.iloc[2,:]\n\nseason                   1\nepisode                  3\ntitle          Health Care\nimdb_rating            7.9\ntotal_votes           2983\nair_date        2005-04-05\nName: 2, dtype: object\n\n\nWe can use negative values in indexes to indicate ‘from the end’. So, an index of [-10, :] returns the 10th from last row.\n\ndf.iloc[-10,:]\n\nseason                  9\nepisode                14\ntitle           Vandalism\nimdb_rating           7.6\ntotal_votes          1402\nair_date       2013-01-31\nName: 178, dtype: object\n\n\nInstead of using tail, we could ask for the last 10 rows with an index of [-10:, :]. I read : as ‘and everything else’ in these cases.\n\ndf.iloc[-5:,:]\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\nimdb_rating\ntotal_votes\nair_date\n\n\n\n\n183\n9\n19\nStairmageddon\n8.0\n1484\n2013-04-11\n\n\n184\n9\n20\nPaper Airplane\n8.0\n1482\n2013-04-25\n\n\n185\n9\n21\nLivin' the Dream\n8.9\n2041\n2013-05-02\n\n\n186\n9\n22\nA.A.R.M.\n9.3\n2860\n2013-05-09\n\n\n187\n9\n23\nFinale\n9.7\n7934\n2013-05-16\n\n\n\n\n\n\n\n\ndf.tail()\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\nimdb_rating\ntotal_votes\nair_date\n\n\n\n\n183\n9\n19\nStairmageddon\n8.0\n1484\n2013-04-11\n\n\n184\n9\n20\nPaper Airplane\n8.0\n1482\n2013-04-25\n\n\n185\n9\n21\nLivin' the Dream\n8.9\n2041\n2013-05-02\n\n\n186\n9\n22\nA.A.R.M.\n9.3\n2860\n2013-05-09\n\n\n187\n9\n23\nFinale\n9.7\n7934\n2013-05-16\n\n\n\n\n\n\n\nNote that the row is shown on the left. That will stop you getting lost in slices of the data.\nFor the top ten rows\n\ndf.iloc[:10,:]\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\nimdb_rating\ntotal_votes\nair_date\n\n\n\n\n0\n1\n1\nPilot\n7.6\n3706\n2005-03-24\n\n\n1\n1\n2\nDiversity Day\n8.3\n3566\n2005-03-29\n\n\n2\n1\n3\nHealth Care\n7.9\n2983\n2005-04-05\n\n\n3\n1\n4\nThe Alliance\n8.1\n2886\n2005-04-12\n\n\n4\n1\n5\nBasketball\n8.4\n3179\n2005-04-19\n\n\n5\n1\n6\nHot Girl\n7.8\n2852\n2005-04-26\n\n\n6\n2\n1\nThe Dundies\n8.7\n3213\n2005-09-20\n\n\n7\n2\n2\nSexual Harassment\n8.2\n2736\n2005-09-27\n\n\n8\n2\n3\nOffice Olympics\n8.4\n2742\n2005-10-04\n\n\n9\n2\n4\nThe Fire\n8.4\n2713\n2005-10-11\n\n\n\n\n\n\n\nOf course, we can run methods on these slices. We could, if we wanted to, calculate the mean imdb rating of only the first and last 100 episodes. Note the indexing starts at 0 so we want the column index of 3 (0:season, 1:episode, 2:title, 3:imdb_rating).\n\ndf.iloc[:100,3].mean()\n\n8.483\n\n\n\ndf.iloc[-100:,3].mean()\n\n8.062\n\n\nIf you are unsure how many rows you have then the count method comes to the rescue.\n\ndf.iloc[-100:,3].count()\n\n100\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nseason\nepisode\nimdb_rating\ntotal_votes\n\n\n\n\ncount\n188.000000\n188.000000\n188.000000\n188.000000\n\n\nmean\n5.468085\n11.877660\n8.257447\n2126.648936\n\n\nstd\n2.386245\n7.024855\n0.538067\n787.098275\n\n\nmin\n1.000000\n1.000000\n6.700000\n1393.000000\n\n\n25%\n3.000000\n6.000000\n7.900000\n1631.500000\n\n\n50%\n6.000000\n11.500000\n8.200000\n1952.500000\n\n\n75%\n7.250000\n18.000000\n8.600000\n2379.000000\n\n\nmax\n9.000000\n26.000000\n9.700000\n7934.000000\n\n\n\n\n\n\n\nSo it looks like the last 100 episodes were less good than the first 100. I guess that is why it was cancelled.\nOur data is organised by season. Looking at the average by season might help.\n\ndf[['season', 'imdb_rating']].groupby('season').mean()\n\n\n\n\n\n\n\n\nimdb_rating\n\n\nseason\n\n\n\n\n\n1\n8.016667\n\n\n2\n8.436364\n\n\n3\n8.573913\n\n\n4\n8.600000\n\n\n5\n8.492308\n\n\n6\n8.219231\n\n\n7\n8.316667\n\n\n8\n7.666667\n\n\n9\n7.956522\n\n\n\n\n\n\n\nThe above line groups our dataframe by values in the season column and then displays the mean for each group. Pretty nifty.\nSeason 8 looks pretty bad. We can look at just the rows for season 8.\n\ndf[df['season'] == 8]\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\nimdb_rating\ntotal_votes\nair_date\n\n\n\n\n141\n8\n1\nThe List\n8.2\n1829\n2011-09-22\n\n\n142\n8\n2\nThe Incentive\n8.2\n1668\n2011-09-29\n\n\n143\n8\n3\nLotto\n7.3\n1601\n2011-10-06\n\n\n144\n8\n4\nGarden Party\n8.1\n1717\n2011-10-13\n\n\n145\n8\n5\nSpooked\n7.6\n1543\n2011-10-27\n\n\n146\n8\n6\nDoomsday\n7.8\n1476\n2011-11-03\n\n\n147\n8\n7\nPam's Replacement\n7.7\n1563\n2011-11-10\n\n\n148\n8\n8\nGettysburg\n7.0\n1584\n2011-11-17\n\n\n149\n8\n9\nMrs. California\n7.7\n1553\n2011-12-01\n\n\n150\n8\n10\nChristmas Wishes\n8.0\n1547\n2011-12-08\n\n\n151\n8\n11\nTrivia\n7.9\n1488\n2012-01-12\n\n\n152\n8\n12\nPool Party\n8.0\n1612\n2012-01-19\n\n\n153\n8\n13\nJury Duty\n7.5\n1478\n2012-02-02\n\n\n154\n8\n14\nSpecial Project\n7.8\n1432\n2012-02-09\n\n\n155\n8\n15\nTallahassee\n7.9\n1522\n2012-02-16\n\n\n156\n8\n16\nAfter Hours\n8.1\n1567\n2012-02-23\n\n\n157\n8\n17\nTest the Store\n7.8\n1478\n2012-03-01\n\n\n158\n8\n18\nLast Day in Florida\n7.8\n1429\n2012-03-08\n\n\n159\n8\n19\nGet the Girl\n6.7\n1642\n2012-03-15\n\n\n160\n8\n20\nWelcome Party\n7.2\n1489\n2012-04-12\n\n\n161\n8\n21\nAngry Andy\n7.1\n1585\n2012-04-19\n\n\n162\n8\n22\nFundraiser\n7.1\n1453\n2012-04-26\n\n\n163\n8\n23\nTurf War\n7.7\n1393\n2012-05-03\n\n\n164\n8\n24\nFree Family Portrait Studio\n7.8\n1464\n2012-05-10\n\n\n\n\n\n\n\nWe can get an overview of the rating of all chapters within season 8 by:\n\ndf.loc[df['season'] == 8, 'imdb_rating'].describe()\n\ncount    24.000000\nmean      7.666667\nstd       0.405041\nmin       6.700000\n25%       7.450000\n50%       7.800000\n75%       7.925000\nmax       8.200000\nName: imdb_rating, dtype: float64\n\n\n\ndf['season'] == 8\n\n0      False\n1      False\n2      False\n3      False\n4      False\n       ...  \n183    False\n184    False\n185    False\n186    False\n187    False\nName: season, Length: 188, dtype: bool\n\n\nGenerally pretty bad, but there is clearly one very disliked episode."
  },
  {
    "objectID": "content/labs/Lab_2/IM939_Lab_2_1.html#adding-columns",
    "href": "content/labs/Lab_2/IM939_Lab_2_1.html#adding-columns",
    "title": "5  Lab: Pandas",
    "section": "5.5 Adding columns",
    "text": "5.5 Adding columns\nWe can add new columns pretty simply.\n\ndf['x'] = 44\ndf.head()\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\nimdb_rating\ntotal_votes\nair_date\nx\n\n\n\n\n0\n1\n1\nPilot\n7.6\n3706\n2005-03-24\n44\n\n\n1\n1\n2\nDiversity Day\n8.3\n3566\n2005-03-29\n44\n\n\n2\n1\n3\nHealth Care\n7.9\n2983\n2005-04-05\n44\n\n\n3\n1\n4\nThe Alliance\n8.1\n2886\n2005-04-12\n44\n\n\n4\n1\n5\nBasketball\n8.4\n3179\n2005-04-19\n44\n\n\n\n\n\n\n\nOur new column can be an operation on other columns\n\ndf['rating_div_total_votes'] = df['imdb_rating'] / df['total_votes']\ndf.head()\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\nimdb_rating\ntotal_votes\nair_date\nx\nrating_div_total_votes\n\n\n\n\n0\n1\n1\nPilot\n7.6\n3706\n2005-03-24\n44\n0.002051\n\n\n1\n1\n2\nDiversity Day\n8.3\n3566\n2005-03-29\n44\n0.002328\n\n\n2\n1\n3\nHealth Care\n7.9\n2983\n2005-04-05\n44\n0.002648\n\n\n3\n1\n4\nThe Alliance\n8.1\n2886\n2005-04-12\n44\n0.002807\n\n\n4\n1\n5\nBasketball\n8.4\n3179\n2005-04-19\n44\n0.002642\n\n\n\n\n\n\n\nor as simple as adding one to every value.\n\ndf['y'] = df['season'] + 1\ndf.iloc[0:5,:]\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\nimdb_rating\ntotal_votes\nair_date\nx\nrating_div_total_votes\ny\n\n\n\n\n0\n1\n1\nPilot\n7.6\n3706\n2005-03-24\n44\n0.002051\n2\n\n\n1\n1\n2\nDiversity Day\n8.3\n3566\n2005-03-29\n44\n0.002328\n2\n\n\n2\n1\n3\nHealth Care\n7.9\n2983\n2005-04-05\n44\n0.002648\n2\n\n\n3\n1\n4\nThe Alliance\n8.1\n2886\n2005-04-12\n44\n0.002807\n2\n\n\n4\n1\n5\nBasketball\n8.4\n3179\n2005-04-19\n44\n0.002642\n2\n\n\n\n\n\n\n\n\ny =  df['season'] + 1"
  },
  {
    "objectID": "content/labs/Lab_2/IM939_Lab_2_1.html#writing-data",
    "href": "content/labs/Lab_2/IM939_Lab_2_1.html#writing-data",
    "title": "5  Lab: Pandas",
    "section": "5.6 Writing data",
    "text": "5.6 Writing data\nPandas supports writing out data frames to various formats.\n\n?df.to_csv\n\nNow you can uncomment the code below to save your dataframe into a csv file. But before doing so, check that your data/output folder is empty:\n\n# df.to_csv('data/output/my_output_ratings.csv', encoding='UTF-8')\n\n\n?df.to_excel\n\nNow you can uncomment the code below to save your dataframe into an excel file. But before doing so, check that your data/output folder is empty:\n\n# df.to_excel('data/output/my_output_ratings.xlsx')"
  },
  {
    "objectID": "content/labs/Lab_2/IM939_Lab_2_2.html#plots",
    "href": "content/labs/Lab_2/IM939_Lab_2_2.html#plots",
    "title": "6  Lab: Visualising Data (1)",
    "section": "6.1 Plots",
    "text": "6.1 Plots"
  },
  {
    "objectID": "content/labs/Lab_2/IM939_Lab_2_2.html#univariate---a-single-variable",
    "href": "content/labs/Lab_2/IM939_Lab_2_2.html#univariate---a-single-variable",
    "title": "6  Lab: Visualising Data (1)",
    "section": "6.2 Univariate - a single variable",
    "text": "6.2 Univariate - a single variable\nPlots are a great way to see trends.\n\n?df.plot\n\n\ndf.plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\nWe can look at points instead of lines.\n\ndf['total_votes'].plot(title='Total Votes')\n\n&lt;Axes: title={'center': 'Total Votes'}&gt;\n\n\n\n\n\n\n?df.plot\n\n\ndf['imdb_rating'].plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\nOr we could create subplots.\n\ndf.plot(subplots=True)\n\narray([&lt;Axes: &gt;, &lt;Axes: &gt;, &lt;Axes: &gt;, &lt;Axes: &gt;], dtype=object)\n\n\n\n\n\nSeason and episode is not at all informative here.\n\ndf[['imdb_rating', 'total_votes']].plot(subplots=True)\n\narray([&lt;Axes: &gt;, &lt;Axes: &gt;], dtype=object)\n\n\n\n\n\n\n?df.plot\n\n\ndf[['imdb_rating', 'total_votes']].plot(subplots=True, kind='hist')\n\narray([&lt;Axes: ylabel='Frequency'&gt;, &lt;Axes: ylabel='Frequency'&gt;],\n      dtype=object)\n\n\n\n\n\nUnfortunatly, our x axis is bunched up. The above tells us that the all our IMDB ratings are between 0 and a little less than 1000… not useful.\nProbably best to plot them individually.\n\ndf[['imdb_rating']].plot(kind='hist')\n\n&lt;Axes: ylabel='Frequency'&gt;\n\n\n\n\n\nQuite a sensible guassian shape (a central point with the frequency decreasing symmetrically).\n\ndf[['total_votes']].plot(kind='hist')\n\n&lt;Axes: ylabel='Frequency'&gt;\n\n\n\n\n\nA positively skewed distribution - many smaller values and very few high values."
  },
  {
    "objectID": "content/labs/Lab_2/IM939_Lab_2_2.html#bivariate",
    "href": "content/labs/Lab_2/IM939_Lab_2_2.html#bivariate",
    "title": "6  Lab: Visualising Data (1)",
    "section": "6.3 Bivariate",
    "text": "6.3 Bivariate\nThe number of votes and the imdb rating are not independent events. These two data variables are related.\nScatter plots are simple ways to explore the relationship between two data variables. Note, I use the term data variables instead or just variables to avoid any confusion.\n\ndf.plot(x='imdb_rating', y='total_votes', kind='scatter', title='IMDB ratings and total number of votes')\n\n&lt;Axes: title={'center': 'IMDB ratings and total number of votes'}, xlabel='imdb_rating', ylabel='total_votes'&gt;\n\n\n\n\n\nThat is really interesting. The episodes with the highest rating also have the greatest number of votes. There was a cleary a great outpouring of happiness there.\nWhich episodes were they?\n\ndf[df['total_votes'] &gt; 5000]\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\nimdb_rating\ntotal_votes\nair_date\n\n\n\n\n77\n5\n13\nStress Relief\n9.6\n5948\n2009-02-01\n\n\n137\n7\n21\nGoodbye, Michael\n9.7\n5749\n2011-04-28\n\n\n187\n9\n23\nFinale\n9.7\n7934\n2013-05-16\n\n\n\n\n\n\n\nExcellent. Any influence of season on votes?\n\ndf.plot(x='season', y='imdb_rating', kind='scatter', title='IMDB ratings and season')\n\n&lt;Axes: title={'center': 'IMDB ratings and season'}, xlabel='season', ylabel='imdb_rating'&gt;\n\n\n\n\n\nSeason 8 seems to be a bit low. But nothing too extreme."
  },
  {
    "objectID": "content/labs/Lab_2/IM939_Lab_2_2.html#dates",
    "href": "content/labs/Lab_2/IM939_Lab_2_2.html#dates",
    "title": "6  Lab: Visualising Data (1)",
    "section": "6.4 Dates",
    "text": "6.4 Dates\nOur data contains air date information. Currently, that column is ‘object’ or a string.\n\ndf.head()\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\nimdb_rating\ntotal_votes\nair_date\n\n\n\n\n0\n1\n1\nPilot\n7.6\n3706\n2005-03-24\n\n\n1\n1\n2\nDiversity Day\n8.3\n3566\n2005-03-29\n\n\n2\n1\n3\nHealth Care\n7.9\n2983\n2005-04-05\n\n\n3\n1\n4\nThe Alliance\n8.1\n2886\n2005-04-12\n\n\n4\n1\n5\nBasketball\n8.4\n3179\n2005-04-19\n\n\n\n\n\n\n\n\ndf.dtypes\n\nseason           int64\nepisode          int64\ntitle           object\nimdb_rating    float64\ntotal_votes      int64\nair_date        object\ndtype: object\n\n\nWe can set this to be datetime instead. That will help us plot the time series of the data.\n\ndf['air_date'] =  pd.to_datetime(df['air_date'])\ndf.dtypes\n\nseason                  int64\nepisode                 int64\ntitle                  object\nimdb_rating           float64\ntotal_votes             int64\nair_date       datetime64[ns]\ndtype: object\n\n\n\ndf.plot(x = 'air_date', y = 'total_votes', kind='scatter')\n\n&lt;Axes: xlabel='air_date', ylabel='total_votes'&gt;\n\n\n\n\n\nWe can look at multiple variables using subplots.\n\ndf[['air_date', 'total_votes', 'imdb_rating']].plot(x = 'air_date', subplots=True)\n\narray([&lt;Axes: xlabel='air_date'&gt;, &lt;Axes: xlabel='air_date'&gt;], dtype=object)"
  },
  {
    "objectID": "content/labs/Lab_2/IM939_Lab_2_2.html#multivariate",
    "href": "content/labs/Lab_2/IM939_Lab_2_2.html#multivariate",
    "title": "6  Lab: Visualising Data (1)",
    "section": "6.5 Multivariate",
    "text": "6.5 Multivariate\nOur dataset is quite simple. But we can look at two variables (total_votes, imdb_rating) by a third (season).\n\ndf.groupby('season').plot(kind='scatter', y = 'total_votes', x = 'imdb_rating')\n\nseason\n1    Axes(0.125,0.11;0.775x0.77)\n2    Axes(0.125,0.11;0.775x0.77)\n3    Axes(0.125,0.11;0.775x0.77)\n4    Axes(0.125,0.11;0.775x0.77)\n5    Axes(0.125,0.11;0.775x0.77)\n6    Axes(0.125,0.11;0.775x0.77)\n7    Axes(0.125,0.11;0.775x0.77)\n8    Axes(0.125,0.11;0.775x0.77)\n9    Axes(0.125,0.11;0.775x0.77)\ndtype: object\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere is a lot more you can do with plots with Pandas and Matplotlib. A good resource is the visualisation section of the pandas documentation."
  },
  {
    "objectID": "content/labs/Lab_2/IM939_Lab_2_3.html#missing-values",
    "href": "content/labs/Lab_2/IM939_Lab_2_3.html#missing-values",
    "title": "7  Lab: Missing data",
    "section": "7.1 Missing values",
    "text": "7.1 Missing values\nI have removed some of the ratings data in the office_ratings_missing.csv.\n\nimport pandas as pd\ndf = pd.read_csv('data/raw/office_ratings_missing.csv', encoding = 'UTF-8')\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 188 entries, 0 to 187\nData columns (total 6 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   season       188 non-null    int64  \n 1   episode      188 non-null    int64  \n 2   title        188 non-null    object \n 3   imdb_rating  170 non-null    float64\n 4   total_votes  168 non-null    float64\n 5   air_date     188 non-null    object \ndtypes: float64(2), int64(2), object(2)\nmemory usage: 8.9+ KB\n\n\nWe are missing values in our imdb_rating and total votes columns.\n\ndf.shape[0] - df.count()\n\nseason          0\nepisode         0\ntitle           0\nimdb_rating    18\ntotal_votes    20\nair_date        0\ndtype: int64\n\n\nWhat to do? A quick solution is to either 0 the values or give them a roughtly central value (the mean).\nTo do this we use the fillna method.\n\ndf['imdb_rating_with_0'] = df['imdb_rating'].fillna(0)\n\nTo fill with the mean.\n\ndf['imdb_rating_with_mean'] = df['imdb_rating'].fillna(df['imdb_rating'].mean())\n\n\ndf.head()\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\nimdb_rating\ntotal_votes\nair_date\nimdb_rating_with_0\nimdb_rating_with_mean\n\n\n\n\n0\n1\n1\nPilot\n7.6\nNaN\n24/03/2005\n7.6\n7.6\n\n\n1\n1\n2\nDiversity Day\n8.3\n3566.0\n29/03/2005\n8.3\n8.3\n\n\n2\n1\n3\nHealth Care\n7.9\n2983.0\n05/04/2005\n7.9\n7.9\n\n\n3\n1\n4\nThe Alliance\n8.1\n2886.0\n12/04/2005\n8.1\n8.1\n\n\n4\n1\n5\nBasketball\n8.4\n3179.0\n19/04/2005\n8.4\n8.4\n\n\n\n\n\n\n\nWe can plot these to see what looks most reasonable (you can probably also make an educated guess here).\n\ndf['imdb_rating_with_mean'].plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\ndf['imdb_rating_with_0'].plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\nGoing with the mean seems quite sensible in this case. Especially as the data is guassian so the mean is probably an accurate represenation of the central value.\n\nax = df['imdb_rating'].hist()\nax.axvline(df['imdb_rating'].mean(), color='k', linestyle='--')\n\n&lt;matplotlib.lines.Line2D at 0x155ee8610&gt;"
  },
  {
    "objectID": "content/labs/Lab_2/IM939_Lab_2_3.html#transformations",
    "href": "content/labs/Lab_2/IM939_Lab_2_3.html#transformations",
    "title": "7  Lab: Missing data",
    "section": "7.2 Transformations",
    "text": "7.2 Transformations\nSome statistical models, such as standard linear regression, require the predicted variable to be gaussian distributed (a single central point and a roughly symmetrical decrease in frequency, see this Wolfram alpha page.\nThe distribution of votes is positively skewed (most values are low).\n\ndf['total_votes'].hist()\n\n&lt;Axes: &gt;\n\n\n\n\n\nA log transformation can make this data closer to a guassian distributed data variable. For the log transformation we are going to use numpy (numerical python) which is a rather excellent library.\n\nimport numpy as np\n\ndf['total_votes_log'] = np.log2(df['total_votes'])\n\n\ndf['total_votes_log'].hist()\n\n&lt;Axes: &gt;\n\n\n\n\n\nThat is less skewed, but not ideal. Perhaps a square root transformation instead?\n\ndf['total_votes_sqrt'] = np.sqrt(df['total_votes'])\ndf['total_votes_sqrt'].hist()\n\n&lt;Axes: &gt;\n\n\n\n\n\n…well, maybe a inverse/reciprocal transformation. It is possible we have hit the limit on what we can do.\n\ndf['total_votes_recip'] = np.reciprocal(df['total_votes'])\ndf['total_votes_recip'].hist()\n\n&lt;Axes: &gt;\n\n\n\n\n\nAt this point, I think we should conceded that we can make the distribution less positively skewed. However, transformation are not magic and we cannot turn a heavily positively skewed distribution into a normally distributed one.\nOh well.\nWe can calculate z scores though so we can plot both total_votes and imdb_ratings on a single plot. Currently, the IMDB scores vary between 0 and 10 whereas the number of votes number in the thousands.\n\ndf['total_votes_z'] = (df['total_votes'] - df['total_votes'].mean()) / df['total_votes'].std()\ndf['imdb_rating_z'] = (df['imdb_rating'] - df['imdb_rating'].mean()) / df['imdb_rating'].std()\n\n\ndf['total_votes_z'].hist()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\ndf['imdb_rating_z'].hist()\n\n&lt;Axes: &gt;\n\n\n\n\n\nNow we can compare the trends in score and number of votes on a single plot.\nWe are going to use a slightly different approach to creating the plots. Called to the plot() method from Pandas actually use a library called matplotlib. We are going to use the pyplot module of matplotlib directly.\n\nimport matplotlib.pyplot as plt\n\nConvert the air date into a datetime object.\n\ndf['air_date'] =  pd.to_datetime(df['air_date'])\n\n/var/folders/7v/zl9mv52s3ls94kntlt_l9ryh0000gq/T/ipykernel_40749/2716201442.py:1: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n  df['air_date'] =  pd.to_datetime(df['air_date'])\n\n\nThen call the subplots function fom pyplot to create two plots. From this we take the two plot axis (ax1, ax2) and call the method scatter for each to plot imdb_rating_z and total_votes_z.\n\nplt.style.use('ggplot')\n\nf, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\nax1.scatter( df['air_date'], df['imdb_rating_z'], color = 'red')\nax1.set_title('IMDB rating')\nax2.scatter( df['air_date'], df['total_votes_z'], color = 'blue')\nax2.set_title('Total votes')\n\nText(0.5, 1.0, 'Total votes')\n\n\n\n\n\n\nf\n\n\n\n\nWe can do better than that.\n\nplt.scatter(df['air_date'], df['imdb_rating_z'], color = 'red', alpha = 0.1)\nplt.scatter(df['air_date'], df['total_votes_z'], color = 'blue', alpha = 0.1)\n\n&lt;matplotlib.collections.PathCollection at 0x15611ff10&gt;\n\n\n\n\n\n\n?plt.scatter\n\nWe have done a lot so far. Exploring data in part 1, plotting data with the inbuilt Pandas methods in part 2 and dealing with both missing data and transfromations in part 3.\nIn part 4, we will look at creating your own functions, a plotting library called seaborn and introduce a larger dataset."
  },
  {
    "objectID": "content/labs/Lab_2/IM939_Lab_2_4.html#functions",
    "href": "content/labs/Lab_2/IM939_Lab_2_4.html#functions",
    "title": "8  Lab: Seaborn",
    "section": "8.1 Functions",
    "text": "8.1 Functions\nWe can define our own functions. A function helps us with code we are going to run multiple times. For instance, the below function scales values between 0 and 1.\nHere is a modified function from stackoverflow.\n\noffice_df.head()\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\nimdb_rating\ntotal_votes\nair_date\n\n\n\n\n0\n1\n1\nPilot\n7.6\n3706\n2005-03-24\n\n\n1\n1\n2\nDiversity Day\n8.3\n3566\n2005-03-29\n\n\n2\n1\n3\nHealth Care\n7.9\n2983\n2005-04-05\n\n\n3\n1\n4\nThe Alliance\n8.1\n2886\n2005-04-12\n\n\n4\n1\n5\nBasketball\n8.4\n3179\n2005-04-19\n\n\n\n\n\n\n\n\ndef normalize(df, feature_name):\n    result = df.copy()\n    \n    max_value = df[feature_name].max()\n    min_value = df[feature_name].min()\n    \n    result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n    \n    return result\n\nPassing the dataframe and name of the column will return a dataframe with that column scaled between 0 and 1.\n\nnormalize(office_df, 'imdb_rating')\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\nimdb_rating\ntotal_votes\nair_date\n\n\n\n\n0\n1\n1\nPilot\n0.300000\n3706\n2005-03-24\n\n\n1\n1\n2\nDiversity Day\n0.533333\n3566\n2005-03-29\n\n\n2\n1\n3\nHealth Care\n0.400000\n2983\n2005-04-05\n\n\n3\n1\n4\nThe Alliance\n0.466667\n2886\n2005-04-12\n\n\n4\n1\n5\nBasketball\n0.566667\n3179\n2005-04-19\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n183\n9\n19\nStairmageddon\n0.433333\n1484\n2013-04-11\n\n\n184\n9\n20\nPaper Airplane\n0.433333\n1482\n2013-04-25\n\n\n185\n9\n21\nLivin' the Dream\n0.733333\n2041\n2013-05-02\n\n\n186\n9\n22\nA.A.R.M.\n0.866667\n2860\n2013-05-09\n\n\n187\n9\n23\nFinale\n1.000000\n7934\n2013-05-16\n\n\n\n\n188 rows × 6 columns\n\n\n\nReplacing the origonal dataframe. We can normalize both out votes and rating.\n\noffice_df = normalize(office_df, 'imdb_rating')\n\n\noffice_df = normalize(office_df, 'total_votes')\n\n\noffice_df\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\nimdb_rating\ntotal_votes\nair_date\n\n\n\n\n0\n1\n1\nPilot\n0.300000\n0.353616\n2005-03-24\n\n\n1\n1\n2\nDiversity Day\n0.533333\n0.332212\n2005-03-29\n\n\n2\n1\n3\nHealth Care\n0.400000\n0.243082\n2005-04-05\n\n\n3\n1\n4\nThe Alliance\n0.466667\n0.228253\n2005-04-12\n\n\n4\n1\n5\nBasketball\n0.566667\n0.273047\n2005-04-19\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n183\n9\n19\nStairmageddon\n0.433333\n0.013912\n2013-04-11\n\n\n184\n9\n20\nPaper Airplane\n0.433333\n0.013606\n2013-04-25\n\n\n185\n9\n21\nLivin' the Dream\n0.733333\n0.099067\n2013-05-02\n\n\n186\n9\n22\nA.A.R.M.\n0.866667\n0.224278\n2013-05-09\n\n\n187\n9\n23\nFinale\n1.000000\n1.000000\n2013-05-16\n\n\n\n\n188 rows × 6 columns\n\n\n\nSeaborn prefers a long format table. Details of melt can be found here.\n\noffice_df_long=pd.melt(office_df, id_vars=['season', 'episode', 'title', 'air_date'], value_vars=['imdb_rating', 'total_votes'])\noffice_df_long\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\nair_date\nvariable\nvalue\n\n\n\n\n0\n1\n1\nPilot\n2005-03-24\nimdb_rating\n0.300000\n\n\n1\n1\n2\nDiversity Day\n2005-03-29\nimdb_rating\n0.533333\n\n\n2\n1\n3\nHealth Care\n2005-04-05\nimdb_rating\n0.400000\n\n\n3\n1\n4\nThe Alliance\n2005-04-12\nimdb_rating\n0.466667\n\n\n4\n1\n5\nBasketball\n2005-04-19\nimdb_rating\n0.566667\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n371\n9\n19\nStairmageddon\n2013-04-11\ntotal_votes\n0.013912\n\n\n372\n9\n20\nPaper Airplane\n2013-04-25\ntotal_votes\n0.013606\n\n\n373\n9\n21\nLivin' the Dream\n2013-05-02\ntotal_votes\n0.099067\n\n\n374\n9\n22\nA.A.R.M.\n2013-05-09\ntotal_votes\n0.224278\n\n\n375\n9\n23\nFinale\n2013-05-16\ntotal_votes\n1.000000\n\n\n\n\n376 rows × 6 columns\n\n\n\nWhich we can plot in seaborn like so.\n\nsns.relplot(x='air_date', y='value', size='variable', data=office_df_long)\n\n\n\n\n\n?sns.relplot"
  },
  {
    "objectID": "content/labs/Lab_3/IM939_Lab_3_1_Data_Processing_and_Summarization.html#outliers",
    "href": "content/labs/Lab_3/IM939_Lab_3_1_Data_Processing_and_Summarization.html#outliers",
    "title": "9  Lab: Data Processing and Summarization",
    "section": "9.1 Outliers",
    "text": "9.1 Outliers\n\nLoad the data on properties of cars into a pd dataframe\n\n\n# Loading Data\ndf = pd.read_csv('data/raw/accord_sedan.csv')\n\n# Inspecting the few first rows of the dataframe\ndf.head()\n\n\n\n\n\n\n\n\nprice\nmileage\nyear\ntrim\nengine\ntransmission\n\n\n\n\n0\n14995\n67697\n2006\nex\n4 Cyl\nManual\n\n\n1\n11988\n73738\n2006\nex\n4 Cyl\nManual\n\n\n2\n11999\n80313\n2006\nlx\n4 Cyl\nAutomatic\n\n\n3\n12995\n86096\n2006\nlx\n4 Cyl\nAutomatic\n\n\n4\n11333\n79607\n2006\nlx\n4 Cyl\nAutomatic\n\n\n\n\n\n\n\n\nVisualise the columns: “price” and “mileage”\n\n\n\n# 1D Visualizations\nplt.figure(figsize=(9, 5))\nplt.subplot(1,2,1)\nplt.boxplot(df.price)\nplt.title(\"Boxplot of the Price attribute\")\nplt.subplot(1,2,2)\nplt.boxplot(df.mileage)\nplt.title(\"Boxplot of the Mileage attribute\");\n\n# A semicolon in Python denotes separation, rather than termination. \n# It allows you to write multiple statements on the same line. \n\n\n\n\n\nIdentify the 2D outliers using the visualisation\n\n\n# 2D Visualization\nplt.scatter(df.price, df.mileage, alpha = .5)\nplt.xlabel('Price')\nplt.ylabel('Mileage')\nplt.title('Mileage vs. Price\\n');\n\n\n\n\n\nVisually, outliers appear to be outside the ‘normal’ range of the rest of the points. A few outliers are quite obvious to spot, but the choice of the threshold (the limit after which you decide to label a point as an outlier) visually remains a very subjective matter.\n\n4) Add two new columns to the dataframe called isOutlierPrice and isOutlierMileage. For the price column, calculate the mean and standard deviation. Find any rows that are more than 2 times standard deviations away from the mean and mark them with a 1 in the isOutlierPrice column. Do the same for mileage column\n\n# Computing the isOutlierPrice column\nupper_threshold_price = df.price.mean() + 2*df.price.std()\nlower_threshold_price = df.price.mean() - 2*df.price.std()\ndf['isOutlierPrice'] = ((df.price &gt; upper_threshold_price) | (df.price &lt; lower_threshold_price))\n\n# Computing the isOutlierMileage column\nupper_threshold_mileage = df.mileage.mean() + 2*df.mileage.std()\nlower_threshold_mileage = df.mileage.mean() - 2*df.mileage.std()\ndf['isOutlierMileage'] = ((df.mileage &gt; upper_threshold_mileage) | (df.mileage &lt; lower_threshold_mileage))\n\n# Second way of doing the above using the np.where() function\n#df['isOutlierPrice'] = np.where(abs(df.price - df.price.mean()) &lt; 2*df.price.std(), False, True)\n#df['isOutlierMileage'] = np.where(abs(df.mileage - df.price.mileage()) &lt; 2*df.mileage.std(), False, True)\n\n# Inspect the new DataFrame with the added columns\ndf.head()\n\n\n\n\n\n\n\n\nprice\nmileage\nyear\ntrim\nengine\ntransmission\nisOutlierPrice\nisOutlierMileage\n\n\n\n\n0\n14995\n67697\n2006\nex\n4 Cyl\nManual\nFalse\nFalse\n\n\n1\n11988\n73738\n2006\nex\n4 Cyl\nManual\nFalse\nFalse\n\n\n2\n11999\n80313\n2006\nlx\n4 Cyl\nAutomatic\nFalse\nFalse\n\n\n3\n12995\n86096\n2006\nlx\n4 Cyl\nAutomatic\nFalse\nFalse\n\n\n4\n11333\n79607\n2006\nlx\n4 Cyl\nAutomatic\nFalse\nFalse\n\n\n\n\n\n\n\n\nVisualize these values with a different color in the plot. Observe whether they are the same as you would mark them\n\n\n# Visualizing outliers in a different color\ncol = ['tomato' if i+j else 'seagreen' for i,j in zip(df.isOutlierPrice, df.isOutlierMileage)]\nplt.scatter(df.price, df.mileage, color = col)\nplt.xlabel('Price')\nplt.ylabel('Mileage')\nplt.title('Mileage vs. Price : Outliers 2+ std\\'s away from the mean\\n');\n\n\n\n\n\nVisually filtering out outliers can be an effective tactic if we’re just trying to conduct a quick and dirty experimentation. However, when we need to perform a solid and founded analysis, it’s better to have a robust justification for our choices. In this case, we can use the deviation from the mean to define a threshold that separates ‘normal’ values from ‘outliers’. Here, we opted for a two standard deviation threshold. The mathematical intuition behind this, is that under the normality assumption (if we assume our variable is normally distributed, which it almost is, refer to the next plot), then the probability of it having a value two standard deviations OR MORE away from the mean, is around 5%, which is very unlikely to happen. This is why we label these data points as outliers with respect to the (assumed) probability distribution of the variable. But this remains a way to identify 1D outliers only (identifying outliers within each column separately)\n\n\n# Histograms of Price and Mileage (checking the normality assumption)\nplt.subplot(1,2,1)\nplt.hist(df.price, bins = 12)\nplt.title('Price')\nplt.subplot(1,2,2)\nplt.hist(df.mileage, bins = 15)\nplt.title('Mileage');\n\n\n\n\n\nUsing the 2D Mahalanobis distance to find outliers\n\n\n# Mean vector (computing the mean returns a Series, which we need to convert back to a DataFrame because cdist requires it)\nmean_v = df.iloc[:, 0:2].mean().to_frame().T # DataFrame.T returns the Transpose of the DataFrame\n#mean_v = np.asarray([df.price.mean(), df.mileage.mean() ]).reshape(1,2) # This is a better way of writing the line before (for our use case : cdist function)\n\n# Computing the Mahalanobis distance of each row to the mean vector\nd = cdist(df.iloc[:, 0:2], mean_v, metric='mahalanobis')\n#d = cdist(df[['price', 'mileage']].values, mean_v, metric='mahalanobis') # Another way of writing the line before\n\n# Visualizing the scatter plot while coloring each point (i.e row) with a color from a chosen gradient colormap corresponding to the mahalanobis score\nplt.figure(figsize=(12, 5))\nplt.scatter(df.price, df.mileage, c = d.flatten(), cmap = 'plasma') # in order to know why we use flatten() on d, try printing d with and without flatten\nplt.colorbar() # to show the colorbar\nplt.xlabel('Price')\nplt.ylabel('Mileage')\nplt.title('Mileage vs. Price colored by a 2D Mahalanobis score\\n');"
  },
  {
    "objectID": "content/labs/Lab_3/IM939_Lab_3_1_Data_Processing_and_Summarization.html#q-q-plots",
    "href": "content/labs/Lab_3/IM939_Lab_3_1_Data_Processing_and_Summarization.html#q-q-plots",
    "title": "9  Lab: Data Processing and Summarization",
    "section": "9.2 Q-Q Plots",
    "text": "9.2 Q-Q Plots\n\n# Getting the Data in\ndf_tuber = pd.read_csv('data/raw/TB_burden_countries_2014-09-29.csv')\n\n# Filling missing numeric values (I repeat NUMERIC COLUMNS, I didn't touch categorical ones because we don't need them here)\ndf_tuber = df_tuber.fillna(value=df_tuber.mean(numeric_only = True))\n\n# Inspecting missing values in dataset\npd.isnull(df_tuber).sum()\n\ncountry                      0\niso2                        23\niso3                         0\niso_numeric                  0\ng_whoregion                  0\nyear                         0\ne_pop_num                    0\ne_prev_100k                  0\ne_prev_100k_lo               0\ne_prev_100k_hi               0\ne_prev_num                   0\ne_prev_num_lo                0\ne_prev_num_hi                0\ne_mort_exc_tbhiv_100k        0\ne_mort_exc_tbhiv_100k_lo     0\ne_mort_exc_tbhiv_100k_hi     0\ne_mort_exc_tbhiv_num         0\ne_mort_exc_tbhiv_num_lo      0\ne_mort_exc_tbhiv_num_hi      0\nsource_mort                  1\ne_inc_100k                   0\ne_inc_100k_lo                0\ne_inc_100k_hi                0\ne_inc_num                    0\ne_inc_num_lo                 0\ne_inc_num_hi                 0\ne_tbhiv_prct                 0\ne_tbhiv_prct_lo              0\ne_tbhiv_prct_hi              0\ne_inc_tbhiv_100k             0\ne_inc_tbhiv_100k_lo          0\ne_inc_tbhiv_100k_hi          0\ne_inc_tbhiv_num              0\ne_inc_tbhiv_num_lo           0\ne_inc_tbhiv_num_hi           0\nsource_tbhiv                 1\nc_cdr                        0\nc_cdr_lo                     0\nc_cdr_hi                     0\ndtype: int64\n\n\n\nPick one of the columns from the Tuberculosis data and copy it into a numpy array as before\n\n\n# Picking a column (I created a variable for this so I (and you (: ) can modify the column easily here and the change will be carried out everywhere I use the variable colname)\ncolname = 'e_prev_100k'\n\n# Creating a numpy array from our column\ncol = np.array(df_tuber[colname])\n\n# Printing the type of our newly created column\nprint(type(col))\n\n&lt;class 'numpy.ndarray'&gt;\n\n\n\nCompare this selected column to a Normal distribution. Then Sample from a Normal distribution and show a second Q-Q plot\n\n\n# Plotting the Q-Q Plot for our column\nsm.qqplot(col, line='r')\nplt.title('Q-Q Plot of the \"{}\" column of our dataset'.format(colname));\n\n# Plotting the Q-Q Plot for the log of our column\nsm.qqplot(np.log(col), line='r')\nplt.title('Q-Q Plot of the Log of the \"{}\" column'.format(colname));\n\n# Sampling from a Gaussian and a uniform distribution\nsample_norm = np.random.randn(1000)\nsample_unif = np.random.rand(1000)\n\n# Plotting the second Q-Q Plot for our sample (that was generated using a normal distribution)\nsm.qqplot(sample_norm, line='r')\nplt.title('Q-Q Plot of the generated sample (Gaussian)')\nsm.qqplot(sample_unif, line='r')\nplt.title('Q-Q Plot of the generated sample (Uniform)');\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGo ahead and change the colname variable (question 1) into a different column name (that you can pick from the list you have just before question 1 (but do pick a numeric column). And re-execute the code from question 1 and question 2 and you’ll see your new Q-Q plot of the column you just picked.\n\n\nHave a look at the slides from Week 03 for different shapes\n\n\nOk ? Now try to guess the shape of the distribution of our selected column (shape of its histogram) from its Q-Q Plot above.\n\n\nVisualise the column on a histogram and reflect on whether the shape you inferred from Q-Q plots and the shape of the histogram correlate\n\n\n# Histogramming the column we picked (not sure the verb exists though)\nplt.figure(figsize=(12, 5))\nplt.subplot(1,2,1)\nplt.title('Histogram of \"{}\"'.format(colname))\nplt.hist(col)\nplt.subplot(1,2,2)\nplt.title('Histogram of Log \"{}\"'.format(colname))\nplt.hist(np.log(col));\n\n\n\n\n\nOf course it does ! From the shape of the Q-Q Plot above (convex, slope upwards) and the Slide of Q-Q Plots from Week 3, we could conclude before looking at the histogram that our distribution was right tailed (or positively skewed if you’re into complex vocabulary lol). And it is !"
  },
  {
    "objectID": "content/labs/Lab_3/IM939_Lab_3_1_Data_Processing_and_Summarization.html#distributions-sampling",
    "href": "content/labs/Lab_3/IM939_Lab_3_1_Data_Processing_and_Summarization.html#distributions-sampling",
    "title": "9  Lab: Data Processing and Summarization",
    "section": "9.3 Distributions, Sampling",
    "text": "9.3 Distributions, Sampling\n\nInspecting the effect of sample size on descriptive statistics\n\n\n# Defining a few variables se we can change their values easiliy without having to change the rest of the code\nn = [5, 20, 100, 2000]\nstds = [0.5, 1, 3]\n\n# Initializing empty 2D arrays where we're going to store the results of our simulation\nmean = np.empty([len(n), len(stds)])\nstd = np.empty([len(n), len(stds)])\nskewness = np.empty([len(n), len(stds)])\nkurtos = np.empty([len(n), len(stds)])\n\n# Conducting the experiments and storing the results in the respective 2D arrays\nfor i, sample_size in enumerate(n):\n    for j, theoritical_std in enumerate(stds):\n        sample = np.random.normal(loc=0, scale=theoritical_std, size=sample_size)\n        mean[i,j] = sample.mean()\n        std[i,j] = sample.std()\n        skewness[i,j] = skew(sample)\n        kurtos[i,j] = kurtosis(sample)\n\n# Turning the mean 2D array into a pandas dataframe\nmean = pd.DataFrame(mean, columns = stds, index = n)\nmean = mean.rename_axis('Sample Size').rename_axis(\"Standard Deviation\", axis=\"columns\")\n\n# Turning the std 2D array into a pandas dataframe\nstd = pd.DataFrame(std, columns = stds, index = n)\nstd = std.rename_axis('Sample Size').rename_axis(\"Standard Deviation\", axis=\"columns\")\n\n# Turning the skewness 2D array into a pandas dataframe\nskewness = pd.DataFrame(skewness, columns = stds, index = n)\nskewness = skewness.rename_axis('Sample Size').rename_axis(\"Standard Deviation\", axis=\"columns\")\n\n# Turning the kurtosis 2D array into a pandas dataframe\nkurtos = pd.DataFrame(kurtos, columns = stds, index = n)\nkurtos = kurtos.rename_axis('Sample Size').rename_axis(\"Standard Deviation\", axis=\"columns\")\n\nprint(\"GAUSSIAN DISTRIBUTION\\n\")\nprint('Results for the Mean :')\nmean # This is a dataframe containing the means of the samples generated with different values of std and sample size\nprint('Results for the Standard Deviation :')\nstd # This is a dataframe containing the standard deviations of the samples generated with different values of std and sample size\nprint('Results for the Skewness :')\nskewness # This is a dataframe containing the skews of the samples generated with different values of std and sample size\nprint('Results for the Kurtosis :')\nkurtos # This is a dataframe containing the kurtosis of the samples generated with different values of std and sample size\n\nGAUSSIAN DISTRIBUTION\n\nResults for the Mean :\nResults for the Standard Deviation :\nResults for the Skewness :\nResults for the Kurtosis :\n\n\n\n\n\n\n\n\nStandard Deviation\n0.5\n1.0\n3.0\n\n\nSample Size\n\n\n\n\n\n\n\n5\n-0.803008\n-1.562678\n-1.453071\n\n\n20\n-0.918077\n-1.124123\n0.463808\n\n\n100\n0.038527\n-0.385476\n-0.589182\n\n\n2000\n-0.128110\n-0.015128\n-0.165063\n\n\n\n\n\n\n\n\nBasically, the more data you have (the bigger your sample), the more accurate your empirical estimates are going to be. Observe for example the values of mean (1st DataFrame) and variance (2nd DataFrame) for the 2000 sample size (last row). In the first one, the values are all close to 0, because we generated our sample from a Gaussian with mean 0, and the values in the second one are all close to the values in the column names (which refer to the variance of the distribution of the sample). This means that for with a sample size of 2000, our estimates are really close to the “True” values (with which we generated the sample). Also, the Skew of a Gaussian distribution should be 0, and it is confirmed in the 3rd DataFrame where the values are close to 0 in the last row (i.e big sample size).\n\n2) Same as before but with a Poisson distribution (which has just one parameter lambda instead of 2 like the gaussian)\n\n# Defining a few variables se we can change their values easiliy without having to change the rest of the code\nn = [5, 20, 100, 2000]\nlambd = [0.5, 1, 3] # In a gaussian we had two parameters, a var specified here and a mean we chose to be 0 \n#everywhere. Here we have one parameter called lambda.\n\n# Initializing empty 2D arrays where we're going to store the results of our simulation\nmean = np.empty([len(n), len(lambd)])\nstd = np.empty([len(n), len(lambd)])\nskewness = np.empty([len(n), len(lambd)])\nkurtos = np.empty([len(n), len(lambd)])\n\n# Conducting the experiments and storing the results in the respective 2D arrays\nfor i, sample_size in enumerate(n):\n    for j, theoritical_lambd in enumerate(lambd):\n        #**********************************************************************\n        sample = np.random.poisson(lam = theoritical_lambd, size = sample_size) # THIS IS WHAT WE CHANGED IN Q2 !\n        #**********************************************************************\n        mean[i,j] = sample.mean()\n        std[i,j] = sample.std()\n        skewness[i,j] = skew(sample)\n        kurtos[i,j] = kurtosis(sample)\n\n# Turning the mean 2D array into a pandas dataframe\nmean = pd.DataFrame(mean, columns = lambd, index = n)\nmean = mean.rename_axis('Sample Size').rename_axis(\"Lambda\", axis=\"columns\")\n\n# Turning the std 2D array into a pandas dataframe\nstd = pd.DataFrame(std, columns = lambd, index = n)\nstd = std.rename_axis('Sample Size').rename_axis(\"Lambda\", axis=\"columns\")\n\n# Turning the skewness 2D array into a pandas dataframe\nskewness = pd.DataFrame(skewness, columns = lambd, index = n)\nskewness = skewness.rename_axis('Sample Size').rename_axis(\"Lambda\", axis=\"columns\")\n\n# Turning the kurtosis 2D array into a pandas dataframe\nkurtos = pd.DataFrame(kurtos, columns = lambd, index = n)\nkurtos = kurtos.rename_axis('Sample Size').rename_axis(\"Lambda\", axis=\"columns\")\n\nprint(\"POISSON DISTRIBUTION\\n\")\nprint('Results for the Mean :')\nmean # This is a dataframe containing the means of the samples generated with different values of std and sample size\nprint('Results for the Standard Deviation :')\nstd # This is a dataframe containing the standard deviations of the samples generated with different values of std \n#and sample size\nprint('Results for the Skewness :')\nskewness # This is a dataframe containing the skews of the samples generated with different values of std and sample \n#size\nprint('Results for the Kurtosis :')\nkurtos # This is a dataframe containing the kurtosis of the samples generated with different values of std and sample \n#size\n\n\nPOISSON DISTRIBUTION\n\nResults for the Mean :\nResults for the Standard Deviation :\nResults for the Skewness :\nResults for the Kurtosis :\n\n\n\n\n\n\n\n\nLambda\n0.5\n1.0\n3.0\n\n\nSample Size\n\n\n\n\n\n\n\n5\n0.250000\n0.250000\n-1.460184\n\n\n20\n-0.448980\n-0.634516\n-0.649787\n\n\n100\n1.665828\n0.181929\n-0.140095\n\n\n2000\n2.054000\n0.834341\n0.537287\n\n\n\n\n\n\n\n\nJust remember, the lambda parameter that defines the Poisson distribution is also the mean of the distribution. This is confirmed in the first DataFrame where the values (means of samples) are close to the column labels (theoretical lambda which is also equal to theoretical mean), especially in the last row."
  },
  {
    "objectID": "content/labs/Lab_3/IM939_Lab_3_1_Data_Processing_and_Summarization.html#robust-statistics",
    "href": "content/labs/Lab_3/IM939_Lab_3_1_Data_Processing_and_Summarization.html#robust-statistics",
    "title": "9  Lab: Data Processing and Summarization",
    "section": "9.4 Robust Statistics",
    "text": "9.4 Robust Statistics\n\nChoose a number of columns with different shapes, for instance, “e_prev_100k_hi” is left skewed or some columns where the variation is high or you notice potential outliers. You can make use of a series of boxplots to exploratively analyse the data for outliers\n\n\n# Listing the columns\ndf_tuber.columns\n\nIndex(['country', 'iso2', 'iso3', 'iso_numeric', 'g_whoregion', 'year',\n       'e_pop_num', 'e_prev_100k', 'e_prev_100k_lo', 'e_prev_100k_hi',\n       'e_prev_num', 'e_prev_num_lo', 'e_prev_num_hi', 'e_mort_exc_tbhiv_100k',\n       'e_mort_exc_tbhiv_100k_lo', 'e_mort_exc_tbhiv_100k_hi',\n       'e_mort_exc_tbhiv_num', 'e_mort_exc_tbhiv_num_lo',\n       'e_mort_exc_tbhiv_num_hi', 'source_mort', 'e_inc_100k', 'e_inc_100k_lo',\n       'e_inc_100k_hi', 'e_inc_num', 'e_inc_num_lo', 'e_inc_num_hi',\n       'e_tbhiv_prct', 'e_tbhiv_prct_lo', 'e_tbhiv_prct_hi',\n       'e_inc_tbhiv_100k', 'e_inc_tbhiv_100k_lo', 'e_inc_tbhiv_100k_hi',\n       'e_inc_tbhiv_num', 'e_inc_tbhiv_num_lo', 'e_inc_tbhiv_num_hi',\n       'source_tbhiv', 'c_cdr', 'c_cdr_lo', 'c_cdr_hi'],\n      dtype='object')\n\n\n\n# Alright I already know a few columns with outliers but let's try to find them together exploratively using BoxPlots\ncolname = 'c_cdr' # change the column name by choosing different ones from above (numeric ones)\nplt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nplt.hist(df_tuber[colname])\nplt.title('Histogram of \"{}\"'.format(colname))\nplt.subplot(1,2,2)\nplt.boxplot(df_tuber[colname]);\nplt.title('Boxplot of \"{}\"'.format(colname));\n\n\n\n\n\n# Chosen columns : I picked 3, feel free to change them and experiment\nchosen_colnames = ['e_pop_num', 'e_prev_100k', 'c_cdr']\n\n2) For the chosen columns, estimate both the conventional and the robust descriptive statistics and compare. Observe how these pairs deviate from each other based on the characteristics of the underlying data\n\n# Central Tendency : Mean vs Median (Median is the robust version of the mean, because it takes into account \n#the ordering of the points and not the actual values like the mean does)\ndf_tuber[chosen_colnames].describe().loc[['mean', '50%'], :] # The 50% is the median (50% quantile)\n\n\n\n\n\n\n\n\ne_pop_num\ne_prev_100k\nc_cdr\n\n\n\n\nmean\n2.899179e+07\n207.694422\n67.570706\n\n\n50%\n5.140332e+06\n93.000000\n70.000000\n\n\n\n\n\n\n\nLook at how the values are different between the mean and the median … LOOOOOOOK ! This is why when you have a skewed (unsymmetrical) distribution it’s usually more interesting to use the median as a measure of the central tendency of the data. One important thing to note here, for the two first attributes, the mean is higher than the median, but for the last it’s the opposite. This can tell you a thing or two about the shape of your distribution : if the mean is higher than the median, this means that the distribution is skewed to the right (right tail) which pulls the mean higher. And vice-versa.\nMoral of the story is … outliers are a pain in the a**.\n\n# Spread : Standard Deviation vs Inter-Quartile Range vs Median Absolute Deviation (MAD)\nstds = df_tuber[chosen_colnames].std()\niqrs = df_tuber[chosen_colnames].quantile(0.75) - df_tuber[chosen_colnames].quantile(0.25)\nmedianAD = mad(df_tuber[chosen_colnames])\n\noutput = pd.DataFrame(stds, columns = ['std']).T\noutput = pd.concat([output, pd.DataFrame(iqrs, columns = ['IQR']).T], ignore_index=False)\noutput = pd.concat([output, pd.DataFrame(medianAD, columns = ['MAD'], index = chosen_colnames).T], ignore_index=False, names = ['std', 'iqr', 'mad'])\noutput\n\n\n\n\n\n\n\n\ne_pop_num\ne_prev_100k\nc_cdr\n\n\n\n\nstd\n1.177827e+08\n269.418159\n25.234773\n\n\nIQR\n1.677193e+07\n280.500000\n34.000000\n\n\nMAD\n7.454908e+06\n120.090780\n25.204238\n\n\n\n\n\n\n\nThe values here are different as well, maybe more so for the “e_pop_num” attribute than the others, but that is just because of the scaling : “e_pop_num” takes big values overall compared to the other columns, which you can check with the mean values right above.\nFor the first attribute, the standard deviation is higher, and both the IQR and MAD are close to each other. For the second attribute, the inter-quartile range is slightly higher than the standard deviation, but the MAD is far below (less than half) the other two values, and the reason for that is a little bit involved : Basically, the standard deviation measures the spread by computing the squared deviation from the mean while the median absolute deviation evaluates the spread by computing the absolute deviation. This means that when the outliers have much bigger values than the “normal” points, the squared difference explodes (figuratively of course ;p) compared to the absolute difference. And this is actually the case for our second distribution (e_prev_100k) where most values are between 50 and 300 while many outliers lay above the 750 mark and go all the way up to 1800 (look at the boxplots below). For the third attribute the values are somewhat close, especially the std and the MAD, that’s because if you inspect the boxplot, this column doesn’t have many outliers to begin with.\nNonetheless, the differences are real, and if we don’t want to have to handle outliers, then we should be using robust statistics like the median to describe the central tendency and inter-quartile range or median absolute deviation to measure the spread of our data.\n\n# Boxplots of the different columns\nplt.figure(figsize=(12,20))\n\nplt.subplot(3,2,1)\nplt.hist(df_tuber[chosen_colnames[0]])\nplt.title('Histogram of \"{}\"'.format(chosen_colnames[0]))\nplt.subplot(3,2,2)\nplt.boxplot(df_tuber[chosen_colnames[0]])\nplt.title('Boxplot of \"{}\"'.format(chosen_colnames[0]))\n\nplt.subplot(3,2,3)\nplt.hist(df_tuber[chosen_colnames[1]])\nplt.title('Histogram of \"{}\"'.format(chosen_colnames[1]))\nplt.subplot(3,2,4)\nplt.boxplot(df_tuber[chosen_colnames[1]])\nplt.title('Boxplot of \"{}\"'.format(chosen_colnames[1]))\n\nplt.subplot(3,2,5)\nplt.hist(df_tuber[chosen_colnames[2]])\nplt.title('Histogram of \"{}\"'.format(chosen_colnames[2]))\nplt.subplot(3,2,6)\nplt.boxplot(df_tuber[chosen_colnames[2]])\nplt.title('Boxplot of \"{}\"'.format(chosen_colnames[2]));"
  },
  {
    "objectID": "content/labs/Lab_3/IM939_Lab_3_2_Linear_Regression.html#sea-ice-dataset",
    "href": "content/labs/Lab_3/IM939_Lab_3_2_Linear_Regression.html#sea-ice-dataset",
    "title": "10  Lab: Regressions",
    "section": "10.1 Sea Ice Dataset",
    "text": "10.1 Sea Ice Dataset\nThe script below is the based on the Sea Ice dataset from Igual and Seguí (2017). You can find the data here: ftp://sidads.colorado.edu/DATASETS/NOAA/ or read more about the project here: https://nsidc.org/data/seaice_index\nThis notebook will walk you through an example of a simple linear regression and the other notebook “IM939 Lab 3 - Linear Regression Exercise.ipynb” will include the example of a multiple linear regression, too."
  },
  {
    "objectID": "content/labs/Lab_3/IM939_Lab_3_2_Linear_Regression.html#simple-and-multiple-linear-regression",
    "href": "content/labs/Lab_3/IM939_Lab_3_2_Linear_Regression.html#simple-and-multiple-linear-regression",
    "title": "10  Lab: Regressions",
    "section": "10.2 Simple and Multiple Linear Regression",
    "text": "10.2 Simple and Multiple Linear Regression\nIn the linear model the response \\(\\textbf{y}\\) depends linearly from the covariates \\(\\textbf{x}_i\\).\nIn the simple linear regression, with a single variable, we described the relationship between the predictor and the response with a straight line. The general linear model: \\[ \\textbf{y}  =  a_0+ a_1 \\textbf{x}_1 \\]\nThe parameter \\(a_0\\) is called the constant term or the intercept.\nIn the case of multiple linear regression we extend this idea by fitting a m-dimensional hyperplane to our m predictors.\n\\[ \\textbf{y}  =  a_1 \\textbf{x}_1  + \\dots + a_m \\textbf{x}_{m} \\]\nThe \\(a_i\\) are termed the parameters of the model or the coefficients."
  },
  {
    "objectID": "content/labs/Lab_3/IM939_Lab_3_2_Linear_Regression.html#ordinary-least-squares",
    "href": "content/labs/Lab_3/IM939_Lab_3_2_Linear_Regression.html#ordinary-least-squares",
    "title": "10  Lab: Regressions",
    "section": "10.3 Ordinary Least Squares",
    "text": "10.3 Ordinary Least Squares\nOrdinary Least Squares (OLS) is the simplest and most common estimator in which the coefficients \\(a\\)’s of the simple linear regression: \\(\\textbf{y} = a_0+a_1 \\textbf{x}\\), are chosen to minimize the square of the distance between the predicted values and the actual values.\n\\[ ||a_0 + a_1 \\textbf{x} -  \\textbf{y} ||^2_2 = \\sum_{j=1}^n (a_0+a_1 x_{j} -  y_j )^2,\\]\nThis expression is often called sum of squared errors of prediction (SSE)."
  },
  {
    "objectID": "content/labs/Lab_3/IM939_Lab_3_2_Linear_Regression.html#case-study-climate-change-and-sea-ice-extent",
    "href": "content/labs/Lab_3/IM939_Lab_3_2_Linear_Regression.html#case-study-climate-change-and-sea-ice-extent",
    "title": "10  Lab: Regressions",
    "section": "10.4 Case study: Climate Change and Sea Ice Extent",
    "text": "10.4 Case study: Climate Change and Sea Ice Extent\nThe question: Has there been a decrease in the amount of ice in the last years?\n\n10.4.1 Reading Data\nThere are five steps. First, let’s load the data that is already in the data folder: SeaIce.txt. It is a text file, specifically is a Tab separated file where each Tab delimites the following columns:\n\nYear: 4-digit year\nmo: 1- or 2-digit month\ndata_type: Input data set (Goddard/NRTSI-G)\nregion: Hemisphere that this data covers (N: Northern; S: Southern)\nextent: Sea ice extent in millions of square km\narea: Sea ice area in millions of square km\n\nOnce we upload the data, we can create a DataFrame1 using Pandas.1 A reminder what is DataFrame: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\n\nimport pandas as pd\nice = pd.read_csv('data/raw/SeaIce.txt', \n                  delim_whitespace=True)\n\nprint('shape:', ice.shape) #this returns number of rows and columns in a dataset\nice.head() \n\nshape: (424, 6)\n\n\n\n\n\n\n\n\n\nyear\nmo\ndata_type\nregion\nextent\narea\n\n\n\n\n0\n1979\n1\nGoddard\nN\n15.54\n12.33\n\n\n1\n1980\n1\nGoddard\nN\n14.96\n11.85\n\n\n2\n1981\n1\nGoddard\nN\n15.03\n11.82\n\n\n3\n1982\n1\nGoddard\nN\n15.26\n12.11\n\n\n4\n1983\n1\nGoddard\nN\n15.10\n11.92\n\n\n\n\n\n\n\nWe can compute the mean for that interval of time (1981 through 2010), before data cleaning.\n\nice.mean(numeric_only = True)\n\nyear      1996.000000\nmo           6.500000\nextent     -35.443066\narea       -37.921108\ndtype: float64\n\n\nDid we receive a negative mean…?\n\n\n10.4.2 Data visualisation to explore data\nDo you remember Seaborn? We will use lmplot() function from Seaborn to explore linear relationship of different forms, e.g. relationship between the month of the year (variable) and extent (responses):\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n\n# Visualize the data\nx = ice.year\ny = ice.extent\nplt.scatter(x, y, color = 'red')\nplt.xlabel('Year')\nplt.ylabel('Extent')\n\nText(0, 0.5, 'Extent')\n\n\n\n\n\nWe detect some outlier or missing data. we are going to use function np.unique and find the unique elements of an array.\n\n?np.unique\n\n\nprint ('Different values in data_type field:', np.unique(ice.data_type.values))   # there is a -9999 value!\n\nDifferent values in data_type field: ['-9999' 'Goddard' 'NRTSI-G']\n\n\nLet’s see what type of data we have, other than the ones printed above\n\nprint (ice[(ice.data_type != 'Goddard')\n          & (ice.data_type != 'NRTSI-G')])\n\n     year  mo data_type region  extent    area\n9    1988   1     -9999      N -9999.0 -9999.0\n397  1987  12     -9999      N -9999.0 -9999.0\n\n\nData cleaning: we checked all the values and notice -9999 values in data_type field which should contain ‘Goddard’ or ‘NRTSI-G’ (some type of input dataset). So we clean them by removing these instances\n\n# We can easily clean the data now:\nice2 = ice[ice.data_type != '-9999']\nprint ('shape:', ice2.shape)\n# And repeat the plot\nx = ice2.year\ny = ice2.extent\nplt.scatter(x, y, color = 'red')\nplt.xlabel('Month')\nplt.ylabel('Extent')\n\nshape: (422, 6)\n\n\nText(0, 0.5, 'Extent')\n\n\n\n\n\n\nsns.lmplot(data = ice2, x = \"mo\", y = \"extent\")\n\n\n\n\nAbove you can see ice extent data by month. You can see a monthly fluctuation of the sea ice extent, as would be expected for the different seasons of the year. In order to run regression, and avoid this fluctuation we can normalize data. This will let us see the evolution of the extent over the years.\n\n\n10.4.3 Normalization\nThe lmplot() function from the Seaborn module is intended for exploring linear relationships of different forms in multidimensional datesets. Input data must be in a Pandas DataFrame. To plot them, we provide the predictor and response variable names along with the dataset\n\nsns.lmplot(ice2, x = \"mo\", y = \"extent\", height = 5.2, aspect = 2);\n\n# Uncomment below to save the resulting plot.\n#plt.savefig(\"figs/CleanedByMonth.png\", dpi = 300, bbox_inches = 'tight')\n\n\n\n\n\n# Compute the mean for each month.\ngrouped = ice2.groupby('mo')\nmonth_means = grouped.extent.mean()\nmonth_variances = grouped.extent.var()\nprint ('Means:', month_means)\nprint ('Variances:',month_variances)\n\nMeans: mo\n1     14.479429\n2     15.298889\n3     15.491714\n4     14.766000\n5     13.396000\n6     11.860000\n7      9.601143\n8      7.122286\n9      6.404857\n10     8.809143\n11    10.964722\n12    13.059429\nName: extent, dtype: float64\nVariances: mo\n1     0.304906\n2     0.295804\n3     0.237209\n4     0.215378\n5     0.189901\n6     0.247918\n7     0.679175\n8     0.824577\n9     1.143902\n10    0.630361\n11    0.412511\n12    0.284870\nName: extent, dtype: float64\n\n\nTo capture variation per month, we can compute mean for the i-th interval of time (using 1979-2014) and subtract it from the set of extent values for that month . This can be converted to a relative pecentage difference by dividing it by the total avareage (1979-2014) and multiplying by 100.\n\n# Data normalization\nfor i in range(12):\n    ice2.extent[ice2.mo == i+1] = 100*(ice2.extent[ice2.mo == i+1] - month_means[i+1])/month_means.mean().copy()\n    \nsns.lmplot(ice2 , x = \"mo\", y = \"extent\", height = 5.2, aspect = 2);\nplt.savefig(\"figs/IceExtentNormalizedByMonth.png\", dpi = 300, bbox_inches = 'tight')\n\n/var/folders/7v/zl9mv52s3ls94kntlt_l9ryh0000gq/T/ipykernel_40896/751478846.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  ice2.extent[ice2.mo == i+1] = 100*(ice2.extent[ice2.mo == i+1] - month_means[i+1])/month_means.mean().copy()\n/var/folders/7v/zl9mv52s3ls94kntlt_l9ryh0000gq/T/ipykernel_40896/751478846.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  ice2.extent[ice2.mo == i+1] = 100*(ice2.extent[ice2.mo == i+1] - month_means[i+1])/month_means.mean().copy()\n/var/folders/7v/zl9mv52s3ls94kntlt_l9ryh0000gq/T/ipykernel_40896/751478846.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  ice2.extent[ice2.mo == i+1] = 100*(ice2.extent[ice2.mo == i+1] - month_means[i+1])/month_means.mean().copy()\n/var/folders/7v/zl9mv52s3ls94kntlt_l9ryh0000gq/T/ipykernel_40896/751478846.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  ice2.extent[ice2.mo == i+1] = 100*(ice2.extent[ice2.mo == i+1] - month_means[i+1])/month_means.mean().copy()\n/var/folders/7v/zl9mv52s3ls94kntlt_l9ryh0000gq/T/ipykernel_40896/751478846.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  ice2.extent[ice2.mo == i+1] = 100*(ice2.extent[ice2.mo == i+1] - month_means[i+1])/month_means.mean().copy()\n/var/folders/7v/zl9mv52s3ls94kntlt_l9ryh0000gq/T/ipykernel_40896/751478846.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  ice2.extent[ice2.mo == i+1] = 100*(ice2.extent[ice2.mo == i+1] - month_means[i+1])/month_means.mean().copy()\n/var/folders/7v/zl9mv52s3ls94kntlt_l9ryh0000gq/T/ipykernel_40896/751478846.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  ice2.extent[ice2.mo == i+1] = 100*(ice2.extent[ice2.mo == i+1] - month_means[i+1])/month_means.mean().copy()\n/var/folders/7v/zl9mv52s3ls94kntlt_l9ryh0000gq/T/ipykernel_40896/751478846.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  ice2.extent[ice2.mo == i+1] = 100*(ice2.extent[ice2.mo == i+1] - month_means[i+1])/month_means.mean().copy()\n/var/folders/7v/zl9mv52s3ls94kntlt_l9ryh0000gq/T/ipykernel_40896/751478846.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  ice2.extent[ice2.mo == i+1] = 100*(ice2.extent[ice2.mo == i+1] - month_means[i+1])/month_means.mean().copy()\n/var/folders/7v/zl9mv52s3ls94kntlt_l9ryh0000gq/T/ipykernel_40896/751478846.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  ice2.extent[ice2.mo == i+1] = 100*(ice2.extent[ice2.mo == i+1] - month_means[i+1])/month_means.mean().copy()\n/var/folders/7v/zl9mv52s3ls94kntlt_l9ryh0000gq/T/ipykernel_40896/751478846.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  ice2.extent[ice2.mo == i+1] = 100*(ice2.extent[ice2.mo == i+1] - month_means[i+1])/month_means.mean().copy()\n/var/folders/7v/zl9mv52s3ls94kntlt_l9ryh0000gq/T/ipykernel_40896/751478846.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  ice2.extent[ice2.mo == i+1] = 100*(ice2.extent[ice2.mo == i+1] - month_means[i+1])/month_means.mean().copy()\n\n\n\n\n\n\nprint ('mean:', ice2.extent.mean())\nprint ('var:', ice2.extent.var())\n\nmean: -7.745252569896827e-16\nvar: 31.983239774968798\n\n\nLet us consider the entire year\n\nsns.lmplot(ice2, x = \"year\", y = \"extent\", height = 5.2, aspect = 2);\nplt.savefig(\"figs/IceExtentAllMonthsByYearlmplot.png\", dpi = 300, bbox_inches = 'tight')\n\n\n\n\n\n\n10.4.4 Pearson’s correlation\nLet’s calculate Pearson’s correlation coefficient and the p-value for testing non-correlation.\n\nimport scipy.stats\nscipy.stats.pearsonr(ice2.year.values, ice2.extent.values)\n\nPearsonRResult(statistic=-0.8183500709897178, pvalue=4.4492318168687107e-103)\n\n\n\n\n10.4.5 Simple OLS\nWe can also compute the trend as a simple linear regression (OLS) and quantitatively evaluate it.\nFor that we use using Scikit-learn, library that provides a variety of both supervised and unsupervised machine learning techniques. Scikit-learn provides an object-oriented interface centered around the concept of an Estimator. The Estimator.fit method sets the state of the estimator based on the training data. Usually, the data is comprised of a two-dimensional numpy array \\(X\\) of shape (n_samples, n_predictors) that holds the so-called feature matrix and a one-dimensional numpy array \\(\\textbf{y}\\) that holds the responses. Some estimators allow the user to control the fitting behavior. For example, the sklearn.linear_model.LinearRegression estimator allows the user to specify whether or not to fit an intercept term. This is done by setting the corresponding constructor arguments of the estimator object. During the fitting process, the state of the estimator is stored in instance attributes that have a trailing underscore (’’). For example, the coefficients of a LinearRegression estimator are stored in the attribute coef.\nEstimators that can generate predictions provide a Estimator.predict method. In the case of regression, Estimator.predict will return the predicted regression values, \\(\\hat{\\textbf{y}}\\).\n\nfrom sklearn.linear_model import LinearRegression\n\nest = LinearRegression(fit_intercept = True)\n\nx = ice2[['year']]\ny = ice2[['extent']]\n\nest.fit(x, y)\n\nprint(\"Coefficients:\", est.coef_)\nprint (\"Intercept:\", est.intercept_)\n\nCoefficients: [[-0.45275459]]\nIntercept: [903.71640207]\n\n\nWe can evaluate the model fitting by computing the mean squared error (\\(MSE\\)) and the coefficient of determination (\\(R^2\\)) of the model. The coefficient \\(R^2\\) is defined as \\((1 - \\textbf{u}/\\textbf{v})\\), where \\(\\textbf{u}\\) is the residual sum of squares \\(\\sum (\\textbf{y} - \\hat{\\textbf{y}})^2\\) and \\(\\textbf{v}\\) is the regression sum of squares \\(\\sum (\\textbf{y} - \\bar{\\textbf{y}})^2\\), where \\(\\bar{\\textbf{y}}\\) is the mean. The best possible score for \\(R^2\\) is 1.0: lower values are worse. These measures can provide a quantitative answer to the question we are facing: Is there a negative trend in the evolution of sea ice extent over recent years?\n\nfrom sklearn import metrics\n\n# Analysis for all months together.\nx = ice2[['year']]\ny = ice2[['extent']]\nmodel = LinearRegression()\nmodel.fit(x, y)\ny_hat = model.predict(x)\nplt.plot(x, y,'o', alpha = 0.5)\nplt.plot(x, y_hat, 'r', alpha = 0.5)\nplt.xlabel('year')\nplt.ylabel('extent (All months)')\nprint (\"MSE:\", metrics.mean_squared_error(y_hat, y))\nprint (\"R^2:\", metrics.r2_score(y_hat, y))\nprint (\"var:\", y.var())\nplt.savefig(\"figs/IceExtentLinearRegressionAllMonthsByYearPrediction.png\", dpi = 300, bbox_inches = 'tight')\n\nMSE: 10.539131639803488\nR^2: 0.5067870382100226\nvar: extent    31.98324\ndtype: float64\n\n\n\n\n\nWe can conclude that the data show a long-term negative trend in recent years.\n\n# Analysis for a particular month.\n#For January\njan = ice2[ice2.mo == 1];\n\nx = jan[['year']]\ny = jan[['extent']]\n\nmodel = LinearRegression()\nmodel.fit(x, y)\n\ny_hat = model.predict(x)\n\nplt.figure()\nplt.plot(x, y,'-o', alpha = 0.5)\nplt.plot(x, y_hat, 'r', alpha = 0.5)\nplt.xlabel('year')\nplt.ylabel('extent (January)')\n\nprint (\"MSE:\", metrics.mean_squared_error(y_hat, y))\nprint (\"R^2:\", metrics.r2_score(y_hat, y))\n\nMSE: 3.8395160752867565\nR^2: 0.7810636041396216\n\n\n\n\n\nWe can also estimate the extent value for 2025. For that we use the function predict of the model.\n\nX = np.array(2025) \ny_hat = model.predict(X.reshape(-1, 1))\nj = 1 # January\n# Original value (before normalization)\ny_hat = (y_hat*month_means.mean()/100) + month_means[j]\nprint (\"Prediction of extent for January 2025 (in millions of square km):\", y_hat)\n\nPrediction of extent for January 2025 (in millions of square km): [[13.14449923]]\n\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n\n\nPrediction of extent for January 2025 (in millions of square km): [ 13.14449923]\n\n\n\n\nIgual, Laura, and Santi Seguí. 2017. “Regression Analysis.” In Introduction to Data Science: A Python Approach to Concepts, Techniques and Applications, 97–114. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-50017-1_6."
  },
  {
    "objectID": "content/labs/Lab_3/IM939_Lab_3_Exercise_Linear_Regression.html#scikit-learn",
    "href": "content/labs/Lab_3/IM939_Lab_3_Exercise_Linear_Regression.html#scikit-learn",
    "title": "11  Exercise: Regression",
    "section": "11.1 Scikit Learn",
    "text": "11.1 Scikit Learn\nYou can use here as well Scikit Learn library. More information you can find here: https://www.tutorialspoint.com/scikit_learn/scikit_learn_linear_regression.htm"
  },
  {
    "objectID": "content/labs/Lab_3/IM939_Lab_3_Exercise_Linear_Regression.html#wine-dataset",
    "href": "content/labs/Lab_3/IM939_Lab_3_Exercise_Linear_Regression.html#wine-dataset",
    "title": "11  Exercise: Regression",
    "section": "11.2 Wine Dataset",
    "text": "11.2 Wine Dataset\nMore information about the dataset here: https://archive.ics.uci.edu/ml/datasets/wine+quality\nWhat would be your research question? What do you like to learn, given the data you have?"
  },
  {
    "objectID": "content/labs/Lab_3/IM939_Lab_3_Exercise_Linear_Regression.html#reading-data",
    "href": "content/labs/Lab_3/IM939_Lab_3_Exercise_Linear_Regression.html#reading-data",
    "title": "11  Exercise: Regression",
    "section": "11.3 Reading Data",
    "text": "11.3 Reading Data\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n\nwine = pd.read_excel('data/raw/winequality-red_v2.xlsx', engine = 'openpyxl')\n\n#You might need to use encoding, then the code will look like:\n# wine = pd.read_excel('data/raw/winequality-red_v2.xlsx', engine = 'openpyxl', encoding='UTF-8')"
  },
  {
    "objectID": "content/labs/Lab_3/IM939_Lab_3_Exercise_Linear_Regression.html#data-exploration",
    "href": "content/labs/Lab_3/IM939_Lab_3_Exercise_Linear_Regression.html#data-exploration",
    "title": "11  Exercise: Regression",
    "section": "11.4 Data exploration",
    "text": "11.4 Data exploration\nLet’s check the data, their distribution and central tendencies\n\nprint('shape:', wine.shape)\nwine.head()\n\nshape: (1599, 12)\n\n\n\n\n\n\n\n\n\nfixed_acidity\nvolatile_acidity\ncitric_acid\nresidual_sugar\nchlorides\nfree_sulfur_dioxide\ntotal_sulfur_dioxide\ndensity\npH\nsulphates\nalcohol\nquality\n\n\n\n\n0\n7.4\n0.70\n0.00\n1.9\n0.076\n11.0\n34.0\n0.9978\n3.51\n0.56\n9.4\n5\n\n\n1\n7.8\n0.88\n0.00\n2.6\n0.098\n25.0\n67.0\n0.9968\n3.20\n0.68\n9.8\n5\n\n\n2\n7.8\n0.76\n0.04\n2.3\n0.092\n15.0\n54.0\n0.9970\n3.26\n0.65\n9.8\n5\n\n\n3\n11.2\n0.28\n0.56\n1.9\n0.075\n17.0\n60.0\n0.9980\n3.16\n0.58\n9.8\n6\n\n\n4\n7.4\n0.70\n0.00\n1.9\n0.076\n11.0\n34.0\n0.9978\n3.51\n0.56\n9.4\n5\n\n\n\n\n\n\n\n\n11.4.1 Check your variables\nUse lmplot() function from Seaborn to explore linear relationship Input data must be in a Pandas DataFrame. To plot them, we provide the predictor and response variable names along with the dataset\nDid you find outliers or missing data? You can use function np.unique and find the unique elements of an array.\n\n?np.unique\n\nDo you need to remove any cases?\n\n \n\nDid you need to standarize data?\nIf you standarized data, try to plot them again\n\n \n\nYou can calculates a Pearson correlation coefficient and the p-value for testing non-correlation.\n\nimport scipy.stats\nscipy.stats.pearsonr(wine.???.values, wine.???.values)\n\nSyntaxError: invalid syntax (987973612.py, line 2)\n\n\nusing Scikit-learn, build a simple linear regression (OLS)\n\nfrom sklearn.linear_model import LinearRegression\n\nest = LinearRegression(fit_intercept = True)\n\nx = wine[['???']]\ny = wine[['???']]\n\nest.fit(x, y)\n\nprint(\"Coefficients:\", est.coef_)\nprint (\"Intercept:\", est.intercept_)\n\nKeyError: \"None of [Index(['???'], dtype='object')] are in the [columns]\"\n\n\nWhat is the model’s mean squared error (\\(MSE\\)) and the coefficient of determination (\\(R^2\\)) ?\n\nfrom sklearn import metrics\n\n# Analysis for all months together.\nx = wdi[['???']]\ny = wdi[['???']]\nmodel = LinearRegression()\nmodel.fit(x, y)\ny_hat = model.predict(x)\nplt.plot(x, y,'o', alpha = 0.5)\nplt.plot(x, y_hat, 'r', alpha = 0.5)\nplt.xlabel('?')\nplt.ylabel('?')\nprint (\"MSE:\", metrics.mean_squared_error(y_hat, y))\nprint (\"R^2:\", metrics.r2_score(y_hat, y))\nprint (\"var:\", y.var())\nplt.savefig(\"?.png\", dpi = 300, bbox_inches = 'tight')\n\nNameError: name 'wdi' is not defined\n\n\nWhat’s the conclusion?"
  },
  {
    "objectID": "content/labs/Lab_4/IM939_Lab_4_1_Iris.html#data",
    "href": "content/labs/Lab_4/IM939_Lab_4_1_Iris.html#data",
    "title": "12  Lab: Dimension Reduction",
    "section": "12.1 Data",
    "text": "12.1 Data\nThe simple Iris dataset (Fisher 1936) is great for introducing these methods.\n\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\n\nThe iris dataset is in an odd format.\n\ntype(iris)\n\nsklearn.utils._bunch.Bunch\n\n\nFollowing this stackoverflow answer we can convert it into the pandas dataframe format we know and love.\n\nimport pandas as pd\n\niris_df = pd.DataFrame(iris.data, columns = iris.feature_names)\niris_df.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n\n\n\n\n\nWe will scale the data between 0 and 1 to be on the safe side. All we are doing is placing the data on the same scale which is often called Normalisation (see this blog entry).\nStandardisation often means centering the data around the mean.\nSome algorithms are senstive to the size of variables. For example, if the sepal widths were in meters and the other variables in cm then an algorithm may underweight sepal widths. Normalising the data puts all the data on a single scale.\nIf you cannot choose between them then try it both ways. You could compare the result with your raw data, the normalised data and the standardised data.\n\nfrom sklearn.preprocessing import MinMaxScaler\ncol_names = iris_df.columns\niris_df =  pd.DataFrame(MinMaxScaler().fit_transform(iris_df))\niris_df.columns = col_names # Column names were lost, so we need to re-introduce\niris_df\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n0.222222\n0.625000\n0.067797\n0.041667\n\n\n1\n0.166667\n0.416667\n0.067797\n0.041667\n\n\n2\n0.111111\n0.500000\n0.050847\n0.041667\n\n\n3\n0.083333\n0.458333\n0.084746\n0.041667\n\n\n4\n0.194444\n0.666667\n0.067797\n0.041667\n\n\n...\n...\n...\n...\n...\n\n\n145\n0.666667\n0.416667\n0.711864\n0.916667\n\n\n146\n0.555556\n0.208333\n0.677966\n0.750000\n\n\n147\n0.611111\n0.416667\n0.711864\n0.791667\n\n\n148\n0.527778\n0.583333\n0.745763\n0.916667\n\n\n149\n0.444444\n0.416667\n0.694915\n0.708333\n\n\n\n\n150 rows × 4 columns\n\n\n\n\niris_df.shape\n\n(150, 4)\n\n\nGreat.\nOur dataset show us the length and width of both the sepal (leaf) and petals of 150 plants. The dataset is quite famous and you can find a wikipedia page with details of the dataset."
  },
  {
    "objectID": "content/labs/Lab_4/IM939_Lab_4_1_Iris.html#questions",
    "href": "content/labs/Lab_4/IM939_Lab_4_1_Iris.html#questions",
    "title": "12  Lab: Dimension Reduction",
    "section": "12.2 Questions",
    "text": "12.2 Questions\nTo motivate our exploration of the data, consider the sorts of questions we can ask:\n\nAre all our plants from the same species?\nDo some plants have similiar leaf and petal sizes?\nCan we differentiate between the plants using all 4 variables (dimensions)?\nDo we need to include both length and width, or can we reduce these dimensions and simplify our analysis?"
  },
  {
    "objectID": "content/labs/Lab_4/IM939_Lab_4_1_Iris.html#initial-exploration",
    "href": "content/labs/Lab_4/IM939_Lab_4_1_Iris.html#initial-exploration",
    "title": "12  Lab: Dimension Reduction",
    "section": "12.3 Initial exploration",
    "text": "12.3 Initial exploration\nWe can explore a dataset with few variables using plots.\n\nimport seaborn as sns\n\n# some plots require a long dataframe structure\niris_df_long = iris_df.melt()\niris_df_long\n\n\n\n\n\n\n\n\nvariable\nvalue\n\n\n\n\n0\nsepal length (cm)\n0.222222\n\n\n1\nsepal length (cm)\n0.166667\n\n\n2\nsepal length (cm)\n0.111111\n\n\n3\nsepal length (cm)\n0.083333\n\n\n4\nsepal length (cm)\n0.194444\n\n\n...\n...\n...\n\n\n595\npetal width (cm)\n0.916667\n\n\n596\npetal width (cm)\n0.750000\n\n\n597\npetal width (cm)\n0.791667\n\n\n598\npetal width (cm)\n0.916667\n\n\n599\npetal width (cm)\n0.708333\n\n\n\n\n600 rows × 2 columns\n\n\n\n\nsns.violinplot(data = iris_df_long, x = 'variable', y = 'value')\n\n&lt;Axes: xlabel='variable', ylabel='value'&gt;\n\n\n\n\n\nThe below plots use the wide data structure.\n\niris_df\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n0.222222\n0.625000\n0.067797\n0.041667\n\n\n1\n0.166667\n0.416667\n0.067797\n0.041667\n\n\n2\n0.111111\n0.500000\n0.050847\n0.041667\n\n\n3\n0.083333\n0.458333\n0.084746\n0.041667\n\n\n4\n0.194444\n0.666667\n0.067797\n0.041667\n\n\n...\n...\n...\n...\n...\n\n\n145\n0.666667\n0.416667\n0.711864\n0.916667\n\n\n146\n0.555556\n0.208333\n0.677966\n0.750000\n\n\n147\n0.611111\n0.416667\n0.711864\n0.791667\n\n\n148\n0.527778\n0.583333\n0.745763\n0.916667\n\n\n149\n0.444444\n0.416667\n0.694915\n0.708333\n\n\n\n\n150 rows × 4 columns\n\n\n\n\nsns.scatterplot(data = iris_df, x = 'sepal length (cm)', y = 'sepal width (cm)')\n\n&lt;Axes: xlabel='sepal length (cm)', ylabel='sepal width (cm)'&gt;\n\n\n\n\n\n\nsns.scatterplot(data = iris_df, x = 'sepal length (cm)', y = 'petal length (cm)')\n\n&lt;Axes: xlabel='sepal length (cm)', ylabel='petal length (cm)'&gt;\n\n\n\n\n\nInteresting. There seem to be two groupings in the data.\nIt might be easier to look at all the variables at once.\n\nsns.pairplot(iris_df)\n\n\n\n\nThere seem to be some groupings in the data. Though we cannot easily identify which point corresponds to which row."
  },
  {
    "objectID": "content/labs/Lab_4/IM939_Lab_4_1_Iris.html#clustering",
    "href": "content/labs/Lab_4/IM939_Lab_4_1_Iris.html#clustering",
    "title": "12  Lab: Dimension Reduction",
    "section": "12.4 Clustering",
    "text": "12.4 Clustering\nA cluster is simply a group based on simliarity. There are several methods and we will use a relatively simple one called K-means clustering.\nIn K-means clustering an algorithm tries to group our items (plants in the iris dataset) based on similarity. We decide how many groups we want and the algorithm does the best it can (an accessible introduction to k-means clustering is here).\nTo start, we import the KMeans function from sklearn cluster module and turn our data into a matrix.\n\nfrom sklearn.cluster import KMeans\n\niris = iris_df.values\niris\n\narray([[0.22222222, 0.625     , 0.06779661, 0.04166667],\n       [0.16666667, 0.41666667, 0.06779661, 0.04166667],\n       [0.11111111, 0.5       , 0.05084746, 0.04166667],\n       [0.08333333, 0.45833333, 0.08474576, 0.04166667],\n       [0.19444444, 0.66666667, 0.06779661, 0.04166667],\n       [0.30555556, 0.79166667, 0.11864407, 0.125     ],\n       [0.08333333, 0.58333333, 0.06779661, 0.08333333],\n       [0.19444444, 0.58333333, 0.08474576, 0.04166667],\n       [0.02777778, 0.375     , 0.06779661, 0.04166667],\n       [0.16666667, 0.45833333, 0.08474576, 0.        ],\n       [0.30555556, 0.70833333, 0.08474576, 0.04166667],\n       [0.13888889, 0.58333333, 0.10169492, 0.04166667],\n       [0.13888889, 0.41666667, 0.06779661, 0.        ],\n       [0.        , 0.41666667, 0.01694915, 0.        ],\n       [0.41666667, 0.83333333, 0.03389831, 0.04166667],\n       [0.38888889, 1.        , 0.08474576, 0.125     ],\n       [0.30555556, 0.79166667, 0.05084746, 0.125     ],\n       [0.22222222, 0.625     , 0.06779661, 0.08333333],\n       [0.38888889, 0.75      , 0.11864407, 0.08333333],\n       [0.22222222, 0.75      , 0.08474576, 0.08333333],\n       [0.30555556, 0.58333333, 0.11864407, 0.04166667],\n       [0.22222222, 0.70833333, 0.08474576, 0.125     ],\n       [0.08333333, 0.66666667, 0.        , 0.04166667],\n       [0.22222222, 0.54166667, 0.11864407, 0.16666667],\n       [0.13888889, 0.58333333, 0.15254237, 0.04166667],\n       [0.19444444, 0.41666667, 0.10169492, 0.04166667],\n       [0.19444444, 0.58333333, 0.10169492, 0.125     ],\n       [0.25      , 0.625     , 0.08474576, 0.04166667],\n       [0.25      , 0.58333333, 0.06779661, 0.04166667],\n       [0.11111111, 0.5       , 0.10169492, 0.04166667],\n       [0.13888889, 0.45833333, 0.10169492, 0.04166667],\n       [0.30555556, 0.58333333, 0.08474576, 0.125     ],\n       [0.25      , 0.875     , 0.08474576, 0.        ],\n       [0.33333333, 0.91666667, 0.06779661, 0.04166667],\n       [0.16666667, 0.45833333, 0.08474576, 0.04166667],\n       [0.19444444, 0.5       , 0.03389831, 0.04166667],\n       [0.33333333, 0.625     , 0.05084746, 0.04166667],\n       [0.16666667, 0.66666667, 0.06779661, 0.        ],\n       [0.02777778, 0.41666667, 0.05084746, 0.04166667],\n       [0.22222222, 0.58333333, 0.08474576, 0.04166667],\n       [0.19444444, 0.625     , 0.05084746, 0.08333333],\n       [0.05555556, 0.125     , 0.05084746, 0.08333333],\n       [0.02777778, 0.5       , 0.05084746, 0.04166667],\n       [0.19444444, 0.625     , 0.10169492, 0.20833333],\n       [0.22222222, 0.75      , 0.15254237, 0.125     ],\n       [0.13888889, 0.41666667, 0.06779661, 0.08333333],\n       [0.22222222, 0.75      , 0.10169492, 0.04166667],\n       [0.08333333, 0.5       , 0.06779661, 0.04166667],\n       [0.27777778, 0.70833333, 0.08474576, 0.04166667],\n       [0.19444444, 0.54166667, 0.06779661, 0.04166667],\n       [0.75      , 0.5       , 0.62711864, 0.54166667],\n       [0.58333333, 0.5       , 0.59322034, 0.58333333],\n       [0.72222222, 0.45833333, 0.66101695, 0.58333333],\n       [0.33333333, 0.125     , 0.50847458, 0.5       ],\n       [0.61111111, 0.33333333, 0.61016949, 0.58333333],\n       [0.38888889, 0.33333333, 0.59322034, 0.5       ],\n       [0.55555556, 0.54166667, 0.62711864, 0.625     ],\n       [0.16666667, 0.16666667, 0.38983051, 0.375     ],\n       [0.63888889, 0.375     , 0.61016949, 0.5       ],\n       [0.25      , 0.29166667, 0.49152542, 0.54166667],\n       [0.19444444, 0.        , 0.42372881, 0.375     ],\n       [0.44444444, 0.41666667, 0.54237288, 0.58333333],\n       [0.47222222, 0.08333333, 0.50847458, 0.375     ],\n       [0.5       , 0.375     , 0.62711864, 0.54166667],\n       [0.36111111, 0.375     , 0.44067797, 0.5       ],\n       [0.66666667, 0.45833333, 0.57627119, 0.54166667],\n       [0.36111111, 0.41666667, 0.59322034, 0.58333333],\n       [0.41666667, 0.29166667, 0.52542373, 0.375     ],\n       [0.52777778, 0.08333333, 0.59322034, 0.58333333],\n       [0.36111111, 0.20833333, 0.49152542, 0.41666667],\n       [0.44444444, 0.5       , 0.6440678 , 0.70833333],\n       [0.5       , 0.33333333, 0.50847458, 0.5       ],\n       [0.55555556, 0.20833333, 0.66101695, 0.58333333],\n       [0.5       , 0.33333333, 0.62711864, 0.45833333],\n       [0.58333333, 0.375     , 0.55932203, 0.5       ],\n       [0.63888889, 0.41666667, 0.57627119, 0.54166667],\n       [0.69444444, 0.33333333, 0.6440678 , 0.54166667],\n       [0.66666667, 0.41666667, 0.6779661 , 0.66666667],\n       [0.47222222, 0.375     , 0.59322034, 0.58333333],\n       [0.38888889, 0.25      , 0.42372881, 0.375     ],\n       [0.33333333, 0.16666667, 0.47457627, 0.41666667],\n       [0.33333333, 0.16666667, 0.45762712, 0.375     ],\n       [0.41666667, 0.29166667, 0.49152542, 0.45833333],\n       [0.47222222, 0.29166667, 0.69491525, 0.625     ],\n       [0.30555556, 0.41666667, 0.59322034, 0.58333333],\n       [0.47222222, 0.58333333, 0.59322034, 0.625     ],\n       [0.66666667, 0.45833333, 0.62711864, 0.58333333],\n       [0.55555556, 0.125     , 0.57627119, 0.5       ],\n       [0.36111111, 0.41666667, 0.52542373, 0.5       ],\n       [0.33333333, 0.20833333, 0.50847458, 0.5       ],\n       [0.33333333, 0.25      , 0.57627119, 0.45833333],\n       [0.5       , 0.41666667, 0.61016949, 0.54166667],\n       [0.41666667, 0.25      , 0.50847458, 0.45833333],\n       [0.19444444, 0.125     , 0.38983051, 0.375     ],\n       [0.36111111, 0.29166667, 0.54237288, 0.5       ],\n       [0.38888889, 0.41666667, 0.54237288, 0.45833333],\n       [0.38888889, 0.375     , 0.54237288, 0.5       ],\n       [0.52777778, 0.375     , 0.55932203, 0.5       ],\n       [0.22222222, 0.20833333, 0.33898305, 0.41666667],\n       [0.38888889, 0.33333333, 0.52542373, 0.5       ],\n       [0.55555556, 0.54166667, 0.84745763, 1.        ],\n       [0.41666667, 0.29166667, 0.69491525, 0.75      ],\n       [0.77777778, 0.41666667, 0.83050847, 0.83333333],\n       [0.55555556, 0.375     , 0.77966102, 0.70833333],\n       [0.61111111, 0.41666667, 0.81355932, 0.875     ],\n       [0.91666667, 0.41666667, 0.94915254, 0.83333333],\n       [0.16666667, 0.20833333, 0.59322034, 0.66666667],\n       [0.83333333, 0.375     , 0.89830508, 0.70833333],\n       [0.66666667, 0.20833333, 0.81355932, 0.70833333],\n       [0.80555556, 0.66666667, 0.86440678, 1.        ],\n       [0.61111111, 0.5       , 0.69491525, 0.79166667],\n       [0.58333333, 0.29166667, 0.72881356, 0.75      ],\n       [0.69444444, 0.41666667, 0.76271186, 0.83333333],\n       [0.38888889, 0.20833333, 0.6779661 , 0.79166667],\n       [0.41666667, 0.33333333, 0.69491525, 0.95833333],\n       [0.58333333, 0.5       , 0.72881356, 0.91666667],\n       [0.61111111, 0.41666667, 0.76271186, 0.70833333],\n       [0.94444444, 0.75      , 0.96610169, 0.875     ],\n       [0.94444444, 0.25      , 1.        , 0.91666667],\n       [0.47222222, 0.08333333, 0.6779661 , 0.58333333],\n       [0.72222222, 0.5       , 0.79661017, 0.91666667],\n       [0.36111111, 0.33333333, 0.66101695, 0.79166667],\n       [0.94444444, 0.33333333, 0.96610169, 0.79166667],\n       [0.55555556, 0.29166667, 0.66101695, 0.70833333],\n       [0.66666667, 0.54166667, 0.79661017, 0.83333333],\n       [0.80555556, 0.5       , 0.84745763, 0.70833333],\n       [0.52777778, 0.33333333, 0.6440678 , 0.70833333],\n       [0.5       , 0.41666667, 0.66101695, 0.70833333],\n       [0.58333333, 0.33333333, 0.77966102, 0.83333333],\n       [0.80555556, 0.41666667, 0.81355932, 0.625     ],\n       [0.86111111, 0.33333333, 0.86440678, 0.75      ],\n       [1.        , 0.75      , 0.91525424, 0.79166667],\n       [0.58333333, 0.33333333, 0.77966102, 0.875     ],\n       [0.55555556, 0.33333333, 0.69491525, 0.58333333],\n       [0.5       , 0.25      , 0.77966102, 0.54166667],\n       [0.94444444, 0.41666667, 0.86440678, 0.91666667],\n       [0.55555556, 0.58333333, 0.77966102, 0.95833333],\n       [0.58333333, 0.45833333, 0.76271186, 0.70833333],\n       [0.47222222, 0.41666667, 0.6440678 , 0.70833333],\n       [0.72222222, 0.45833333, 0.74576271, 0.83333333],\n       [0.66666667, 0.45833333, 0.77966102, 0.95833333],\n       [0.72222222, 0.45833333, 0.69491525, 0.91666667],\n       [0.41666667, 0.29166667, 0.69491525, 0.75      ],\n       [0.69444444, 0.5       , 0.83050847, 0.91666667],\n       [0.66666667, 0.54166667, 0.79661017, 1.        ],\n       [0.66666667, 0.41666667, 0.71186441, 0.91666667],\n       [0.55555556, 0.20833333, 0.6779661 , 0.75      ],\n       [0.61111111, 0.41666667, 0.71186441, 0.79166667],\n       [0.52777778, 0.58333333, 0.74576271, 0.91666667],\n       [0.44444444, 0.41666667, 0.69491525, 0.70833333]])\n\n\nSpecify our number of clusters.\n\nk_means = KMeans(n_clusters = 3, init = 'random',  n_init = 10)\n\nFit our kmeans model to the data\n\nk_means.fit(iris)\n\nKMeans(init='random', n_clusters=3, n_init=10)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KMeansKMeans(init='random', n_clusters=3, n_init=10)\n\n\nThe algorithm has assigned the a label to each row.\n\nk_means.labels_\n\narray([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2,\n       2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 0, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2,\n       2, 0, 0, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 0], dtype=int32)\n\n\nEach row has been assigned a label.\nTo tidy things up we should put everything into a dataframe.\n\niris_df['Three clusters'] = pd.Series(k_means.predict(iris_df.values), index = iris_df.index)\n\n\niris_df\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\nThree clusters\n\n\n\n\n0\n0.222222\n0.625000\n0.067797\n0.041667\n1\n\n\n1\n0.166667\n0.416667\n0.067797\n0.041667\n1\n\n\n2\n0.111111\n0.500000\n0.050847\n0.041667\n1\n\n\n3\n0.083333\n0.458333\n0.084746\n0.041667\n1\n\n\n4\n0.194444\n0.666667\n0.067797\n0.041667\n1\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n0.666667\n0.416667\n0.711864\n0.916667\n2\n\n\n146\n0.555556\n0.208333\n0.677966\n0.750000\n0\n\n\n147\n0.611111\n0.416667\n0.711864\n0.791667\n2\n\n\n148\n0.527778\n0.583333\n0.745763\n0.916667\n2\n\n\n149\n0.444444\n0.416667\n0.694915\n0.708333\n0\n\n\n\n\n150 rows × 5 columns\n\n\n\n\nsns.pairplot(iris_df, hue = 'Three clusters')\n\n\n\n\nThat seems quite nice. We can also do individual plots if preferred.\n\nsns.scatterplot(data = iris_df, x = 'sepal length (cm)', y = 'petal width (cm)', hue = 'Three clusters')\n\n&lt;Axes: xlabel='sepal length (cm)', ylabel='petal width (cm)'&gt;\n\n\n\n\n\nK-means works by clustering the data around central points (often called centroids, means or cluster centers). We can extract the cluster centres from the kmeans object.\n\nk_means.cluster_centers_\n\narray([[0.44125683, 0.30737705, 0.57571548, 0.54918033],\n       [0.19611111, 0.595     , 0.07830508, 0.06083333],\n       [0.70726496, 0.4508547 , 0.79704476, 0.82478632]])\n\n\nIt is tricky to plot these using seaborn but we can use a normal maplotlib scatter plot.\nLet us grab the groups.\n\ngroup1 = iris_df[iris_df['Three clusters'] == 0]\ngroup2 = iris_df[iris_df['Three clusters'] == 1]\ngroup3 = iris_df[iris_df['Three clusters'] == 2]\n\nGrab the centroids\n\nimport pandas as pd\n\ncentres = k_means.cluster_centers_\n\ndata = {'x': [centres[0][0], centres[1][0], centres[2][0]],\n        'y': [centres[0][3], centres[1][3], centres[2][3]]}\n\ndf = pd.DataFrame (data, columns = ['x', 'y'])\n\nCreate the plot\n\nimport matplotlib.pyplot as plt\n\n# Plot each group individually\nplt.scatter(\n    x = group1['sepal length (cm)'], \n    y = group1['petal width (cm)'], \n    alpha = 0.1, color = 'blue'\n)\n\nplt.scatter(\n    x = group2['sepal length (cm)'], \n    y = group2['petal width (cm)'], \n    alpha = 0.1, color = 'orange'\n)\n\nplt.scatter(\n    x = group3['sepal length (cm)'], \n    y = group3['petal width (cm)'], \n    alpha = 0.1, color = 'red'\n)\n\n# Plot cluster centres\nplt.scatter(\n    x = df['x'], \n    y = df['y'], \n    alpha = 1, color = 'black'\n)\n\n&lt;matplotlib.collections.PathCollection at 0x160770cd0&gt;\n\n\n\n\n\n\n12.4.1 Number of clusters\nWhat happens if we change the number of clusters?\nTwo groups\n\nk_means_2 = KMeans(n_clusters = 2, init = 'random', n_init = 10)\nk_means_2.fit(iris)\niris_df['Two clusters'] = pd.Series(k_means_2.predict(iris_df.iloc[:,0:4].values), index = iris_df.index)\n\nNote that I have added a new column to the iris dataframe called ‘cluster 2 means’ and pass only our origonal 4 columns to the predict function (hence me using .iloc[:,0:4]).\nHow do our groupings look now (without plotting the cluster column)?\n\nsns.pairplot(iris_df.loc[:, iris_df.columns != 'Three clusters'], hue = 'Two clusters')\n\n\n\n\nHmm, does the data have more than two groups in it?\nPerhaps we should try 5 clusters instead.\n\nk_means_5 = KMeans(n_clusters = 5, init = 'random', n_init = 10)\nk_means_5.fit(iris)\niris_df['Five clusters'] = pd.Series(k_means_5.predict(iris_df.iloc[:,0:4].values), index = iris_df.index)\n\nPlot without the columns called ‘cluster’ and ‘Two cluster’\n\nsns.pairplot(iris_df.loc[:, (iris_df.columns != 'Three clusters') & (iris_df.columns != 'Two clusters')], hue = 'Five clusters')\n\n\n\n\n\niris_df\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\nThree clusters\nTwo clusters\nFive clusters\n\n\n\n\n0\n0.222222\n0.625000\n0.067797\n0.041667\n1\n1\n0\n\n\n1\n0.166667\n0.416667\n0.067797\n0.041667\n1\n1\n3\n\n\n2\n0.111111\n0.500000\n0.050847\n0.041667\n1\n1\n3\n\n\n3\n0.083333\n0.458333\n0.084746\n0.041667\n1\n1\n3\n\n\n4\n0.194444\n0.666667\n0.067797\n0.041667\n1\n1\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n145\n0.666667\n0.416667\n0.711864\n0.916667\n2\n0\n2\n\n\n146\n0.555556\n0.208333\n0.677966\n0.750000\n0\n0\n4\n\n\n147\n0.611111\n0.416667\n0.711864\n0.791667\n2\n0\n4\n\n\n148\n0.527778\n0.583333\n0.745763\n0.916667\n2\n0\n2\n\n\n149\n0.444444\n0.416667\n0.694915\n0.708333\n0\n0\n4\n\n\n\n\n150 rows × 7 columns\n\n\n\nWhich did best?\n\nk_means.inertia_\n\n6.982216473785234\n\n\n\nk_means_2.inertia_\n\n12.127790750538193\n\n\n\nk_means_5.inertia_\n\n4.580948640117293\n\n\nIt looks like our k = 5 model captures the data well. Intertia, looking at the sklearn documentation as the Sum of squared distances of samples to their closest cluster center..\nIf you want to dive further into this then Real Python’s practical guide to K-Means Clustering is quite good."
  },
  {
    "objectID": "content/labs/Lab_4/IM939_Lab_4_1_Iris.html#principal-component-analysis-pca",
    "href": "content/labs/Lab_4/IM939_Lab_4_1_Iris.html#principal-component-analysis-pca",
    "title": "12  Lab: Dimension Reduction",
    "section": "12.5 Principal Component Analysis (PCA)",
    "text": "12.5 Principal Component Analysis (PCA)\nPCA reduces the dimension of our data. The method derives point in an n dimentional space from our data which are uncorrelated.\nTo carry out a PCA on our Iris dataset where there are only two dimensions.\n\nfrom sklearn.decomposition import PCA\n\nn_components = 2\n\npca = PCA(n_components=n_components)\niris_pca = pca.fit(iris_df.iloc[:,0:4])\n\nWe can look at the components.\n\niris_pca.components_\n\narray([[ 0.42494212, -0.15074824,  0.61626702,  0.64568888],\n       [ 0.42320271,  0.90396711, -0.06038308, -0.00983925]])\n\n\nThese components are intersting. You may want to look at a PennState article on interpreting PCA components.\nOur second column, ‘sepal width (cm)’ is positively correlated with our second principle component whereas the first column ‘sepal length (cm)’ is postively correlated with both.\nYou may want to consider:\n\nDo we need more than two components?\nIs it useful to keep sepal length (cm) in the dataset?\n\nWe can also examine the explained variance of the each principle component.\n\niris_pca.explained_variance_\n\narray([0.23245325, 0.0324682 ])\n\n\nA nice worked example showing the link between the explained variance and the component is here.\nOur first principle component explains a lot more of the variance of data then the second.\n\n12.5.1 Dimension reduction\nFor our purposes, we are interested in using PCA for reducing the number of dimension in our data whilst preseving the maximal data variance.\nWe can extract the projected components from the model.\n\niris_pca_vals = pca.fit_transform(iris_df.iloc[:,0:4])\n\nThe numpy arrays contains the projected values.\n\ntype(iris_pca_vals)\n\nnumpy.ndarray\n\n\n\niris_pca_vals\n\narray([[-6.30702931e-01,  1.07577910e-01],\n       [-6.22904943e-01, -1.04259833e-01],\n       [-6.69520395e-01, -5.14170597e-02],\n       [-6.54152759e-01, -1.02884871e-01],\n       [-6.48788056e-01,  1.33487576e-01],\n       [-5.35272778e-01,  2.89615724e-01],\n       [-6.56537790e-01,  1.07244911e-02],\n       [-6.25780499e-01,  5.71335411e-02],\n       [-6.75643504e-01, -2.00703283e-01],\n       [-6.45644619e-01, -6.72080097e-02],\n       [-5.97408238e-01,  2.17151953e-01],\n       [-6.38943190e-01,  3.25988375e-02],\n       [-6.61612593e-01, -1.15605495e-01],\n       [-7.51967943e-01, -1.71313322e-01],\n       [-6.00371589e-01,  3.80240692e-01],\n       [-5.52157227e-01,  5.15255982e-01],\n       [-5.77053593e-01,  2.93709492e-01],\n       [-6.03799228e-01,  1.07167941e-01],\n       [-5.20483461e-01,  2.87627289e-01],\n       [-6.12197555e-01,  2.19140388e-01],\n       [-5.57674300e-01,  1.02109180e-01],\n       [-5.79012675e-01,  1.81065123e-01],\n       [-7.37784662e-01,  9.05588211e-02],\n       [-5.06093857e-01,  2.79470846e-02],\n       [-6.07607579e-01,  2.95285112e-02],\n       [-5.90210587e-01, -9.45510863e-02],\n       [-5.61527888e-01,  5.52901611e-02],\n       [-6.08453780e-01,  1.18310099e-01],\n       [-6.12617807e-01,  8.16682448e-02],\n       [-6.38184784e-01, -5.44873860e-02],\n       [-6.20099660e-01, -8.03970516e-02],\n       [-5.24757301e-01,  1.03336126e-01],\n       [-6.73044544e-01,  3.44711846e-01],\n       [-6.27455379e-01,  4.18257508e-01],\n       [-6.18740916e-01, -6.76179787e-02],\n       [-6.44553756e-01, -1.51267253e-02],\n       [-5.93932344e-01,  1.55623876e-01],\n       [-6.87495707e-01,  1.22141914e-01],\n       [-6.92369885e-01, -1.62014545e-01],\n       [-6.13976551e-01,  6.88891719e-02],\n       [-6.26048380e-01,  9.64357527e-02],\n       [-6.09693996e-01, -4.14325957e-01],\n       [-7.04932239e-01, -8.66839521e-02],\n       [-5.14001659e-01,  9.21355196e-02],\n       [-5.43513037e-01,  2.14636651e-01],\n       [-6.07805187e-01, -1.16425433e-01],\n       [-6.28656055e-01,  2.18526915e-01],\n       [-6.70879139e-01, -6.41961326e-02],\n       [-6.09212186e-01,  2.05396323e-01],\n       [-6.29944525e-01,  2.04916869e-02],\n       [ 2.79951766e-01,  1.79245790e-01],\n       [ 2.15141376e-01,  1.10348921e-01],\n       [ 3.22223106e-01,  1.27368010e-01],\n       [ 5.94030131e-02, -3.28502275e-01],\n       [ 2.62515235e-01, -2.95800761e-02],\n       [ 1.03831043e-01, -1.21781742e-01],\n       [ 2.44850362e-01,  1.33801733e-01],\n       [-1.71529386e-01, -3.52976762e-01],\n       [ 2.14230599e-01,  2.06607890e-02],\n       [ 1.53249619e-02, -2.12494509e-01],\n       [-1.13710323e-01, -4.93929201e-01],\n       [ 1.37348380e-01, -2.06894998e-02],\n       [ 4.39928190e-02, -3.06159511e-01],\n       [ 1.92559767e-01, -3.95507760e-02],\n       [-8.26091518e-03, -8.66610981e-02],\n       [ 2.19485489e-01,  1.09383928e-01],\n       [ 1.33272148e-01, -5.90267184e-02],\n       [-5.75757060e-04, -1.42367733e-01],\n       [ 2.54345249e-01, -2.89815304e-01],\n       [-5.60800300e-03, -2.39572672e-01],\n       [ 2.68168358e-01,  4.72705335e-02],\n       [ 9.88208151e-02, -6.96420088e-02],\n       [ 2.89086481e-01, -1.69157553e-01],\n       [ 1.45033538e-01, -7.63961345e-02],\n       [ 1.59287093e-01,  2.19853643e-04],\n       [ 2.13962718e-01,  5.99630005e-02],\n       [ 2.91913782e-01,  4.04990109e-03],\n       [ 3.69148997e-01,  6.43480720e-02],\n       [ 1.86769115e-01, -4.96694916e-02],\n       [-6.87697501e-02, -1.85648007e-01],\n       [-2.15759776e-02, -2.87970157e-01],\n       [-5.89248844e-02, -2.86536746e-01],\n       [ 3.23412419e-02, -1.41140786e-01],\n       [ 2.88906394e-01, -1.31550706e-01],\n       [ 1.09664252e-01, -8.25379800e-02],\n       [ 1.82266934e-01,  1.38247021e-01],\n       [ 2.77724803e-01,  1.05903632e-01],\n       [ 1.95615410e-01, -2.38550997e-01],\n       [ 3.76839264e-02, -5.41130122e-02],\n       [ 4.68406593e-02, -2.53171683e-01],\n       [ 5.54365941e-02, -2.19190186e-01],\n       [ 1.75833387e-01, -8.62037590e-04],\n       [ 4.90676225e-02, -1.79829525e-01],\n       [-1.53444261e-01, -3.78886428e-01],\n       [ 6.69726607e-02, -1.68132343e-01],\n       [ 3.30293747e-02, -4.29708545e-02],\n       [ 6.62142547e-02, -8.10461198e-02],\n       [ 1.35679197e-01, -2.32914079e-02],\n       [-1.58634575e-01, -2.89139847e-01],\n       [ 6.20502279e-02, -1.17687974e-01],\n       [ 6.22771338e-01,  1.16807265e-01],\n       [ 3.46009609e-01, -1.56291874e-01],\n       [ 6.17986434e-01,  1.00519741e-01],\n       [ 4.17789309e-01, -2.68903690e-02],\n       [ 5.63621248e-01,  3.05994289e-02],\n       [ 7.50122599e-01,  1.52133800e-01],\n       [ 1.35857804e-01, -3.30462554e-01],\n       [ 6.08945212e-01,  8.35018443e-02],\n       [ 5.11020215e-01, -1.32575915e-01],\n       [ 7.20608541e-01,  3.34580389e-01],\n       [ 4.24135062e-01,  1.13914054e-01],\n       [ 4.37723702e-01, -8.78049736e-02],\n       [ 5.40793776e-01,  6.93466165e-02],\n       [ 3.63226514e-01, -2.42764625e-01],\n       [ 4.74246948e-01, -1.20676423e-01],\n       [ 5.13932631e-01,  9.88816323e-02],\n       [ 4.24670824e-01,  3.53096310e-02],\n       [ 7.49026039e-01,  4.63778390e-01],\n       [ 8.72194272e-01,  9.33798117e-03],\n       [ 2.82963372e-01, -3.18443776e-01],\n       [ 6.14733184e-01,  1.53566018e-01],\n       [ 3.22133832e-01, -1.40500924e-01],\n       [ 7.58030401e-01,  8.79453649e-02],\n       [ 3.57235237e-01, -9.50568671e-02],\n       [ 5.31036706e-01,  1.68539991e-01],\n       [ 5.46962123e-01,  1.87812429e-01],\n       [ 3.28704908e-01, -6.81237595e-02],\n       [ 3.14783811e-01, -5.57223965e-03],\n       [ 5.16585543e-01, -5.40299414e-02],\n       [ 4.84826663e-01,  1.15348658e-01],\n       [ 6.33043632e-01,  5.92290940e-02],\n       [ 6.87490917e-01,  4.91179916e-01],\n       [ 5.43489246e-01, -5.44399104e-02],\n       [ 2.91133358e-01, -5.82085481e-02],\n       [ 3.05410131e-01, -1.61757644e-01],\n       [ 7.63507935e-01,  1.68186703e-01],\n       [ 5.47805644e-01,  1.58976299e-01],\n       [ 4.06585699e-01,  6.12192966e-02],\n       [ 2.92534659e-01, -1.63044284e-02],\n       [ 5.35871344e-01,  1.19790986e-01],\n       [ 6.13864965e-01,  9.30029331e-02],\n       [ 5.58343139e-01,  1.22041374e-01],\n       [ 3.46009609e-01, -1.56291874e-01],\n       [ 6.23819644e-01,  1.39763503e-01],\n       [ 6.38651518e-01,  1.66900115e-01],\n       [ 5.51461624e-01,  5.98413741e-02],\n       [ 4.07146497e-01, -1.71820871e-01],\n       [ 4.47142619e-01,  3.75600193e-02],\n       [ 4.88207585e-01,  1.49677521e-01],\n       [ 3.12066323e-01, -3.11303854e-02]])\n\n\nEach row corresponds to a row in our data.\n\niris_pca_vals.shape\n\n(150, 2)\n\n\n\niris_df.shape\n\n(150, 7)\n\n\nWe can add the component to our dataset. I prefer to keep everything in one table and it is not at all required. You can just assign the values whichever variables you prefer.\n\niris_df['c1'] = [item[0] for item in iris_pca_vals]\niris_df['c2'] = [item[1] for item in iris_pca_vals]\n\n\niris_df\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\nThree clusters\nTwo clusters\nFive clusters\nc1\nc2\n\n\n\n\n0\n0.222222\n0.625000\n0.067797\n0.041667\n1\n1\n0\n-0.630703\n0.107578\n\n\n1\n0.166667\n0.416667\n0.067797\n0.041667\n1\n1\n3\n-0.622905\n-0.104260\n\n\n2\n0.111111\n0.500000\n0.050847\n0.041667\n1\n1\n3\n-0.669520\n-0.051417\n\n\n3\n0.083333\n0.458333\n0.084746\n0.041667\n1\n1\n3\n-0.654153\n-0.102885\n\n\n4\n0.194444\n0.666667\n0.067797\n0.041667\n1\n1\n0\n-0.648788\n0.133488\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n145\n0.666667\n0.416667\n0.711864\n0.916667\n2\n0\n2\n0.551462\n0.059841\n\n\n146\n0.555556\n0.208333\n0.677966\n0.750000\n0\n0\n4\n0.407146\n-0.171821\n\n\n147\n0.611111\n0.416667\n0.711864\n0.791667\n2\n0\n4\n0.447143\n0.037560\n\n\n148\n0.527778\n0.583333\n0.745763\n0.916667\n2\n0\n2\n0.488208\n0.149678\n\n\n149\n0.444444\n0.416667\n0.694915\n0.708333\n0\n0\n4\n0.312066\n-0.031130\n\n\n\n\n150 rows × 9 columns\n\n\n\nPlotting out our data on our new two component space.\n\nsns.scatterplot(data = iris_df, x = 'c1', y = 'c2')\n\n&lt;Axes: xlabel='c1', ylabel='c2'&gt;\n\n\n\n\n\nWe have reduced our three dimensions to two.\nWe can also colour by our clusters. What does this show us and is it useful?\n\nsns.scatterplot(data = iris_df, x = 'c1', y = 'c2', hue = 'Three clusters')\n\n&lt;Axes: xlabel='c1', ylabel='c2'&gt;\n\n\n\n\n\n\niris_df\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\nThree clusters\nTwo clusters\nFive clusters\nc1\nc2\n\n\n\n\n0\n0.222222\n0.625000\n0.067797\n0.041667\n1\n1\n0\n-0.630703\n0.107578\n\n\n1\n0.166667\n0.416667\n0.067797\n0.041667\n1\n1\n3\n-0.622905\n-0.104260\n\n\n2\n0.111111\n0.500000\n0.050847\n0.041667\n1\n1\n3\n-0.669520\n-0.051417\n\n\n3\n0.083333\n0.458333\n0.084746\n0.041667\n1\n1\n3\n-0.654153\n-0.102885\n\n\n4\n0.194444\n0.666667\n0.067797\n0.041667\n1\n1\n0\n-0.648788\n0.133488\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n145\n0.666667\n0.416667\n0.711864\n0.916667\n2\n0\n2\n0.551462\n0.059841\n\n\n146\n0.555556\n0.208333\n0.677966\n0.750000\n0\n0\n4\n0.407146\n-0.171821\n\n\n147\n0.611111\n0.416667\n0.711864\n0.791667\n2\n0\n4\n0.447143\n0.037560\n\n\n148\n0.527778\n0.583333\n0.745763\n0.916667\n2\n0\n2\n0.488208\n0.149678\n\n\n149\n0.444444\n0.416667\n0.694915\n0.708333\n0\n0\n4\n0.312066\n-0.031130\n\n\n\n\n150 rows × 9 columns\n\n\n\n\n\n12.5.2 PCA to Clusters\nWe have reduced our 4D dataset to 2D whilst keeping the data variance. Reducing the data to fewer dimensions can help with the ‘curse of dimensionality’, reduce the change of overfitting a machine learning model (see here) and reduce the computational complexity of a model fit.\nPutting our new dimensions into a kMeans model\n\nk_means_pca = KMeans(n_clusters = 3, init = 'random', n_init = 10)\niris_pca_kmeans = k_means_pca.fit(iris_df.iloc[:,-2:])\n\n\ntype(iris_df.iloc[:,-2:].values)\n\nnumpy.ndarray\n\n\n\niris_df['PCA 3 clusters'] = pd.Series(k_means_pca.predict(iris_df.iloc[:,-2:].values), index = iris_df.index)\niris_df\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but KMeans was fitted with feature names\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\nThree clusters\nTwo clusters\nFive clusters\nc1\nc2\nPCA 3 clusters\n\n\n\n\n0\n0.222222\n0.625000\n0.067797\n0.041667\n1\n1\n0\n-0.630703\n0.107578\n0\n\n\n1\n0.166667\n0.416667\n0.067797\n0.041667\n1\n1\n3\n-0.622905\n-0.104260\n0\n\n\n2\n0.111111\n0.500000\n0.050847\n0.041667\n1\n1\n3\n-0.669520\n-0.051417\n0\n\n\n3\n0.083333\n0.458333\n0.084746\n0.041667\n1\n1\n3\n-0.654153\n-0.102885\n0\n\n\n4\n0.194444\n0.666667\n0.067797\n0.041667\n1\n1\n0\n-0.648788\n0.133488\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n145\n0.666667\n0.416667\n0.711864\n0.916667\n2\n0\n2\n0.551462\n0.059841\n1\n\n\n146\n0.555556\n0.208333\n0.677966\n0.750000\n0\n0\n4\n0.407146\n-0.171821\n2\n\n\n147\n0.611111\n0.416667\n0.711864\n0.791667\n2\n0\n4\n0.447143\n0.037560\n1\n\n\n148\n0.527778\n0.583333\n0.745763\n0.916667\n2\n0\n2\n0.488208\n0.149678\n1\n\n\n149\n0.444444\n0.416667\n0.694915\n0.708333\n0\n0\n4\n0.312066\n-0.031130\n2\n\n\n\n\n150 rows × 10 columns\n\n\n\nAs we only have two dimensions we can easily plot this on a single scatterplot.\n\n# a different seaborn theme\n# see https://python-graph-gallery.com/104-seaborn-themes/\nsns.set_style(\"darkgrid\")\nsns.scatterplot(data = iris_df, x = 'c1', y = 'c2', hue = 'PCA 3 clusters')\n\n&lt;Axes: xlabel='c1', ylabel='c2'&gt;\n\n\n\n\n\nI suspect having two clusters would work better. We should try a few different models.\nCopying the code from here we can fit multiple numbers of clusters.\n\nks = range(1, 10)\ninertias = [] # Create an empty list (will be populated later)\nfor k in ks:\n    # Create a KMeans instance with k clusters: model\n    model = KMeans(n_clusters=k, n_init = 10)\n    \n    # Fit model to samples\n    model.fit(iris_df.iloc[:,-2:])\n    \n    # Append the inertia to the list of inertias\n    inertias.append(model.inertia_)\n    \nplt.plot(ks, inertias, '-o', color='black')\nplt.xlabel('number of clusters, k')\nplt.ylabel('inertia')\nplt.xticks(ks)\nplt.show()\n\n\n\n\nThree seems ok. We clearly want no more than three.\nThese types of plots show an point about model complexity. More free parameters in the model (here the number of clusters) will improve how well the model captures the data, often with reducing returns. However, a model which overfits the data will not be able to fit new data well - referred to overfitting. Randomish internet blogs introduce the topic pretty well, see here, and also wikipedia, see here.\n\n\n12.5.3 Missing values\nFinally, how we deal with missing values can impact the results of PCA and kMeans clustering.\nLets us load in the iris dataset again and randomly remove 10% of the data (see code from here).\n\nimport numpy as np\n\nx = load_iris()\n\n\niris_df = pd.DataFrame(x.data, columns = x.feature_names)\n\nmask = np.random.choice([True, False], size = iris_df.shape, p = [0.2, 0.8])\nmask[mask.all(1),-1] = 0\n\ndf = iris_df.mask(mask)\n\ndf.isna().sum()\n\nsepal length (cm)    26\nsepal width (cm)     31\npetal length (cm)    29\npetal width (cm)     34\ndtype: int64\n\n\n\ndf\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\nNaN\n\n\n2\n4.7\nNaN\nNaN\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\nNaN\n0.2\n\n\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\nNaN\n\n\n146\n6.3\n2.5\n5.0\n1.9\n\n\n147\n6.5\n3.0\n5.2\n2.0\n\n\n148\n6.2\n3.4\n5.4\n2.3\n\n\n149\n5.9\n3.0\n5.1\nNaN\n\n\n\n\n150 rows × 4 columns\n\n\n\nAbout 20% of the data is randomly an NaN.\n\n12.5.3.1 Zeroing\nWe can 0 them and fit our models.\n\ndf_1 = df.copy()\ndf_1 = df_1.fillna(0)\n\n\ndf_1\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.0\n\n\n2\n4.7\n0.0\n0.0\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n0.0\n0.2\n\n\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n0.0\n\n\n146\n6.3\n2.5\n5.0\n1.9\n\n\n147\n6.5\n3.0\n5.2\n2.0\n\n\n148\n6.2\n3.4\n5.4\n2.3\n\n\n149\n5.9\n3.0\n5.1\n0.0\n\n\n\n\n150 rows × 4 columns\n\n\n\n\nk_means_zero = KMeans(n_clusters = 4, init = 'random', n_init = 10)\nk_means_zero.fit(df_1)\ndf_1['Four clusters'] = pd.Series(k_means_zero.predict(df_1.iloc[:,0:4].values), index = df_1.index)\nsns.pairplot(df_1, hue = 'Four clusters')\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but KMeans was fitted with feature names\n  warnings.warn(\n\n\n\n\n\nWhat impact has zeroing the values had on our results?\nNow, onto PCA.\n\n# PCA analysis\nn_components = 2\n\npca = PCA(n_components=n_components)\ndf_1_pca = pca.fit(df_1.iloc[:,0:4])\n\n# Extract projected values\ndf_1_pca_vals = df_1_pca.transform(df_1.iloc[:,0:4])\ndf_1['c1'] = [item[0] for item in df_1_pca_vals]\ndf_1['c2'] = [item[1] for item in df_1_pca_vals]\n\nsns.scatterplot(data = df_1, x = 'c1', y = 'c2')\n\n&lt;Axes: xlabel='c1', ylabel='c2'&gt;\n\n\n\n\n\n\ndf_1_pca.explained_variance_\n\narray([5.79371112, 4.55934375])\n\n\n\ndf_1_pca.components_\n\narray([[-0.86986681,  0.06623728, -0.46988902, -0.13471699],\n       [-0.48335295,  0.05975098,  0.8607085 ,  0.14825862]])\n\n\n\n\n12.5.3.2 Replacing with the average\n\ndf_2 = df.copy()\nfor i in range(4):\n    df_2.iloc[:,i] = df_2.iloc[:,i].fillna(df_2.iloc[:,i].mean())\n\n\ndf_2\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n3.500000\n1.40000\n0.20000\n\n\n1\n4.9\n3.000000\n1.40000\n1.22069\n\n\n2\n4.7\n3.063025\n3.82562\n0.20000\n\n\n3\n4.6\n3.100000\n1.50000\n0.20000\n\n\n4\n5.0\n3.600000\n3.82562\n0.20000\n\n\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.000000\n5.20000\n1.22069\n\n\n146\n6.3\n2.500000\n5.00000\n1.90000\n\n\n147\n6.5\n3.000000\n5.20000\n2.00000\n\n\n148\n6.2\n3.400000\n5.40000\n2.30000\n\n\n149\n5.9\n3.000000\n5.10000\n1.22069\n\n\n\n\n150 rows × 4 columns\n\n\n\n\nk_means_zero = KMeans(n_clusters = 4, init = 'random', n_init = 10)\nk_means_zero.fit(df_2)\ndf_2['Four clusters'] = pd.Series(k_means_zero.predict(df_2.iloc[:,0:4].values), index = df_2.index)\nsns.pairplot(df_2, hue = 'Four clusters')\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but KMeans was fitted with feature names\n  warnings.warn(\n\n\n\n\n\n\n# PCA analysis\nn_components = 2\n\npca = PCA(n_components=n_components)\ndf_2_pca = pca.fit(df_2.iloc[:,0:4])\n\n# Extract projected values\ndf_2_pca_vals = df_2_pca.transform(df_2.iloc[:,0:4])\ndf_2['c1'] = [item[0] for item in df_2_pca_vals]\ndf_2['c2'] = [item[1] for item in df_2_pca_vals]\n\nsns.scatterplot(data = df_2, x = 'c1', y = 'c2')\n\n&lt;Axes: xlabel='c1', ylabel='c2'&gt;\n\n\n\n\n\n\ndf_2_pca.explained_variance_\n\narray([2.93542917, 0.27908233])\n\n\n\ndf_2_pca.components_\n\narray([[ 0.32660641, -0.08120829,  0.89071462,  0.3055502 ],\n       [ 0.87370405,  0.21768933, -0.37573644,  0.21925946]])"
  },
  {
    "objectID": "content/labs/Lab_4/IM939_Lab_4_1_Iris.html#useful-resources",
    "href": "content/labs/Lab_4/IM939_Lab_4_1_Iris.html#useful-resources",
    "title": "12  Lab: Dimension Reduction",
    "section": "12.6 Useful resources",
    "text": "12.6 Useful resources\nThe scikit learn UserGuide is very good. Both approaches here are often referred to as unsupervised learning methods and you can find the scikit learn section on these here.\nIf you have issues with the documentation then also look at the scikit-learn examples.\nAlso, in no particular order:\n\nThe In-Depth sections of the Python Data Science Handbook. More for machine learning but interesting all the same.\nPython for Data Analysis (ebook is available via Warwick library)\n\nIn case you are bored:\n\nStack abuse - Some fun blog entries to look at\nTowards data science - a blog that contains a mix of intro, intermediate and advanced topics. Nice to skim through to try and undrestand something new.\n\nPlease do try out some of the techniques detailed in the lecture material The simple examples found in the scikit learn documentation are rather good. Generally, I find it much easier to try to understand a method using a simple dataset.\n\n\n\n\nFisher, R. A. 1936. “Iris.” UCI Machine Learning Repository. https://doi.org/10.24432/C56C76."
  },
  {
    "objectID": "content/labs/Lab_4/IM939_Lab_4_2_Crime.html",
    "href": "content/labs/Lab_4/IM939_Lab_4_2_Crime.html",
    "title": "13  Lab: Dimension reduction (2)",
    "section": "",
    "text": "In the previous lab, we learned how to work with dimension reduction in a relatively simple dataset. In this lab we are going to work with a much larger dataset: a subset of the US Census & Crime data (Redmond 2009). Details about the various variables can be found here.\nHow can we tackle a dataset with 100 different dimensions? We try using PCA and clustering. There is a lot you can do with this dataset. We encourage you to dig in. Try and find some interesting patterns in the data. Even upload your findings to the Teams channel and discuss.\n\nimport pandas as pd\ndf = pd.read_csv('data/censusCrimeClean.csv')\n\n\ndf\n\n\n\n\n\n\n\n\ncommunityname\nfold\npopulation\nhouseholdsize\nracepctblack\nracePctWhite\nracePctAsian\nracePctHisp\nagePct12t21\nagePct12t29\n...\nNumStreet\nPctForeignBorn\nPctBornSameState\nPctSameHouse85\nPctSameCity85\nPctSameState85\nLandArea\nPopDens\nPctUsePubTrans\nViolentCrimesPerPop\n\n\n\n\n0\nLakewoodcity\n1\n0.19\n0.33\n0.02\n0.90\n0.12\n0.17\n0.34\n0.47\n...\n0.00\n0.12\n0.42\n0.50\n0.51\n0.64\n0.12\n0.26\n0.20\n0.20\n\n\n1\nTukwilacity\n1\n0.00\n0.16\n0.12\n0.74\n0.45\n0.07\n0.26\n0.59\n...\n0.00\n0.21\n0.50\n0.34\n0.60\n0.52\n0.02\n0.12\n0.45\n0.67\n\n\n2\nAberdeentown\n1\n0.00\n0.42\n0.49\n0.56\n0.17\n0.04\n0.39\n0.47\n...\n0.00\n0.14\n0.49\n0.54\n0.67\n0.56\n0.01\n0.21\n0.02\n0.43\n\n\n3\nWillingborotownship\n1\n0.04\n0.77\n1.00\n0.08\n0.12\n0.10\n0.51\n0.50\n...\n0.00\n0.19\n0.30\n0.73\n0.64\n0.65\n0.02\n0.39\n0.28\n0.12\n\n\n4\nBethlehemtownship\n1\n0.01\n0.55\n0.02\n0.95\n0.09\n0.05\n0.38\n0.38\n...\n0.00\n0.11\n0.72\n0.64\n0.61\n0.53\n0.04\n0.09\n0.02\n0.03\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1989\nTempleTerracecity\n10\n0.01\n0.40\n0.10\n0.87\n0.12\n0.16\n0.43\n0.51\n...\n0.00\n0.22\n0.28\n0.34\n0.48\n0.39\n0.01\n0.28\n0.05\n0.09\n\n\n1990\nSeasidecity\n10\n0.05\n0.96\n0.46\n0.28\n0.83\n0.32\n0.69\n0.86\n...\n0.00\n0.53\n0.25\n0.17\n0.10\n0.00\n0.02\n0.37\n0.20\n0.45\n\n\n1991\nWaterburytown\n10\n0.16\n0.37\n0.25\n0.69\n0.04\n0.25\n0.35\n0.50\n...\n0.02\n0.25\n0.68\n0.61\n0.79\n0.76\n0.08\n0.32\n0.18\n0.23\n\n\n1992\nWalthamcity\n10\n0.08\n0.51\n0.06\n0.87\n0.22\n0.10\n0.58\n0.74\n...\n0.01\n0.45\n0.64\n0.54\n0.59\n0.52\n0.03\n0.38\n0.33\n0.19\n\n\n1993\nOntariocity\n10\n0.20\n0.78\n0.14\n0.46\n0.24\n0.77\n0.50\n0.62\n...\n0.08\n0.68\n0.50\n0.34\n0.35\n0.68\n0.11\n0.30\n0.05\n0.48\n\n\n\n\n1994 rows × 102 columns\n\n\n\n\nfrom sklearn.decomposition import PCA\n\nn_components = 2\n \npca = PCA(n_components=n_components)\ndf_pca = pca.fit(df.iloc[:, 1:])\n\n\ndf_pca.explained_variance_\n\narray([8.26130789, 1.08655844])\n\n\n\ndf_pca_vals = pca.fit_transform(df.iloc[:,1:])\ndf['c1'] = [item[0] for item in df_pca_vals]\ndf['c2'] = [item[1] for item in df_pca_vals]\n\n\nimport seaborn as sns\nsns.scatterplot(data = df, x = 'c1', y = 'c2')\n\n&lt;Axes: xlabel='c1', ylabel='c2'&gt;\n\n\n\n\n\nHmm, that looks problematic. What is going on?\nWe should look at the component loadings. Though there are going to be over 200 of them. We can put them into a Pandas dataframe and then sort them.\n\ndata = {'columns' : df.columns[1:102],\n        'component 1' : df_pca.components_[0],\n        'component 2' : df_pca.components_[1]}\n\n\nloadings = pd.DataFrame(data)\nloadings.sort_values(by=['component 1'], ascending=False) \n\n\n\n\n\n\n\n\ncolumns\ncomponent 1\ncomponent 2\n\n\n\n\n0\nfold\n0.999788\n-0.016430\n\n\n12\npctUrban\n0.004944\n0.146079\n\n\n85\nRentHighQ\n0.004078\n0.188187\n\n\n13\nmedIncome\n0.003932\n0.185649\n\n\n46\nPctYoungKids2Par\n0.003818\n0.178675\n\n\n...\n...\n...\n...\n\n\n17\npctWSocSec\n-0.003353\n-0.059245\n\n\n29\nPctPopUnderPov\n-0.003594\n-0.190548\n\n\n18\npctWPubAsst\n-0.003697\n-0.173593\n\n\n10\nagePct65up\n-0.003767\n-0.033010\n\n\n37\nPctOccupManu\n-0.004403\n-0.132659\n\n\n\n\n101 rows × 3 columns\n\n\n\n\nloadings.sort_values(by=['component 2'], ascending=False) \n\n\n\n\n\n\n\n\ncolumns\ncomponent 1\ncomponent 2\n\n\n\n\n85\nRentHighQ\n0.004078\n0.188187\n\n\n13\nmedIncome\n0.003932\n0.185649\n\n\n46\nPctYoungKids2Par\n0.003818\n0.178675\n\n\n20\nmedFamInc\n0.003025\n0.177225\n\n\n45\nPctKids2Par\n0.002385\n0.169581\n\n\n...\n...\n...\n...\n\n\n51\nPctIlleg\n-0.001022\n-0.153504\n\n\n31\nPctNotHSGrad\n-0.002967\n-0.158646\n\n\n18\npctWPubAsst\n-0.003697\n-0.173593\n\n\n29\nPctPopUnderPov\n-0.003594\n-0.190548\n\n\n78\nPctHousNoPhone\n-0.001759\n-0.198116\n\n\n\n\n101 rows × 3 columns\n\n\n\nThe fold variable is messing with our PCA. What does that variable look like.\n\ndf.fold.value_counts()\n\nfold\n1     200\n2     200\n3     200\n4     200\n5     199\n6     199\n7     199\n8     199\n9     199\n10    199\nName: count, dtype: int64\n\n\n\ndf['fold'].unique()\n\narray([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n\n\nAha! It looks to be some sort of categorical variable. Looking into the dataset more, it is actually a variable used for cross tabling.\nWe should not include this variable in our analysis.\n\ndf.iloc[:, 2:-2]\n\n\n\n\n\n\n\n\npopulation\nhouseholdsize\nracepctblack\nracePctWhite\nracePctAsian\nracePctHisp\nagePct12t21\nagePct12t29\nagePct16t24\nagePct65up\n...\nNumStreet\nPctForeignBorn\nPctBornSameState\nPctSameHouse85\nPctSameCity85\nPctSameState85\nLandArea\nPopDens\nPctUsePubTrans\nViolentCrimesPerPop\n\n\n\n\n0\n0.19\n0.33\n0.02\n0.90\n0.12\n0.17\n0.34\n0.47\n0.29\n0.32\n...\n0.00\n0.12\n0.42\n0.50\n0.51\n0.64\n0.12\n0.26\n0.20\n0.20\n\n\n1\n0.00\n0.16\n0.12\n0.74\n0.45\n0.07\n0.26\n0.59\n0.35\n0.27\n...\n0.00\n0.21\n0.50\n0.34\n0.60\n0.52\n0.02\n0.12\n0.45\n0.67\n\n\n2\n0.00\n0.42\n0.49\n0.56\n0.17\n0.04\n0.39\n0.47\n0.28\n0.32\n...\n0.00\n0.14\n0.49\n0.54\n0.67\n0.56\n0.01\n0.21\n0.02\n0.43\n\n\n3\n0.04\n0.77\n1.00\n0.08\n0.12\n0.10\n0.51\n0.50\n0.34\n0.21\n...\n0.00\n0.19\n0.30\n0.73\n0.64\n0.65\n0.02\n0.39\n0.28\n0.12\n\n\n4\n0.01\n0.55\n0.02\n0.95\n0.09\n0.05\n0.38\n0.38\n0.23\n0.36\n...\n0.00\n0.11\n0.72\n0.64\n0.61\n0.53\n0.04\n0.09\n0.02\n0.03\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1989\n0.01\n0.40\n0.10\n0.87\n0.12\n0.16\n0.43\n0.51\n0.35\n0.30\n...\n0.00\n0.22\n0.28\n0.34\n0.48\n0.39\n0.01\n0.28\n0.05\n0.09\n\n\n1990\n0.05\n0.96\n0.46\n0.28\n0.83\n0.32\n0.69\n0.86\n0.73\n0.14\n...\n0.00\n0.53\n0.25\n0.17\n0.10\n0.00\n0.02\n0.37\n0.20\n0.45\n\n\n1991\n0.16\n0.37\n0.25\n0.69\n0.04\n0.25\n0.35\n0.50\n0.31\n0.54\n...\n0.02\n0.25\n0.68\n0.61\n0.79\n0.76\n0.08\n0.32\n0.18\n0.23\n\n\n1992\n0.08\n0.51\n0.06\n0.87\n0.22\n0.10\n0.58\n0.74\n0.63\n0.41\n...\n0.01\n0.45\n0.64\n0.54\n0.59\n0.52\n0.03\n0.38\n0.33\n0.19\n\n\n1993\n0.20\n0.78\n0.14\n0.46\n0.24\n0.77\n0.50\n0.62\n0.40\n0.17\n...\n0.08\n0.68\n0.50\n0.34\n0.35\n0.68\n0.11\n0.30\n0.05\n0.48\n\n\n\n\n1994 rows × 100 columns\n\n\n\n\nfrom sklearn.decomposition import PCA\n\nn_components = 2\n\npca_no_fold = PCA(n_components=n_components)\ndf_pca_no_fold = pca_no_fold.fit(df.iloc[:, 2:-2])\n\ndf_pca_vals = pca_no_fold.fit_transform(df.iloc[:, 2:-2])\n\ndf['c1_no_fold'] = [item[0] for item in df_pca_vals]\ndf['c2_no_fold'] = [item[1] for item in df_pca_vals]\n\nsns.scatterplot(data = df, x = 'c1_no_fold', y = 'c2_no_fold')\n\n&lt;Axes: xlabel='c1_no_fold', ylabel='c2_no_fold'&gt;\n\n\n\n\n\nHow do our component loadings look?\n\ndata = {'columns' : df.iloc[:, 2:-4].columns,\n        'component 1' : df_pca_no_fold.components_[0],\n        'component 2' : df_pca_no_fold.components_[1]}\n\n\nloadings = pd.DataFrame(data)\nloadings_sorted = loadings.sort_values(by=['component 1'], ascending=False)\nloadings_sorted.iloc[1:10,:]\n\n\n\n\n\n\n\n\ncolumns\ncomponent 1\ncomponent 2\n\n\n\n\n12\nmedIncome\n0.185822\n0.040774\n\n\n45\nPctYoungKids2Par\n0.178807\n-0.017237\n\n\n19\nmedFamInc\n0.177283\n0.031895\n\n\n44\nPctKids2Par\n0.169533\n-0.042621\n\n\n43\nPctFam2Par\n0.163277\n-0.031094\n\n\n82\nRentLowQ\n0.163120\n0.119274\n\n\n85\nMedRent\n0.163120\n0.106135\n\n\n80\nOwnOccMedVal\n0.159850\n0.135830\n\n\n83\nRentMedian\n0.159605\n0.113360\n\n\n\n\n\n\n\n\nloadings_sorted = loadings.sort_values(by=['component 2'], ascending=False)\nloadings_sorted.iloc[1:10,:]\n\n\n\n\n\n\n\n\ncolumns\ncomponent 1\ncomponent 2\n\n\n\n\n59\nPctRecImmig10\n-0.000556\n0.253476\n\n\n57\nPctRecImmig5\n-0.000901\n0.252463\n\n\n56\nPctRecentImmig\n0.001797\n0.248239\n\n\n91\nPctForeignBorn\n0.024743\n0.241783\n\n\n61\nPctNotSpeakEnglWell\n-0.048595\n0.207575\n\n\n5\nracePctHisp\n-0.063868\n0.187826\n\n\n68\nPctPersDenseHous\n-0.087193\n0.184704\n\n\n11\npctUrban\n0.146557\n0.183317\n\n\n4\nracePctAsian\n0.061547\n0.160710\n\n\n\n\n\n\n\nInteresting that our first component variables are income related whereas our second component variables are immigration related. If we look at the projections of the model, coloured by Crime, what do we see?\n\nsns.scatterplot(data = df,\n                x = 'c1_no_fold',\n                y = 'c2_no_fold',\n                hue = 'ViolentCrimesPerPop',\n                size = 'ViolentCrimesPerPop')\n\n&lt;Axes: xlabel='c1_no_fold', ylabel='c2_no_fold'&gt;\n\n\n\n\n\nWhat about clustering using these ‘income’ and ‘immigration’ components?\n\nfrom sklearn.cluster import KMeans\n\nks = range(1, 10)\ninertias = []\nfor k in ks:\n    # Create a KMeans instance with k clusters: model\n    model = KMeans(n_clusters=k, n_init = 10)\n    \n    # Fit model to samples\n    model.fit(df[['c1_no_fold', 'c2_no_fold']])\n    \n    # Append the inertia to the list of inertias\n    inertias.append(model.inertia_)\n\nimport matplotlib.pyplot as plt\n\nplt.plot(ks, inertias, '-o', color='black')\nplt.xlabel('number of clusters, k')\nplt.ylabel('inertia')\nplt.xticks(ks)\nplt.show()\n\n\n\n\nFour clusters looks good.\n\nk_means_4 = KMeans(n_clusters = 3, init = 'random', n_init = 10)\nk_means_4.fit(df[['c1_no_fold', 'c2_no_fold']])\ndf['Four clusters'] = pd.Series(k_means_4.predict(df[['c1_no_fold', 'c2_no_fold']].values), index = df.index)\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but KMeans was fitted with feature names\n  warnings.warn(\n\n\n\nsns.scatterplot(data = df, x = 'c1_no_fold', y = 'c2_no_fold', hue = 'Four clusters')\n\n&lt;Axes: xlabel='c1_no_fold', ylabel='c2_no_fold'&gt;\n\n\n\n\n\nHmm, might we interpret this plot? The plot is unclear. Let us assume c1 is an income component and c2 is an immigration component. Cluster 0 are places high on income and low on immigration. Cluster 2 are low on income and low on immigration. The interesting group are those high on immigration and relatively low on income.\nWe can include crime on the plot.\n\nimport matplotlib.pyplot as plt\n\nplt.rcParams[\"figure.figsize\"] = (15,10)\nsns.scatterplot(data = df, x = 'c1_no_fold', y = 'c2_no_fold', hue = 'Four clusters', size = 'ViolentCrimesPerPop')\n\n&lt;Axes: xlabel='c1_no_fold', ylabel='c2_no_fold'&gt;\n\n\n\n\n\nAlas, we stop here in this exercise. You could work on this further. There are outliers you might want to remove. Or, at this stage, you might want to look at more components, 3 or 4 and look for some other factors.\n\n\n\n\nRedmond, Michael. 2009. “Communities and Crime.” UCI Machine Learning Repository. https://doi.org/10.24432/C53W3X."
  },
  {
    "objectID": "content/labs/Lab_4/IM939_Lab_4_Exercises.html",
    "href": "content/labs/Lab_4/IM939_Lab_4_Exercises.html",
    "title": "14  Exercise: Wine dataset",
    "section": "",
    "text": "15 Dimension reduction\nfrom sklearn.decomposition import PCA\n\nn_components = 2\n \npca = PCA(n_??????????=n_components)\ndf_pca = pca.fit(df?iloc[:, 0:11])\n\nSyntaxError: invalid syntax (2518476773.py, line 5)\ndf_pca_vals = df_pca.???_transform(df.iloc[:, 0:11])\ndf['c1'] = [item[0] for item in df_pca_????]\ndf['c2'] = [item[1] for item in df_pca_vals]\n\nSyntaxError: invalid syntax (2292344659.py, line 1)\nsns.scatterplot(data = df, x = ?, y = ?, hue = 'quality')\n\nSyntaxError: invalid syntax (321227352.py, line 1)\nprint(df.columns)\ndf_pca.components_\n\nNameError: name 'df' is not defined\nWhat about other dimension reduction methods?"
  },
  {
    "objectID": "content/labs/Lab_4/IM939_Lab_4_Exercises.html#load-data-and-import-libraries",
    "href": "content/labs/Lab_4/IM939_Lab_4_Exercises.html#load-data-and-import-libraries",
    "title": "14  Exercise: Wine dataset",
    "section": "14.1 Load data and import libraries",
    "text": "14.1 Load data and import libraries\n\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport seaborn as sn?\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PC?\nfrom sklearn.decomposition import S????ePCA\nfrom sklearn.manifold import TSNE\n\ndf = pd.read_excel('data/winequality-red_v2.xlsx')\n\nSyntaxError: invalid syntax (1138690625.py, line 6)\n\n\n\ndf.h??d()\n\nSyntaxError: invalid syntax (197674394.py, line 1)\n\n\n\n# May take a while depending on your computer\n# feel free not to run this\nsns.pair????(df)\n\nSyntaxError: invalid syntax (4010129658.py, line 3)"
  },
  {
    "objectID": "content/labs/Lab_4/IM939_Lab_4_Exercises.html#sparcepca",
    "href": "content/labs/Lab_4/IM939_Lab_4_Exercises.html#sparcepca",
    "title": "14  Exercise: Wine dataset",
    "section": "15.1 SparcePCA",
    "text": "15.1 SparcePCA\n\ns_pca = SparsePCA(n_components=n_components)\ndf_s_pca = s_pca.fit(df.????[:, 0:11])\n\nSyntaxError: invalid syntax (2237240520.py, line 2)\n\n\n\ndf_s_pca_vals = s_pca.fit_?????????(df.iloc[:, 0:11])\ndf['c1 spca'] = [item[0] for item in df_s_pca_vals]\ndf['c2 spca'] = [item[1] for item in df_s_pca_vals]\n\nSyntaxError: invalid syntax (496583237.py, line 1)\n\n\n\nsns.scatterplot(data = df, x = 'c1 spca', y = 'c2 spca', hue = 'quality')\n\nNameError: name 'sns' is not defined"
  },
  {
    "objectID": "content/labs/Lab_4/IM939_Lab_4_Exercises.html#tsne",
    "href": "content/labs/Lab_4/IM939_Lab_4_Exercises.html#tsne",
    "title": "14  Exercise: Wine dataset",
    "section": "15.2 tSNE",
    "text": "15.2 tSNE\n\ntsne_model = TSNE(n_components=n_components)\ndf_tsne = tsne_model.fit(df.iloc[:, 0:11])\n\nNameError: name 'TSNE' is not defined\n\n\n\ndf_tsne_vals = tsne_model.fit_transform(df.iloc[:, 0:11])\ndf['c1 tsne'] = [item[0] for item in ??_tsne_vals]\ndf['c2 tsne'] = [item[1] for item in df_tsne_vals]\n\nSyntaxError: invalid syntax (3280343393.py, line 2)\n\n\n\n# This plot does not look right\n# I am not sure why.\nsns.scatterplot(data = ??, x = 'c1 tsne', y = 'c1 tsne', hue = 'quality')\n\nSyntaxError: invalid syntax (847552475.py, line 3)\n\n\nThat looks concerning - there is a straight line. It looks like something in the data has caused the model to have issues.\nDoes normalising the data sort out the issue?\n\nfrom sklearn.preprocessing import MinMaxScaler\ncol_names = df.columns\nscaled_df =  pd.DataFrame(MinMaxScaler().fit_transform(df))\nscaled_df.columns = col_names\n\nNameError: name 'df' is not defined\n\n\n\ntsne_model = TSNE(n_components=n_components)\n\nscaled_df_tsne = tsne_model.fit(scaled_df.iloc[:, 0:11])\nscaled_df_tsne_vals = tsne_model.fit_transform(df.iloc[:, 0:11])\n\nscaled_df['c1 tsne'] = [item[0] for item in scaled_df_tsne_vals]\nscaled_df['c2 tsne'] = [item[1] for item in scaled_df_tsne_vals]\n\nsns.scatterplot(data = scaled_df, x = 'c1 tsne', y = 'c1 tsne', hue = 'quality')\n\nNameError: name 'TSNE' is not defined\n\n\nNormalising the data makes no difference. It could be the model is getting stuck somehow. You could check the various attributes of the tsne fit object (tsne_model.fit), try using only a few columns and search google a lot - this could be a problem other have encountered.\nFor now, we will use PCA components.\n\ndata = {'columns' : df.iloc[:, 0:11].columns,\n        'component 1' : df_pca.components_[0],\n        'component 2' : df_pca.components_[1]}\n\n\nloadings = pd.?????????(data)\nloadings_sorted = loadings.sort_values(by=['component 1'], ascending=False)\nloadings_sorted.iloc[1:10,:]\n\nSyntaxError: invalid syntax (4262587979.py, line 6)\n\n\n\nloadings_sorted = loadings.sort_values(by=['component 2'], ascending=False)\nloadings_sorted.iloc[1:10,:]\n\nNameError: name 'loadings' is not defined"
  },
  {
    "objectID": "content/labs/Lab_4/IM939_Lab_4_Exercises.html#clustering",
    "href": "content/labs/Lab_4/IM939_Lab_4_Exercises.html#clustering",
    "title": "14  Exercise: Wine dataset",
    "section": "15.3 Clustering",
    "text": "15.3 Clustering\n\nfrom sklearn.cluster import KMeans\n\nks = range(1, 10)\ninertias = []\nfor k in ks:\n    # Create a KMeans instance with k clusters: model\n    ????? = KMeans(n_clusters=k)\n    \n    # Fit model to samples\n    model.fit(df[['c1', 'c2']])\n    \n    # Append the inertia to the list of inertias\n    inertias.append(model.inertia_)\n\nimport matplotlib.pyplot as plt\n\nplt.plot(ks, inertias, '-o', color='black')\nplt.xlabel('number of clusters, k')\nplt.ylabel('inertia')\nplt.xticks(ks)\nplt.show()\n\nObject `??? = KMeans(n_clusters=k)` not found.\n\n\nNameError: name 'model' is not defined\n\n\n\nk_means_3 = KMeans(n_clusters = 3, init = 'random')\nk_means_3.fit(df[['c1', 'c2']])\ndf['Three clusters'] = pd.Series(k_means_3.???????(df[['c1', 'c2']].values), index = df.index)\n\nSyntaxError: invalid syntax (2729138574.py, line 3)\n\n\n\nsns.scatterplot(data = df, x = 'c1', y = 'c2', hue = 'Three clusters')\n\nNameError: name 'sns' is not defined\n\n\nConsider:\n\nIs that userful?\nWhat might it mean?\n\nOutside of this session you could try normalising the data (centering around the mean), clustering the raw data (and not the projections from PCA), trying to get tSNE working or using different numbers of components."
  },
  {
    "objectID": "content/labs/Lab_5/IM939_Lab_5_1.html#workflow",
    "href": "content/labs/Lab_5/IM939_Lab_5_1.html#workflow",
    "title": "15  Lab: Clustering and Ground Truth",
    "section": "15.1 Workflow",
    "text": "15.1 Workflow\nOur labs have focused on data analysis. The goal here is to try and understand informative patterns in our data. These patterns allow us to answer questions.\nTo do this we:\n\nRead data into Python.\nLook at our data.\nWrangling our data. Often exploring raw data and dealing with missing values (imputation techniques), transformaing, normalising, standardising, outliers or reshaping.\nCarry out a series of analysis to better understand our data via clustering, regressions analysis, dimension reduction, and many other techniques.\nReflect on what the patterns in our data can tell us.\n\nThese are not mutually exclusive processes and are not exhaustive. One may review our data after cleaning, load in more data, carry out additional analysis and/or fit multiple models, tweak data summaries or adopt new techniques. Reflecting on the patterns are in our data can give way to additional analysis and processing."
  },
  {
    "objectID": "content/labs/Lab_5/IM939_Lab_5_1.html#so-far",
    "href": "content/labs/Lab_5/IM939_Lab_5_1.html#so-far",
    "title": "15  Lab: Clustering and Ground Truth",
    "section": "15.2 So far",
    "text": "15.2 So far\nA quick reminder, our toolkit comprises of:\n\nPandas - table like data structures. Packed with methods for summarising and manipulating data. Documentation. Cheat sheet.\nSeaborn - helping us create statistical data visualisations. Tutorials.\nScikit-learn - An accessible collection of functions and object for analysing data. These analysis include dimension reduction, clustering, regressions and evaluating our models. Examples.\n\nThese tools comprise some of the core Python data science stack and allow us to tackle many of the elements from each week.\nWeek 2 Tidy data, data types, wrangling data, imputation (missing data), transformations.\nWeek 3 Descriptive statistics, distributions, models (e.g., regression).\nWeek 4 Feature selection, dimension reduction (e.g., Principle Component Analysis, Multidimensional scaling, Linear Discriminant Analysis, t-SNE, Correspondance Analysis), clustering (e.g., Hierarchical Clustering, Partioning-based clustering such as K-means).\nWe have also encountered two dataset sources.\n\nsklearn example datasets\nUCI Machine Learning Repository\n\nYou can learn a lot by picking a dataset, choosing a possible research question and carrying a series of analysis. I encourage you to do so outside of the session. It certainly forces one to read the documentation and explore the wonderful possabilities."
  },
  {
    "objectID": "content/labs/Lab_5/IM939_Lab_5_1.html#this-week",
    "href": "content/labs/Lab_5/IM939_Lab_5_1.html#this-week",
    "title": "15  Lab: Clustering and Ground Truth",
    "section": "15.3 This week",
    "text": "15.3 This week\nTrying to understand patterns in data often requires us to fit multiple models. We need to consider how well a given model (a kmeans cluster, a linear regression, dimension reduction, etc.) performs.\nSpecifically, we will look at:\n\nComparing clusters to the ‘ground truth’ - the wine dataset\nCross validation of linear regression - the crime dataset\nInvestigating multidimensional scaling - the london borough dataset\nVisualising the overlap in clustering results\n\n\n15.3.1 Clustering and ground truth\nLoad in the wine dataset. Details of the dataset are [here]https://archive.ics.uci.edu/ml/datasets/wine).\n\nimport pandas as pd\n\ndf = pd.read_csv('data/wine.csv')\n\nLook at our data.\n\ndf.head()\n\n\n\n\n\n\n\n\nClass label\nAlcohol\nMalic acid\nAsh\nAlcalinity of ash\nMagnesium\nTotal phenols\nFlavanoids\nNonflavanoid phenols\nProanthocyanins\nColor intensity\nHue\nOD280/OD315 of diluted wines\nProline\n\n\n\n\n0\n1\n14.23\n1.71\n2.43\n15.6\n127\n2.80\n3.06\n0.28\n2.29\n5.64\n1.04\n3.92\n1065\n\n\n1\n1\n13.20\n1.78\n2.14\n11.2\n100\n2.65\n2.76\n0.26\n1.28\n4.38\n1.05\n3.40\n1050\n\n\n2\n1\n13.16\n2.36\n2.67\n18.6\n101\n2.80\n3.24\n0.30\n2.81\n5.68\n1.03\n3.17\n1185\n\n\n3\n1\n14.37\n1.95\n2.50\n16.8\n113\n3.85\n3.49\n0.24\n2.18\n7.80\n0.86\n3.45\n1480\n\n\n4\n1\n13.24\n2.59\n2.87\n21.0\n118\n2.80\n2.69\n0.39\n1.82\n4.32\n1.04\n2.93\n735\n\n\n\n\n\n\n\nThere is a column called Class label that gives us the ground truth. The wines come from three different cultivars. Knowing the actual grouping helps us to identify how well our methods can capture this ground truth.\nFollowing our process above, we should first get a sense of our data.\n\ndf.describe()\n\n\n\n\n\n\n\n\nClass label\nAlcohol\nMalic acid\nAsh\nAlcalinity of ash\nMagnesium\nTotal phenols\nFlavanoids\nNonflavanoid phenols\nProanthocyanins\nColor intensity\nHue\nOD280/OD315 of diluted wines\nProline\n\n\n\n\ncount\n178.000000\n178.000000\n178.000000\n178.000000\n178.000000\n178.000000\n178.000000\n178.000000\n178.000000\n178.000000\n178.000000\n178.000000\n178.000000\n178.000000\n\n\nmean\n1.938202\n13.000618\n2.336348\n2.366517\n19.494944\n99.741573\n2.295112\n2.029270\n0.361854\n1.590899\n5.058090\n0.957449\n2.611685\n746.893258\n\n\nstd\n0.775035\n0.811827\n1.117146\n0.274344\n3.339564\n14.282484\n0.625851\n0.998859\n0.124453\n0.572359\n2.318286\n0.228572\n0.709990\n314.907474\n\n\nmin\n1.000000\n11.030000\n0.740000\n1.360000\n10.600000\n70.000000\n0.980000\n0.340000\n0.130000\n0.410000\n1.280000\n0.480000\n1.270000\n278.000000\n\n\n25%\n1.000000\n12.362500\n1.602500\n2.210000\n17.200000\n88.000000\n1.742500\n1.205000\n0.270000\n1.250000\n3.220000\n0.782500\n1.937500\n500.500000\n\n\n50%\n2.000000\n13.050000\n1.865000\n2.360000\n19.500000\n98.000000\n2.355000\n2.135000\n0.340000\n1.555000\n4.690000\n0.965000\n2.780000\n673.500000\n\n\n75%\n3.000000\n13.677500\n3.082500\n2.557500\n21.500000\n107.000000\n2.800000\n2.875000\n0.437500\n1.950000\n6.200000\n1.120000\n3.170000\n985.000000\n\n\nmax\n3.000000\n14.830000\n5.800000\n3.230000\n30.000000\n162.000000\n3.880000\n5.080000\n0.660000\n3.580000\n13.000000\n1.710000\n4.000000\n1680.000000\n\n\n\n\n\n\n\nNo missing data. The scales of our features vary (e.g., Magnesium is in the 100s whereas Hue is in the low single digits).\nHow about our feature distributions?\n\ndf_long = df.melt(id_vars='Class label')\n\n\nimport seaborn as sns\n\nsns.violinplot(data = df_long, x = 'variable', y = 'value')\n\n&lt;Axes: xlabel='variable', ylabel='value'&gt;\n\n\n\n\n\nMakes sense to normalise our data.\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n# create a scaler object\nscaler = MinMaxScaler()\n\n# fit and transform the data\ndf_norm = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\ndf_long = df_norm.melt(id_vars='Class label')\ndf_long\n\n\n\n\n\n\n\n\nClass label\nvariable\nvalue\n\n\n\n\n0\n0.0\nAlcohol\n0.842105\n\n\n1\n0.0\nAlcohol\n0.571053\n\n\n2\n0.0\nAlcohol\n0.560526\n\n\n3\n0.0\nAlcohol\n0.878947\n\n\n4\n0.0\nAlcohol\n0.581579\n\n\n...\n...\n...\n...\n\n\n2309\n1.0\nProline\n0.329529\n\n\n2310\n1.0\nProline\n0.336662\n\n\n2311\n1.0\nProline\n0.397290\n\n\n2312\n1.0\nProline\n0.400856\n\n\n2313\n1.0\nProline\n0.201141\n\n\n\n\n2314 rows × 3 columns\n\n\n\n\nsns.violinplot(data = df_long, x = 'variable', y = 'value')\n\n&lt;Axes: xlabel='variable', ylabel='value'&gt;\n\n\n\n\n\nAre there any patterns?\nHow about a pairplot?\n\nsns.pairplot(data = df_norm.iloc[:,1:])\n\n\n\n\nHmm, a few interesting correlations. Some of our variables are skewed. We could apply some PCA here to look at fewer dimension or even log transform some of the skewed variables.\nFor now we will just run a kmeans cluster and then check our results against the ground truth.\nLets decide how many clusters we need.\n\nfrom sklearn.cluster import KMeans\n\nks = range(1, 10)\ninertias = []\nfor k in ks:\n    # Create a KMeans instance with k clusters: model\n    model = KMeans(n_clusters=k, n_init = 10)\n    \n    # Fit model to samples\n    model.fit(df.iloc[:,1:])\n    \n    # Append the inertia to the list of inertias\n    inertias.append(model.inertia_)\n\nimport matplotlib.pyplot as plt\n\nplt.plot(ks, inertias, '-o', color='black')\nplt.xlabel('number of clusters, k')\nplt.ylabel('inertia')\nplt.xticks(ks)\nplt.show()\n\n\n\n\nWhat happens if we use the normalised data instead?\n\nfrom sklearn.cluster import KMeans\n\nks = range(1, 10)\ninertias = []\nfor k in ks:\n    # Create a KMeans instance with k clusters: model\n    model = KMeans(n_clusters=k, n_init = 10)\n    \n    # Fit model to samples\n    model.fit(df_norm.iloc[:,1:])\n    \n    # Append the inertia to the list of inertias\n    inertias.append(model.inertia_)\n\nimport matplotlib.pyplot as plt\n\nplt.plot(ks, inertias, '-o', color='black')\nplt.xlabel('number of clusters, k')\nplt.ylabel('inertia')\nplt.xticks(ks)\nplt.show()\n\n\n\n\nBoth of the graphs are the same. Is that what you would expect?\nThree clusters seems about right (and matches our number of origonal labels).\n\ndf['Class label'].value_counts()\n\nClass label\n2    71\n1    59\n3    48\nName: count, dtype: int64\n\n\n\n# Create a KMeans instance with k clusters: model\nk_means = KMeans(n_clusters=3)\n\n# Fit model to samples\ndf_k_means = k_means.fit(df.iloc[:,1:])\n\ndf['Three clusters'] = pd.Series(df_k_means.predict(df.iloc[:,1:].values), index = df.index)\ndf\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but KMeans was fitted with feature names\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nClass label\nAlcohol\nMalic acid\nAsh\nAlcalinity of ash\nMagnesium\nTotal phenols\nFlavanoids\nNonflavanoid phenols\nProanthocyanins\nColor intensity\nHue\nOD280/OD315 of diluted wines\nProline\nThree clusters\n\n\n\n\n0\n1\n14.23\n1.71\n2.43\n15.6\n127\n2.80\n3.06\n0.28\n2.29\n5.64\n1.04\n3.92\n1065\n1\n\n\n1\n1\n13.20\n1.78\n2.14\n11.2\n100\n2.65\n2.76\n0.26\n1.28\n4.38\n1.05\n3.40\n1050\n1\n\n\n2\n1\n13.16\n2.36\n2.67\n18.6\n101\n2.80\n3.24\n0.30\n2.81\n5.68\n1.03\n3.17\n1185\n1\n\n\n3\n1\n14.37\n1.95\n2.50\n16.8\n113\n3.85\n3.49\n0.24\n2.18\n7.80\n0.86\n3.45\n1480\n1\n\n\n4\n1\n13.24\n2.59\n2.87\n21.0\n118\n2.80\n2.69\n0.39\n1.82\n4.32\n1.04\n2.93\n735\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n173\n3\n13.71\n5.65\n2.45\n20.5\n95\n1.68\n0.61\n0.52\n1.06\n7.70\n0.64\n1.74\n740\n0\n\n\n174\n3\n13.40\n3.91\n2.48\n23.0\n102\n1.80\n0.75\n0.43\n1.41\n7.30\n0.70\n1.56\n750\n0\n\n\n175\n3\n13.27\n4.28\n2.26\n20.0\n120\n1.59\n0.69\n0.43\n1.35\n10.20\n0.59\n1.56\n835\n0\n\n\n176\n3\n13.17\n2.59\n2.37\n20.0\n120\n1.65\n0.68\n0.53\n1.46\n9.30\n0.60\n1.62\n840\n0\n\n\n177\n3\n14.13\n4.10\n2.74\n24.5\n96\n2.05\n0.76\n0.56\n1.35\n9.20\n0.61\n1.60\n560\n2\n\n\n\n\n178 rows × 15 columns\n\n\n\nDo our cluster labels match our ground truth? Did our cluster model capture reality?\n\nct = pd.crosstab(df['Three clusters'], df['Class label'])\nct\n\n\n\n\n\n\n\nClass label\n1\n2\n3\n\n\nThree clusters\n\n\n\n\n\n\n\n0\n13\n20\n29\n\n\n1\n46\n1\n0\n\n\n2\n0\n50\n19\n\n\n\n\n\n\n\nIt might be easier to see as a stacked plot (see this post).\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nct.plot.bar(stacked=True)\nplt.legend(title='Class label')\n\n&lt;matplotlib.legend.Legend at 0x163ba7c90&gt;\n\n\n\n\n\nHow has the kmeans model done compared to our ground truth?\nWe need to be really careful here. We notice that it is not easily possible to compare the known class labels to clustering labels. The reason is that the clustering algorithm labels are just arbitrary and not assigned to any deterministic criteria. Each time you run the algorithm, you might get a different id for the labels. The reason is that the label itself doesn’t actually mean anything, what is important is the list of items that are in the same cluster and their relations.\nA way to come over this ambiguity and evaluate the results is to look at a visualisations of the results and compare. But this brings in the question of what type of visualisation to use for looking at the clusters. An immediate alternative is to use scatterplots. However, it is not clear which axis to use for clustering. A common method to apply at this stage is to make use of PCA to get a 2D plane where we can project the data points and visualise them over this projection.\n\ndf.iloc[:,1:14]\n\n\n\n\n\n\n\n\nAlcohol\nMalic acid\nAsh\nAlcalinity of ash\nMagnesium\nTotal phenols\nFlavanoids\nNonflavanoid phenols\nProanthocyanins\nColor intensity\nHue\nOD280/OD315 of diluted wines\nProline\n\n\n\n\n0\n14.23\n1.71\n2.43\n15.6\n127\n2.80\n3.06\n0.28\n2.29\n5.64\n1.04\n3.92\n1065\n\n\n1\n13.20\n1.78\n2.14\n11.2\n100\n2.65\n2.76\n0.26\n1.28\n4.38\n1.05\n3.40\n1050\n\n\n2\n13.16\n2.36\n2.67\n18.6\n101\n2.80\n3.24\n0.30\n2.81\n5.68\n1.03\n3.17\n1185\n\n\n3\n14.37\n1.95\n2.50\n16.8\n113\n3.85\n3.49\n0.24\n2.18\n7.80\n0.86\n3.45\n1480\n\n\n4\n13.24\n2.59\n2.87\n21.0\n118\n2.80\n2.69\n0.39\n1.82\n4.32\n1.04\n2.93\n735\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n173\n13.71\n5.65\n2.45\n20.5\n95\n1.68\n0.61\n0.52\n1.06\n7.70\n0.64\n1.74\n740\n\n\n174\n13.40\n3.91\n2.48\n23.0\n102\n1.80\n0.75\n0.43\n1.41\n7.30\n0.70\n1.56\n750\n\n\n175\n13.27\n4.28\n2.26\n20.0\n120\n1.59\n0.69\n0.43\n1.35\n10.20\n0.59\n1.56\n835\n\n\n176\n13.17\n2.59\n2.37\n20.0\n120\n1.65\n0.68\n0.53\n1.46\n9.30\n0.60\n1.62\n840\n\n\n177\n14.13\n4.10\n2.74\n24.5\n96\n2.05\n0.76\n0.56\n1.35\n9.20\n0.61\n1.60\n560\n\n\n\n\n178 rows × 13 columns\n\n\n\n\nfrom sklearn.decomposition import PCA\n\nn_components = 2\n\npca = PCA(n_components=n_components)\ndf_pca = pca.fit(df.iloc[:,1:14])\ndf_pca_vals = df_pca.transform(df.iloc[:,1:14])\n\nGrab our projections and plot along with our cluster names.\n\ndf['c1'] = [item[0] for item in df_pca_vals]\ndf['c2'] = [item[1] for item in df_pca_vals]\n\nax = sns.scatterplot(data = df, x = 'c1', y = 'c2', hue = 'Class label')\nax.set_title('Known labels visualised over PCs')\n\nText(0.5, 1.0, 'Known labels visualised over PCs')\n\n\n\n\n\nIn the figure above, we colored the points based on the actual labels, we observe that there has been several misclassifications in the figure above (i.e., in the algorithm’s results). So one may choose to use an alternative algorithm or devise a better distance metric.\n\nax = sns.scatterplot(data = df, x = 'c1', y = 'c2', hue = 'Three clusters')\nax.set_title('Results of the algorithm visualised over PCs')\n\nText(0.5, 1.0, 'Results of the algorithm visualised over PCs')\n\n\n\n\n\nThis shows the parallelism between the clustering algorithm and PCA. By looking at the PCA loadings, we can find out what the x-axis mean and try to interpret the clusters (We leave this as an additional exercise for those interested).\nHow might your interpret the above plots? Did the kmeans model identify the ground truth?\nHow robust is our clustering? It may be that the kmeans algorithm becamse stuck or that a few outliers have biased the clustering.\nTwo ways to check are:\n\nRunning the model multiple times with different initial values.\nRemoving some data and running the modelling multiple times.\n\nRun the below cell a few times. What do you see?\n\n# Create a KMeans instance with k clusters: model\nk_means = KMeans(n_clusters=3, init='random', n_init = 10)\n\n# Fit model to samples\ndf_k_means = k_means.fit(df.iloc[:,1:14])\n\ndf['Three clusters'] = pd.Series(df_k_means.predict(df.iloc[:,1:14].values), index = df.index)\n\nax = sns.scatterplot(data = df, x = 'c1', y = 'c2', hue = 'Three clusters')\nax.set_title('Results of the algorithm visualised over PCs')\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but KMeans was fitted with feature names\n  warnings.warn(\n\n\nText(0.5, 1.0, 'Results of the algorithm visualised over PCs')\n\n\n\n\n\nHow about with only 80% of the data?\n\ndf_sample = df.sample(frac=0.8, replace=False)\n\n# Create a KMeans instance with k clusters: model\nk_means = KMeans(n_clusters=3, init='random', n_init = 10)\n\n# Fit model to samples\ndf_k_means = k_means.fit(df_sample.iloc[:,1:14])\n\ndf_sample['Three clusters'] = pd.Series(df_k_means.predict(df_sample.iloc[:,1:14].values), index = df_sample.index)\n\nax = sns.scatterplot(data = df_sample, x = 'c1', y = 'c2', hue = 'Three clusters')\nax.set_title('Results of the algorithm visualised over PCs')\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but KMeans was fitted with feature names\n  warnings.warn(\n\n\nText(0.5, 1.0, 'Results of the algorithm visualised over PCs')\n\n\n\n\n\nWe may want to automate the process of resampling the data or rerunning the model then perhaps plotting the different inertia values or creating different plots.\nDo you think our clustering algorithm is stable and provide similiar results even when some data is removed or the initial values are random?\nIf so, then is our algorithm capturing the ground truth?"
  },
  {
    "objectID": "content/labs/Lab_5/IM939_Lab_5_2.html",
    "href": "content/labs/Lab_5/IM939_Lab_5_2.html",
    "title": "16  Lab: Cross validation",
    "section": "",
    "text": "Details of the crime dataset are here.\nWe are going to examine the data, fit and then cross-validate a regression model.\n\nimport pandas as pd\ndf = pd.read_csv('data/censusCrimeClean.csv')\ndf.head()\n\n\n\n\n\n\n\n\ncommunityname\nfold\npopulation\nhouseholdsize\nracepctblack\nracePctWhite\nracePctAsian\nracePctHisp\nagePct12t21\nagePct12t29\n...\nNumStreet\nPctForeignBorn\nPctBornSameState\nPctSameHouse85\nPctSameCity85\nPctSameState85\nLandArea\nPopDens\nPctUsePubTrans\nViolentCrimesPerPop\n\n\n\n\n0\nLakewoodcity\n1\n0.19\n0.33\n0.02\n0.90\n0.12\n0.17\n0.34\n0.47\n...\n0.0\n0.12\n0.42\n0.50\n0.51\n0.64\n0.12\n0.26\n0.20\n0.20\n\n\n1\nTukwilacity\n1\n0.00\n0.16\n0.12\n0.74\n0.45\n0.07\n0.26\n0.59\n...\n0.0\n0.21\n0.50\n0.34\n0.60\n0.52\n0.02\n0.12\n0.45\n0.67\n\n\n2\nAberdeentown\n1\n0.00\n0.42\n0.49\n0.56\n0.17\n0.04\n0.39\n0.47\n...\n0.0\n0.14\n0.49\n0.54\n0.67\n0.56\n0.01\n0.21\n0.02\n0.43\n\n\n3\nWillingborotownship\n1\n0.04\n0.77\n1.00\n0.08\n0.12\n0.10\n0.51\n0.50\n...\n0.0\n0.19\n0.30\n0.73\n0.64\n0.65\n0.02\n0.39\n0.28\n0.12\n\n\n4\nBethlehemtownship\n1\n0.01\n0.55\n0.02\n0.95\n0.09\n0.05\n0.38\n0.38\n...\n0.0\n0.11\n0.72\n0.64\n0.61\n0.53\n0.04\n0.09\n0.02\n0.03\n\n\n\n\n5 rows × 102 columns\n\n\n\nOne hundred features. Too many for us to visualise at once.\nInstead, we can pick out particular variables and carry out a linear regression. To make our work simple we will look at ViolentCrimesPerPop as our dependent variable and medIncome as our indpendent variable.\nWe may wonder if there is more violent crime in low income areas.\nLet us create a new dataframe containing our regression variables. We do not have to do this I find it makes our work clearer.\n\ndf_reg = df[['communityname', 'medIncome', 'ViolentCrimesPerPop']]\ndf_reg\n\n\n\n\n\n\n\n\ncommunityname\nmedIncome\nViolentCrimesPerPop\n\n\n\n\n0\nLakewoodcity\n0.37\n0.20\n\n\n1\nTukwilacity\n0.31\n0.67\n\n\n2\nAberdeentown\n0.30\n0.43\n\n\n3\nWillingborotownship\n0.58\n0.12\n\n\n4\nBethlehemtownship\n0.50\n0.03\n\n\n...\n...\n...\n...\n\n\n1989\nTempleTerracecity\n0.42\n0.09\n\n\n1990\nSeasidecity\n0.28\n0.45\n\n\n1991\nWaterburytown\n0.31\n0.23\n\n\n1992\nWalthamcity\n0.44\n0.19\n\n\n1993\nOntariocity\n0.40\n0.48\n\n\n\n\n1994 rows × 3 columns\n\n\n\nPlot our data (a nice page on plotting regressions with seaborn is here).\n\nimport seaborn as sns\nsns.jointplot(data = df[['medIncome', 'ViolentCrimesPerPop']], \n              x = 'ViolentCrimesPerPop', \n              y = 'medIncome', kind='reg',\n              marker = '.')\n\n\n\n\nWe may want to z-transform or log these scores as they are heavily skewed.\n\nimport numpy as np\n\n# some values are 0 so 0.1 is added to prevent log giving us infinity\n# there may be a better way to do this!\ndf_reg.loc[:, 'ViolentCrimesPerPop_log'] = np.log(df_reg['ViolentCrimesPerPop'] + 0.1)\ndf_reg.loc[:,'medIncome_log'] = np.log(df_reg['medIncome'] + 0.1)\n\n/var/folders/7v/zl9mv52s3ls94kntlt_l9ryh0000gq/T/ipykernel_41199/3488182522.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_reg.loc[:, 'ViolentCrimesPerPop_log'] = np.log(df_reg['ViolentCrimesPerPop'] + 0.1)\n/var/folders/7v/zl9mv52s3ls94kntlt_l9ryh0000gq/T/ipykernel_41199/3488182522.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_reg.loc[:,'medIncome_log'] = np.log(df_reg['medIncome'] + 0.1)\n\n\n\ndf_reg\n\n\n\n\n\n\n\n\ncommunityname\nmedIncome\nViolentCrimesPerPop\nViolentCrimesPerPop_log\nmedIncome_log\n\n\n\n\n0\nLakewoodcity\n0.37\n0.20\n-1.203973\n-0.755023\n\n\n1\nTukwilacity\n0.31\n0.67\n-0.261365\n-0.891598\n\n\n2\nAberdeentown\n0.30\n0.43\n-0.634878\n-0.916291\n\n\n3\nWillingborotownship\n0.58\n0.12\n-1.514128\n-0.385662\n\n\n4\nBethlehemtownship\n0.50\n0.03\n-2.040221\n-0.510826\n\n\n...\n...\n...\n...\n...\n...\n\n\n1989\nTempleTerracecity\n0.42\n0.09\n-1.660731\n-0.653926\n\n\n1990\nSeasidecity\n0.28\n0.45\n-0.597837\n-0.967584\n\n\n1991\nWaterburytown\n0.31\n0.23\n-1.108663\n-0.891598\n\n\n1992\nWalthamcity\n0.44\n0.19\n-1.237874\n-0.616186\n\n\n1993\nOntariocity\n0.40\n0.48\n-0.544727\n-0.693147\n\n\n\n\n1994 rows × 5 columns\n\n\n\n\nimport seaborn as sns\nsns.jointplot(data = df_reg[['medIncome_log', 'ViolentCrimesPerPop_log']], \n              x = 'ViolentCrimesPerPop_log', \n              y = 'medIncome_log', kind='reg',\n              marker = '.')\n\n\n\n\nIs log transforming our variables the right thing to do here?\nFit our regression to the log transformed data.\n\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\n\nx = df_reg[['ViolentCrimesPerPop_log']]\ny = df_reg[['medIncome_log']]\n\nmodel = LinearRegression()\nmodel.fit(x, y)\n\ny_hat = model.predict(x)\nplt.plot(x, y,'o', alpha = 0.5)\nplt.plot(x, y_hat, 'r', alpha = 0.5)\n\nplt.xlabel('Violent Crimes Per Population')\nplt.ylabel('Median Income')\n\nprint (\"MSE:\", metrics.mean_squared_error(y_hat, y))\nprint (\"R^2:\", metrics.r2_score(y, y_hat))\nprint (\"var:\", y.var())\n\nMSE: 0.1531885348757034\nR^2: 0.22763497704356928\nvar: medIncome_log    0.198436\ndtype: float64\n\n\n\n\n\nHas our log transformation distorted the pattern in the data?\n\nx = df_reg[['ViolentCrimesPerPop']]\ny = df_reg[['medIncome']]\n\nmodel = LinearRegression()\nmodel.fit(x, y)\n\ny_hat = model.predict(x)\nplt.plot(x, y,'o', alpha = 0.5)\nplt.plot(x, y_hat, 'r', alpha = 0.5)\n\nplt.xlabel('Violent Crimes Per Population')\nplt.ylabel('Median Income')\n\nprint (\"MSE:\", metrics.mean_squared_error(y_hat, y))\nprint (\"R^2:\", metrics.r2_score(y, y_hat))\nprint (\"var:\", y.var())\n\nMSE: 0.03592636778157073\nR^2: 0.17996313165549482\nvar: medIncome    0.043833\ndtype: float64\n\n\n\n\n\nWhat is the relationship between violent crime and median income? Why might this be?\nAssuming the log data is fine, have we overfit the model? Remember that a good model (which accurately models the relationship between violent crimes per population) need to be robust when faced with new data.\nKfold cross validation splits data into train and test subsets. We can then fit the regression to the training set and see how well it does for the test set.\n\nfrom sklearn.model_selection import KFold\n\nX = df_reg[['ViolentCrimesPerPop']]\ny = df_reg[['medIncome']]\n\n# get four splits, Each split contains a \n# test series and a train series.\nkf = KFold(n_splits=4)\n\n\n# lists to store our statistics\nr_vals = []\nMSEs = []\nmedIncome_coef = []\n\nfor train_index, test_index in kf.split(X):\n    # fit our model and extract statistics\n    model = LinearRegression()\n    model.fit(X.iloc[train_index], y.iloc[train_index])\n    y_hat = model.predict(X.iloc[test_index])\n    \n    MSEs.append(metrics.mean_squared_error(y.iloc[test_index], y_hat))\n    medIncome_coef.append(model.coef_[0][0])\n    r_vals.append(metrics.r2_score(y.iloc[test_index], y_hat))\n\n\ndata = {'MSE' : MSEs, 'medIncome coefficient' : medIncome_coef, 'r squared' : r_vals}\npd.DataFrame(data)\n\n\n\n\n\n\n\n\nMSE\nmedIncome coefficient\nr squared\n\n\n\n\n0\n0.035727\n-0.403609\n0.130479\n\n\n1\n0.035904\n-0.389344\n0.162820\n\n\n2\n0.040777\n-0.353379\n0.200139\n\n\n3\n0.032255\n-0.378883\n0.182403\n\n\n\n\n\n\n\nDoes our model produce similiar coefficients with subsets of the data?\nWe can do this using an inbuild sklearn function (see here).\n\nfrom sklearn.model_selection import cross_val_score\nx = df_reg[['ViolentCrimesPerPop']]\ny = df_reg[['medIncome']]\n\nmodel = LinearRegression()\nmodel.fit(x, y)\n\nprint(cross_val_score(model, x, y, cv=4))\n\n[0.13047946 0.16281953 0.20013867 0.18240261]\n\n\nWhat do these values tell us about our model and data?\nYou might want to carry out multiple regression with more than one predictor variable, or reduce the number of dimensions, or perhaps address different questions using a clustering algorithm instead with all or a subset of features."
  },
  {
    "objectID": "content/labs/Lab_5/IM939_Lab_5_3.html",
    "href": "content/labs/Lab_5/IM939_Lab_5_3.html",
    "title": "17  Lab: Investigating multidimensional scaling",
    "section": "",
    "text": "Here we look at some London Borough data.\n\nimport pandas as pd\n\ndf = pd.read_excel('data/london-borough-profilesV3.xlsx', engine = 'openpyxl')\ndf.columns\n\nIndex(['Code', 'Area/INDICATOR', 'Inner/ Outer London',\n       'GLA Population Estimate 2013', 'GLA Household Estimate 2013',\n       'Inland Area (Hectares)', 'Population density (per hectare) 2013',\n       'Average Age, 2013', 'Proportion of population aged 0-15, 2013',\n       'Proportion of population of working-age, 2013',\n       'Proportion of population aged 65 and over, 2013',\n       '% of resident population born abroad (2013)',\n       'Largest migrant population by country of birth (2013)',\n       '% of largest migrant population (2013)',\n       'Second largest migrant population by country of birth (2013)',\n       '% of second largest migrant population (2013)',\n       'Third largest migrant population by country of birth (2013)',\n       '% of third largest migrant population (2013)',\n       '% of population from BAME groups (2013)',\n       '% people aged 3+ whose main language is not English (2011 census)',\n       'Overseas nationals entering the UK (NINo), (2013/14)',\n       'New migrant (NINo) rates, (2013/14)', 'Employment rate (%) (2013/14)',\n       'Male employment rate (2013/14)', 'Female employment rate (2013/14)',\n       'Unemployment rate (2013/14)', 'Youth Unemployment rate (2013/14)',\n       'Proportion of 16-18 year olds who are NEET (%) (2013)',\n       'Proportion of the working-age population who claim benefits (%) (Feb-2014)',\n       '% working-age with a disability (2012)',\n       'Proportion of working age people with no qualifications (%) 2013',\n       'Proportion of working age people in London with degree or equivalent and above (%) 2013',\n       'Gross Annual Pay, (2013)', 'Gross Annual Pay - Male (2013)',\n       'Gross Annual Pay - Female (2013)',\n       '% adults that volunteered in past 12 months (2010/11 to 2012/13)',\n       'Number of jobs by workplace (2012)',\n       '% of employment that is in public sector (2012)', 'Jobs Density, 2012',\n       'Number of active businesses, 2012',\n       'Two-year business survival rates 2012',\n       'Crime rates per thousand population 2013/14',\n       'Fires per thousand population (2013)',\n       'Ambulance incidents per hundred population (2013)',\n       'Median House Price, 2013',\n       'Average Band D Council Tax charge (£), 2014/15',\n       'New Homes (net) 2012/13', 'Homes Owned outright, (2013) %',\n       'Being bought with mortgage or loan, (2013) %',\n       'Rented from Local Authority or Housing Association, (2013) %',\n       'Rented from Private landlord, (2013) %',\n       '% of area that is Greenspace, 2005', 'Total carbon emissions (2012)',\n       'Household Waste Recycling Rate, 2012/13',\n       'Number of cars, (2011 Census)',\n       'Number of cars per household, (2011 Census)',\n       '% of adults who cycle at least once per month, 2011/12',\n       'Average Public Transport Accessibility score, 2012',\n       'Indices of Multiple Deprivation 2010 Rank of Average Score',\n       'Income Support claimant rate (Feb-14)',\n       '% children living in out-of-work families (2013)',\n       'Achievement of 5 or more A*- C grades at GCSE or equivalent including English and Maths, 2012/13',\n       'Rates of Children Looked After (2013)',\n       '% of pupils whose first language is not English (2014)',\n       'Male life expectancy, (2010-12)', 'Female life expectancy, (2010-12)',\n       'Teenage conception rate (2012)',\n       'Life satisfaction score 2012-13 (out of 10)',\n       'Worthwhileness score 2012-13 (out of 10)',\n       'Happiness score 2012-13 (out of 10)',\n       'Anxiety score 2012-13 (out of 10)', 'Political control in council',\n       'Proportion of seats won by Conservatives in 2014 election',\n       'Proportion of seats won by Labour in 2014 election',\n       'Proportion of seats won by Lib Dems in 2014 election',\n       'Turnout at 2014 local elections'],\n      dtype='object')\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nCode\nArea/INDICATOR\nInner/ Outer London\nGLA Population Estimate 2013\nGLA Household Estimate 2013\nInland Area (Hectares)\nPopulation density (per hectare) 2013\nAverage Age, 2013\nProportion of population aged 0-15, 2013\nProportion of population of working-age, 2013\n...\nTeenage conception rate (2012)\nLife satisfaction score 2012-13 (out of 10)\nWorthwhileness score 2012-13 (out of 10)\nHappiness score 2012-13 (out of 10)\nAnxiety score 2012-13 (out of 10)\nPolitical control in council\nProportion of seats won by Conservatives in 2014 election\nProportion of seats won by Labour in 2014 election\nProportion of seats won by Lib Dems in 2014 election\nTurnout at 2014 local elections\n\n\n\n\n0\nE09000001\nCity of London\nInner London\n8000\n4514.371383\n290.4\n27.525868\n41.303887\n7.948036\n77.541617\n...\n.\n8.10\n8.23\n7.44\nx\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\nE09000002\nBarking and Dagenham\nOuter London\n195600\n73261.408580\n3610.8\n54.160527\n33.228935\n26.072939\n63.835021\n...\n35.4\n7.06\n7.57\n6.97\n3.3\nLab\n0.000000\n100.000000\n0.000000\n38.16\n\n\n2\nE09000003\nBarnet\nOuter London\n370000\n141385.794900\n8674.8\n42.651374\n36.896246\n20.886408\n65.505593\n...\n14.7\n7.35\n7.79\n7.27\n2.63\nCons\n50.793651\n42.857143\n1.587302\n41.1\n\n\n3\nE09000004\nBexley\nOuter London\n236500\n94701.226400\n6058.1\n39.044243\n38.883039\n20.282830\n63.146450\n...\n25.8\n7.47\n7.75\n7.21\n3.22\nCons\n71.428571\n23.809524\n0.000000\nnot avail\n\n\n4\nE09000005\nBrent\nOuter London\n320200\n114318.553900\n4323.3\n74.063670\n35.262694\n20.462585\n68.714872\n...\n19.6\n7.23\n7.32\n7.09\n3.33\nLab\n9.523810\n88.888889\n1.587302\n33\n\n\n\n\n5 rows × 76 columns\n\n\n\nLots of different features. We also have really odd NaN values such as x and not available. We can try and get rid of this.\n\ndef isnumber(x):\n    try:\n        float(x)\n        return True\n    except:\n        if (len(x) &gt; 1) & (\"not avail\" not in x):\n            return True\n        else:\n            return False\n\n# apply isnumber function to every element\ndf = df[df.applymap(isnumber)]\ndf.head()\n\n\n\n\n\n\n\n\nCode\nArea/INDICATOR\nInner/ Outer London\nGLA Population Estimate 2013\nGLA Household Estimate 2013\nInland Area (Hectares)\nPopulation density (per hectare) 2013\nAverage Age, 2013\nProportion of population aged 0-15, 2013\nProportion of population of working-age, 2013\n...\nTeenage conception rate (2012)\nLife satisfaction score 2012-13 (out of 10)\nWorthwhileness score 2012-13 (out of 10)\nHappiness score 2012-13 (out of 10)\nAnxiety score 2012-13 (out of 10)\nPolitical control in council\nProportion of seats won by Conservatives in 2014 election\nProportion of seats won by Labour in 2014 election\nProportion of seats won by Lib Dems in 2014 election\nTurnout at 2014 local elections\n\n\n\n\n0\nE09000001\nCity of London\nInner London\n8000\n4514.371383\n290.4\n27.525868\n41.303887\n7.948036\n77.541617\n...\nNaN\n8.10\n8.23\n7.44\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\nE09000002\nBarking and Dagenham\nOuter London\n195600\n73261.408580\n3610.8\n54.160527\n33.228935\n26.072939\n63.835021\n...\n35.4\n7.06\n7.57\n6.97\n3.3\nLab\n0.000000\n100.000000\n0.000000\n38.16\n\n\n2\nE09000003\nBarnet\nOuter London\n370000\n141385.794900\n8674.8\n42.651374\n36.896246\n20.886408\n65.505593\n...\n14.7\n7.35\n7.79\n7.27\n2.63\nCons\n50.793651\n42.857143\n1.587302\n41.1\n\n\n3\nE09000004\nBexley\nOuter London\n236500\n94701.226400\n6058.1\n39.044243\n38.883039\n20.282830\n63.146450\n...\n25.8\n7.47\n7.75\n7.21\n3.22\nCons\n71.428571\n23.809524\n0.000000\nNaN\n\n\n4\nE09000005\nBrent\nOuter London\n320200\n114318.553900\n4323.3\n74.063670\n35.262694\n20.462585\n68.714872\n...\n19.6\n7.23\n7.32\n7.09\n3.33\nLab\n9.523810\n88.888889\n1.587302\n33\n\n\n\n\n5 rows × 76 columns\n\n\n\nThat looks much cleaner.\nReplace the NaN values in numeric columns with the mean.\n\n# get only numeric columns\nnumericColumns = df._get_numeric_data()\n\n\nfrom sklearn.metrics import euclidean_distances\n\n# keep place names and store them in a variable\nplaceNames = df[\"Area/INDICATOR\"]\n\n# let's fill the missing values with mean()\nnumericColumns = numericColumns.fillna(numericColumns.mean())\n\n# let's centralize the data\nnumericColumns -= numericColumns.mean()\n\n# now we compute the euclidean distances between the columns by passing the same data twice\n# the resulting data matrix now has the pairwise distances between the boroughs.\n# CAUTION: note that we are now building a distance matrix in a high-dimensional data space\n# remember the Curse of Dimensionality -- we need to be cautious with the distance values\ndistMatrix = euclidean_distances(numericColumns, numericColumns)\n\nCheck to make sure everything looks ok.\n\nnumericColumns.head()\n\n\n\n\n\n\n\n\nGLA Population Estimate 2013\nGLA Household Estimate 2013\nInland Area (Hectares)\nPopulation density (per hectare) 2013\nAverage Age, 2013\nProportion of population aged 0-15, 2013\nProportion of population of working-age, 2013\nProportion of population aged 65 and over, 2013\n% of population from BAME groups (2013)\n% people aged 3+ whose main language is not English (2011 census)\n...\nAverage Public Transport Accessibility score, 2012\nIndices of Multiple Deprivation 2010 Rank of Average Score\nIncome Support claimant rate (Feb-14)\nRates of Children Looked After (2013)\nLife satisfaction score 2012-13 (out of 10)\nWorthwhileness score 2012-13 (out of 10)\nHappiness score 2012-13 (out of 10)\nProportion of seats won by Conservatives in 2014 election\nProportion of seats won by Labour in 2014 election\nProportion of seats won by Lib Dems in 2014 election\n\n\n\n\n0\n-247760.606061\n-97761.616805\n-4473.681818\n-43.279630\n5.426932\n-11.500067\n8.480871\n3.019196\n-17.390874\n-4.491385\n...\n3.753658\n157.424242\n-1.726749\n42.212121\n0.816364\n0.651212\n0.23303\n0.000000\n0.000000\n-8.881784e-16\n\n\n1\n-60160.606061\n-29014.579608\n-1153.281818\n-16.644971\n-2.648021\n6.624837\n-5.225725\n-1.399112\n5.764246\n-2.905288\n...\n-0.882730\n-82.575758\n1.787041\n20.212121\n-0.223636\n-0.008788\n-0.23697\n-32.854444\n43.384181\n-6.598065e+00\n\n\n2\n114239.393939\n39109.806712\n3910.718182\n-28.154125\n1.019290\n1.438305\n-3.555153\n2.116847\n-2.799300\n1.775548\n...\n-0.883020\n71.424242\n-0.517827\n-18.787879\n0.066364\n0.211212\n0.06303\n17.939207\n-13.758676\n-5.010764e+00\n\n\n3\n-19260.606061\n-7574.761788\n1294.018182\n-31.761255\n3.006083\n0.834727\n-5.914296\n5.079569\n-20.328016\n-15.598200\n...\n-1.364540\n69.424242\n-0.018377\n-8.787879\n0.186364\n0.171212\n0.00303\n38.574127\n-32.806295\n-6.598065e+00\n\n\n4\n64439.393939\n12042.565712\n-440.781818\n3.258171\n-0.614262\n1.014482\n-0.345874\n-0.668608\n25.000030\n15.521631\n...\n-0.174795\n-69.575758\n0.001370\n-6.787879\n-0.053636\n-0.258788\n-0.11697\n-23.330634\n32.273070\n-5.010764e+00\n\n\n\n\n5 rows × 41 columns\n\n\n\nWe can plot out our many dimension space by uncommenting the code below (also note down how long does this take).\n\n#import seaborn as sns\n#sns_plot = sns.pairplot(numericColumns)\n#sns_plot.savefig(\"figs/output.png\")\n\nGiven that this takes quite a while (around 10 minutes), this is the image that would result from uncommenting and running the code above.\n\nDimension reduction will help us here!\nWe could apply various different types of dimension reduction here. We are specifically going to capture the dissimilarity in the data using multidimensional scaling. Our distance matrix will come in useful here.\n\nfrom sklearn import manifold\n# for instance, typing distMatrix.shape on the console gives:\n# Out[115]: (38, 38) # i.e., the number of rows\n\n# first we generate an MDS object and extract the projections\nmds = manifold.MDS(n_components = 2, max_iter=3000, n_init=1, dissimilarity=\"precomputed\", normalized_stress=False)\nY = mds.fit_transform(distMatrix)\n\nTo interpret what is happening, let us plot the boroughs on the projected two dimensional space.\n\nfrom matplotlib import pyplot as plt\n\nfig, ax = plt.subplots()\nfig.set_size_inches(15, 15)\nplt.suptitle('MDS on only London boroughs')\nax.scatter(Y[:, 0], Y[:, 1], c=\"#D06B36\", s = 100, alpha = 0.8, linewidth=0)\n\nfor i, txt in enumerate(placeNames):\n    ax.annotate(txt, (Y[:, 0][i],Y[:, 1][i]))\n\n\n\n\nOur data also include happiness metrics. Pulling these out of our data and carrying out more multidimensional scaling can help us see how the boroughs differ in happiness.\n\n# get the data columns relating to emotions and feelings\ndataOnEmotions = numericColumns[[\"Life satisfaction score 2012-13 (out of 10)\", \"Worthwhileness score 2012-13 (out of 10)\",\"Happiness score 2012-13 (out of 10)\"]]\n\n# a new distance matrix to represent \"emotional distance\"s\ndistMatrix2 = euclidean_distances(dataOnEmotions, dataOnEmotions)\n\n# compute a new \"embedding\" (machine learners' word for projection)\nY2 = mds.fit_transform(distMatrix2)\n\n# let's look at the results\nfig, ax = plt.subplots()\nfig.set_size_inches(15, 15)\nplt.suptitle('An \\\"emotional\\\" look at London boroughs')\nax.scatter(Y2[:, 0], Y2[:, 1], c=\"#D06B36\", s = 100, alpha = 0.8, linewidth=0)\n\nfor i, txt in enumerate(placeNames):\n    ax.annotate(txt, (Y2[:, 0][i],Y2[:, 1][i]))\n\n\n\n\nThe location of the different boroughs on the 2 dimensional multidimensional scaling space from the happiness metrics is\n\nresults_fixed = Y2.copy()\nprint(results_fixed)\n\n[[ 1.05185977e+00 -1.73525245e-01]\n [-1.51055825e-01 -2.70767296e-01]\n [ 2.13482774e-01  4.42219961e-02]\n [ 2.56574076e-01  1.54624843e-02]\n [-2.99984809e-01 -3.84727002e-03]\n [ 4.13368209e-01  1.73430497e-01]\n [-2.37846279e-01  1.83715618e-04]\n [-2.88527286e-01 -1.25637126e-01]\n [-8.34384427e-02  2.59654937e-01]\n [-5.15251238e-02  2.29795466e-01]\n [-1.58392064e-01 -1.36160840e-01]\n [-2.62642636e-01 -1.79424835e-01]\n [-3.96961608e-02 -2.56013103e-01]\n [-1.75849452e-01 -3.54154668e-02]\n [ 1.27475264e-02  1.76765936e-01]\n [ 1.30190522e-01  7.00861808e-02]\n [ 9.10698687e-02  1.46017034e-01]\n [ 3.22283616e-02  9.37207217e-02]\n [-4.38057604e-01 -3.28751185e-01]\n [ 5.76851786e-01  1.99964051e-01]\n [-1.29063129e-01  3.44116960e-02]\n [-2.76145835e-01 -2.16362939e-01]\n [ 6.95265564e-02 -1.08002809e-01]\n [-9.78308390e-02 -7.14444042e-02]\n [-7.92877906e-02  1.29315091e-01]\n [-1.11428037e-01  1.75561320e-01]\n [ 1.78954880e-01  1.43621723e-01]\n [ 6.94902852e-02 -2.11387697e-02]\n [-4.10177291e-02 -5.76781766e-02]\n [-6.18819726e-03  1.25718361e-01]\n [ 1.06302141e-01 -7.65371952e-04]\n [-4.57399774e-02  8.26689430e-02]\n [-2.28929542e-01 -1.15665317e-01]]\n\n\nWe may want to look at if the general happiness rating captures the position of the boroughs. To do this we need to assign colours based on the binned happiness score.\n\nimport numpy as np\n\ncolorMappingValuesHappiness = np.asarray(dataOnEmotions[[\"Life satisfaction score 2012-13 (out of 10)\"]]).flatten()\nprint(results_fixed.shape)\ncolorMappingValuesHappiness.shape\n\ncolorMappingValuesHappiness\n#c = colorMappingValuesCrime, cmap = plt.cm.Greens\n\n(33, 2)\n\n\narray([ 0.81636364, -0.22363636,  0.06636364,  0.18636364, -0.05363636,\n        0.34636364, -0.06363636, -0.28363636, -0.04363636, -0.10363636,\n       -0.12363636, -0.21363636, -0.05363636, -0.08363636,  0.05636364,\n        0.11636364,  0.06636364,  0.01636364, -0.20363636,  0.39636364,\n        0.00636364, -0.19363636, -0.05363636, -0.10363636, -0.06363636,\n       -0.00363636,  0.13636364, -0.01363636, -0.03363636, -0.00363636,\n       -0.04363636, -0.05363636, -0.19363636])\n\n\nFinally, we can plot this. What can you see?\n\n# let's look at the results\nfig, ax = plt.subplots()\nfig.set_size_inches(15, 15)\nplt.suptitle('An \\\"emotional\\\" look at London boroughs')\n#ax.scatter(results_fixed[:, 0], results_fixed[:, 1], c = colorMappingValuesHappiness, cmap='viridis')\nplt.scatter(results_fixed[:, 0], results_fixed[:, 1], c = colorMappingValuesHappiness, s = 100, cmap=plt.cm.Greens)\n\nfor i, txt in enumerate(placeNames):\n    ax.annotate(txt, (results_fixed[:, 0][i],results_fixed[:, 1][i]))\n\n\n\n\n\n# get the data columns relating to emotions and feelings\ndataOnDiversity = numericColumns[[\"Proportion of population aged 0-15, 2013\", \"Proportion of population of working-age, 2013\", \"Proportion of population aged 65 and over, 2013\", \"% of population from BAME groups (2013)\", \"% people aged 3+ whose main language is not English (2011 census)\"]]\n\n# a new distance matrix to represent \"emotional distance\"s\ndistMatrix3 = euclidean_distances(dataOnDiversity, dataOnDiversity)\n\nmds = manifold.MDS(n_components = 2, max_iter=3000, n_init=1, dissimilarity=\"precomputed\", normalized_stress = False)\nY = mds.fit_transform(distMatrix3)\n\n# Visualising the data.\nfig, ax = plt.subplots()\nfig.set_size_inches(15, 15)\nplt.suptitle('An \\\"diversity\\\" look at London boroughs')\nax.scatter(Y[:, 0], Y[:, 1], s = 100, c = colorMappingValuesHappiness, cmap=plt.cm.Greens)\n\nfor i, txt in enumerate(placeNames):\n    ax.annotate(txt, (Y[:, 0][i],Y[:, 1][i]))\n\n\n\n\nThis looks very different to the one above on “emotion” related variables. Our job now is to relate these two projections to one another. Do you see similarities? Do you see clusters of boroughs? Can you reflect on how you can relate and combine these two maps conceptually?\n\n17.0.1 A small TODO for you:\nQ: Can you think of other maps that you can produce with this data? Have a look at the variables once again and try to produce new “perspectives” to the data and see what they have to say."
  },
  {
    "objectID": "content/labs/Lab_5/IM939_lab_5_Exercise.html",
    "href": "content/labs/Lab_5/IM939_lab_5_Exercise.html",
    "title": "18  IM939 lab 5 - Exercise",
    "section": "",
    "text": "The exercise this week is a chance to apply the code you have been delving into over these past week.\nChoose one of the datasets from a previous week:\n\nCrime Census\nLondon Borough\nWine\nIris\nGlobal warming\n\nOr, if you are feeling confident, another dataset from the sklearn datasets.\nnote Please do refer to the sklean and pandas documentation if you get stuck.\nRead in the data using pandas.\nLook at the first few rows. Get a feel for the structure of the data.\nDeal with missing values, if any.\nCreate a summary of the data. Plot any particular features or groups of features which you think are of interest.\nSettle on a possible question you want to answer. What might you be able to learn from your dataset?\nDecide on your initial analysis. Remember, we have covered:\n\nLinear regressions\nDimension reduction\nClustering\n\nWhich method will best allow you to tackle your question?\nApply your chosen analysis method below. Please do refer to and copy and paste code from previous weeks.\nCan you visualise your result?\nYou may want to use a:\n\nScatterplot\nHistogram\nAny other plot, such as those in the seaborn example library.\n\nAre you able to check if your method is robust (e.g., kfold test of regressions or cluster stability checks)? Perhaps do that below.\nHmm, what have you learned?\nYou may want to consider if you could convince a friend of your conclusion. Perhaps another type of analysis is needed or there are issues with the analysis you chose above!\nUse the space below to explore a bit more."
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_1-illusions.html#clustering-illusion",
    "href": "content/labs/Lab_6/IM939_Lab_6_1-illusions.html#clustering-illusion",
    "title": "19  Lab: Illusions",
    "section": "19.1 Clustering illusion",
    "text": "19.1 Clustering illusion\nYou can find details of the clustering illusion here, here, and here.\nThe illusion suggests you should automatically try and see clusters in random data.\n\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nimport altair as alt\n\nn = 700\n\nd = {'x': np.random.uniform(0, 100, n), 'y': np.random.uniform(0, 100, n)}\ndf = pd.DataFrame(d)\n\nsns.relplot(data = df, x = 'x', y = 'y')\n\n\n\n\nThe syntax for building this type in Altair is pretty straight forward.\n\nalt.Chart(df).mark_circle(size=5).encode(\n    x='x',\n    y='y')\n\n\n\n\n\n\n\nTo remove those pesky lines we need to specify we want an X and Y axis without grid lines.\n\nalt.Chart(df).mark_circle(size=5).encode(\n    alt.X('x', axis=alt.Axis(grid=False)),\n    alt.Y('y', axis=alt.Axis(grid=False)))\n\n\n\n\n\n\n\nWe will do a lot of altering axis and colors in altair. We do this by specifying alt.axistype and then passing various options.\nDo you see any clustering the in the above plots? What about if we give names to the different columns?\n\nalt.Chart(df).mark_circle(size=5).encode(\n    alt.X('x', axis=alt.Axis(grid=False, title='Height')),\n    alt.Y('y', axis=alt.Axis(grid=False, title='Weight')))\n\n\n\n\n\n\n\nAnother example of the clustering illusion is the idea of ‘streaks’. That we see a pattern from a small sample and extrapolate out.\nWhat do you expect the next dice role to be?\n\nn_rolls = 10\nd = {'round': np.linspace(1,n_rolls,n_rolls), 'roll': np.random.randint(1,6,n_rolls)}\ndf_dice = pd.DataFrame(d)\ndf_dice\n\n\n\n\n\n\n\n\nround\nroll\n\n\n\n\n0\n1.0\n2\n\n\n1\n2.0\n2\n\n\n2\n3.0\n4\n\n\n3\n4.0\n3\n\n\n4\n5.0\n1\n\n\n5\n6.0\n4\n\n\n6\n7.0\n4\n\n\n7\n8.0\n1\n\n\n8\n9.0\n3\n\n\n9\n10.0\n2\n\n\n\n\n\n\n\n\nsns.scatterplot(data=df_dice, x='round', y='roll')\n\n&lt;Axes: xlabel='round', ylabel='roll'&gt;\n\n\n\n\n\n\nalt.Chart(df_dice).mark_circle(size=20).encode(\n    alt.X('round', axis=alt.Axis(grid=False)),\n    alt.Y('roll', axis=alt.Axis(grid=False)))\n\n\n\n\n\n\n\nEach number on the dice will occur the same number of times. Any patterns you see are due to extrapolating based on a small sample. We can check that though by rolling the ‘dice’ 1,000,000 times.\n\nn_rolls = 1000000\nd = {'round': np.linspace(1,n_rolls,n_rolls), 'roll': np.random.randint(1,6,n_rolls)}\ndf_dice_many = pd.DataFrame(d)\n\ndf_dice_many.groupby('roll').count()\n\n\n\n\n\n\n\n\nround\n\n\nroll\n\n\n\n\n\n1\n200582\n\n\n2\n199877\n\n\n3\n199726\n\n\n4\n199580\n\n\n5\n200235"
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_1-illusions.html#weber-fechner-law",
    "href": "content/labs/Lab_6/IM939_Lab_6_1-illusions.html#weber-fechner-law",
    "title": "19  Lab: Illusions",
    "section": "19.2 Weber-Fechner Law",
    "text": "19.2 Weber-Fechner Law\n\n‘The Weber-Fechner Law is a famous finding of early psychophysics indicating that differences between stimuli are detected on a logarithmic scale. It takes more additional millimeters of radius to discern two larger circles than two smaller circles. This type of bias is probably one of the most researched biases in visualization research.’\n– (Calero Valdez, Ziefle, and Sedlmair 2018)\n\nLet us see if we can create a plot to demonstrate it.\nWe will load in the car crashes dataset from seaborn. Documentation of the data is here.\n\ndf_crashes = sns.load_dataset('car_crashes')\ndf_crashes.head()\n\n\n\n\n\n\n\n\ntotal\nspeeding\nalcohol\nnot_distracted\nno_previous\nins_premium\nins_losses\nabbrev\n\n\n\n\n0\n18.8\n7.332\n5.640\n18.048\n15.040\n784.55\n145.08\nAL\n\n\n1\n18.1\n7.421\n4.525\n16.290\n17.014\n1053.48\n133.93\nAK\n\n\n2\n18.6\n6.510\n5.208\n15.624\n17.856\n899.47\n110.35\nAZ\n\n\n3\n22.4\n4.032\n5.824\n21.056\n21.280\n827.34\n142.39\nAR\n\n\n4\n12.0\n4.200\n3.360\n10.920\n10.680\n878.41\n165.63\nCA\n\n\n\n\n\n\n\nTo illustrate this ‘illusion’ we will plot the percentage of drivers speeding, percentage of alcohol impaired and set the size of the point equal to the percentage of drivers not previously involves in any accident. Each point is an american state.\nAre there any relationships or patterns in the data?\n\nsns.scatterplot(data=df_crashes, x='speeding', y='alcohol', size='no_previous')\n\n&lt;Axes: xlabel='speeding', ylabel='alcohol'&gt;\n\n\n\n\n\nIs it easier to distinguish the different sizes in the below plot?\n\nsns.scatterplot(data=df_crashes,\n                x='speeding',\n                y='alcohol',\n                size='no_previous',\n                sizes=(10,40))\n\n&lt;Axes: xlabel='speeding', ylabel='alcohol'&gt;\n\n\n\n\n\nHow about this one?\n\nsns.scatterplot(data=df_crashes,\n                x='speeding',\n                y='alcohol',\n                size='no_previous',\n                sizes=(40,70))\n\n&lt;Axes: xlabel='speeding', ylabel='alcohol'&gt;\n\n\n\n\n\nThe values are the same. We have just changed the range of sizes.\nWe can do much the same in altair.\n\nalt.Chart(df_crashes).mark_circle().encode(\n    x='speeding',\n    y='alcohol',\n    size='no_previous'\n)\n\n\n\n\n\n\n\n\nalt.Chart(df_crashes).mark_circle().encode(\n    x='speeding',\n    y='alcohol',\n    size = alt.Size('no_previous', scale=alt.Scale(range=[10,40]))\n)\n\n\n\n\n\n\n\n\nalt.Chart(df_crashes).mark_circle().encode(\n    x='speeding',\n    y='alcohol',\n    size = alt.Size('no_previous', scale=alt.Scale(range=[40,70]))\n)\n\n\n\n\n\n\n\nHave you come across any other illusions? If so, try and plot them out. I sometimes find it easier to understand these things through creating simple illustrations of my own.\n\n\n\n\nCalero Valdez, André, Martina Ziefle, and Michael Sedlmair. 2018. “Studying Biases in Visualization Research: Framework and Methods.” In Cognitive Biases in Visualizations, edited by Geoffrey Ellis, 13–27. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-95831-6_2."
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_2-AxisManipulation.html#data-wrangling",
    "href": "content/labs/Lab_6/IM939_Lab_6_2-AxisManipulation.html#data-wrangling",
    "title": "20  Lab: Axes manipulation",
    "section": "20.1 Data wrangling",
    "text": "20.1 Data wrangling\nWe are going to use polls from the recent USA presidential election. As before, we load and examine the data.\n\nimport pandas as pd \nimport seaborn as sns\nimport altair as alt \n\ndf_polls = pd.read_csv('data/presidential_poll_averages_2020.csv')\ndf_polls.head()\n\n\n\n\n\n\n\n\ncycle\nstate\nmodeldate\ncandidate_name\npct_estimate\npct_trend_adjusted\n\n\n\n\n0\n2020\nWyoming\n11/3/2020\nJoseph R. Biden Jr.\n30.81486\n30.82599\n\n\n1\n2020\nWisconsin\n11/3/2020\nJoseph R. Biden Jr.\n52.12642\n52.09584\n\n\n2\n2020\nWest Virginia\n11/3/2020\nJoseph R. Biden Jr.\n33.49125\n33.51517\n\n\n3\n2020\nWashington\n11/3/2020\nJoseph R. Biden Jr.\n59.34201\n59.39408\n\n\n4\n2020\nVirginia\n11/3/2020\nJoseph R. Biden Jr.\n53.74120\n53.72101\n\n\n\n\n\n\n\nFor our analysis, we are going to pick estimates from 11/3/2020 for the swing states of Florida, Texas, Arizona, Michigan, Minnesota and Pennsylvania.\n\ndf_nov = df_polls[\n    (df_polls.modeldate == '11/3/2020')\n]\n\ndf_nov = df_nov[\n    (df_nov.candidate_name == 'Joseph R. Biden Jr.') |\n    (df_nov.candidate_name == 'Donald Trump')\n]\n\ndf_swing = df_nov[\n    (df_nov['state'] == 'Florida') |\n    (df_nov['state'] == 'Texas' ) |\n    (df_nov['state'] == 'Arizona' ) |\n    (df_nov['state'] == 'Michigan' ) |\n    (df_nov['state'] == 'Minnesota' ) |\n    (df_nov['state'] == 'Pennsylvania' ) \n]\n\ndf_swing\n\n\n\n\n\n\n\n\ncycle\nstate\nmodeldate\ncandidate_name\npct_estimate\npct_trend_adjusted\n\n\n\n\n7\n2020\nTexas\n11/3/2020\nJoseph R. Biden Jr.\n47.46643\n47.44781\n\n\n12\n2020\nPennsylvania\n11/3/2020\nJoseph R. Biden Jr.\n50.22000\n50.20422\n\n\n30\n2020\nMinnesota\n11/3/2020\nJoseph R. Biden Jr.\n51.86992\n51.84517\n\n\n31\n2020\nMichigan\n11/3/2020\nJoseph R. Biden Jr.\n51.17806\n51.15482\n\n\n46\n2020\nFlorida\n11/3/2020\nJoseph R. Biden Jr.\n49.09162\n49.08035\n\n\n53\n2020\nArizona\n11/3/2020\nJoseph R. Biden Jr.\n48.72237\n48.70539\n\n\n63\n2020\nTexas\n11/3/2020\nDonald Trump\n48.57118\n48.58794\n\n\n68\n2020\nPennsylvania\n11/3/2020\nDonald Trump\n45.57216\n45.55034\n\n\n86\n2020\nMinnesota\n11/3/2020\nDonald Trump\n42.63638\n42.66826\n\n\n87\n2020\nMichigan\n11/3/2020\nDonald Trump\n43.20577\n43.23326\n\n\n102\n2020\nFlorida\n11/3/2020\nDonald Trump\n46.68101\n46.61909\n\n\n109\n2020\nArizona\n11/3/2020\nDonald Trump\n46.11074\n46.10181"
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_2-AxisManipulation.html#default-barplot",
    "href": "content/labs/Lab_6/IM939_Lab_6_2-AxisManipulation.html#default-barplot",
    "title": "20  Lab: Axes manipulation",
    "section": "20.2 Default barplot",
    "text": "20.2 Default barplot\nWe can look at the relative performance of the candidates within each state using a nested bar plot.\n\nax = sns.barplot(\n    data = df_swing, \n    x = 'state', \n    y = 'pct_estimate', \n    hue = 'candidate_name')"
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_2-AxisManipulation.html#altering-the-axes",
    "href": "content/labs/Lab_6/IM939_Lab_6_2-AxisManipulation.html#altering-the-axes",
    "title": "20  Lab: Axes manipulation",
    "section": "20.3 Altering the axes",
    "text": "20.3 Altering the axes\nAltering the axis increases the distance between the bars. Some might say that is misleading.\n\nax = sns.barplot(\n    data = df_swing, \n    x = 'state', \n    y = 'pct_estimate', \n    hue = 'candidate_name')\n\nax.set(ylim=(41, 52))\n\n[(41.0, 52.0)]\n\n\n\n\n\nWhat do you think?\nHow about if we instead put the data on the full 0 to 100 scale?\n\nax = sns.barplot(\n    data = df_swing, \n    x = 'state', \n    y = 'pct_estimate', \n    hue = 'candidate_name')\n\nax.set(ylim=(0, 100))\n\n[(0.0, 100.0)]\n\n\n\n\n\nWe can do the same thing in Altair.\n\nalt.Chart(df_swing).mark_bar().encode(\n    x='candidate_name',\n    y='pct_estimate',\n    color='candidate_name',\n    column = alt.Column('state:O', spacing = 5, header = alt.Header(labelOrient = \"bottom\")),\n)\n\n\n\n\n\n\n\nNote the need for the alt column. What happens if you do not provide an alt column?\nPassing the domain option to the scale of the Y axis allows us to choose the y axis range.\n\nalt.Chart(df_swing).mark_bar().encode(\n    x='candidate_name',\n    y=alt.Y('pct_estimate', scale=alt.Scale(domain=[42,53])),\n    color='candidate_name',\n    column = alt.Column('state:O', spacing = 5, header = alt.Header(labelOrient = \"bottom\")),\n)"
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_2-AxisManipulation.html#altering-the-proportions",
    "href": "content/labs/Lab_6/IM939_Lab_6_2-AxisManipulation.html#altering-the-proportions",
    "title": "20  Lab: Axes manipulation",
    "section": "20.4 Altering the proportions",
    "text": "20.4 Altering the proportions\nWe can even be a bit tricky and stretch out the difference.\n\nalt.Chart(df_swing).mark_bar().encode(\n    x='candidate_name',\n    y=alt.Y('pct_estimate', scale=alt.Scale(domain=[42,53])),\n    color='candidate_name',\n    column = alt.Column('state:O', spacing = 5, header = alt.Header(labelOrient = \"bottom\")),\n).properties(\n    width=20,\n    height=600\n)"
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_2-AxisManipulation.html#default-line-plot",
    "href": "content/labs/Lab_6/IM939_Lab_6_2-AxisManipulation.html#default-line-plot",
    "title": "20  Lab: Axes manipulation",
    "section": "20.5 Default line plot",
    "text": "20.5 Default line plot\nIt is not just bar plot that you can have fun with. Line plots are another interesting example.\nFor our simple line plot, we will need the poll data for a single state.\n\ndf_texas = df_polls[\n    df_polls['state'] == 'Texas'\n]\n\ndf_texas_bt = df_texas[\n    (df_texas['candidate_name'] == 'Donald Trump') |\n    (df_texas['candidate_name'] == 'Joseph R. Biden Jr.')\n]\n\ndf_texas_bt.head()\n\n\n\n\n\n\n\n\ncycle\nstate\nmodeldate\ncandidate_name\npct_estimate\npct_trend_adjusted\n\n\n\n\n7\n2020\nTexas\n11/3/2020\nJoseph R. Biden Jr.\n47.46643\n47.44781\n\n\n63\n2020\nTexas\n11/3/2020\nDonald Trump\n48.57118\n48.58794\n\n\n231\n2020\nTexas\n11/2/2020\nJoseph R. Biden Jr.\n47.46643\n47.44781\n\n\n287\n2020\nTexas\n11/2/2020\nDonald Trump\n48.57118\n48.58794\n\n\n455\n2020\nTexas\n11/1/2020\nJoseph R. Biden Jr.\n47.45590\n47.43400\n\n\n\n\n\n\n\nThe modeldate column is a string (object) and not date time. So we need to change that: we will create a new datetime column called modeldate.\n\n#df_texas_bt.loc[df_texas_bt[]]\n\n\nprint('Before\\n')\nprint(df_texas_bt.dtypes)\ndf_texas_bt['date'] = pd.to_datetime(df_texas_bt.loc[:,'modeldate'], format='%m/%d/%Y').copy()\nprint('\\nAfter\\n')\nprint(df_texas_bt.dtypes)\n\nBefore\n\ncycle                   int64\nstate                  object\nmodeldate              object\ncandidate_name         object\npct_estimate          float64\npct_trend_adjusted    float64\ndtype: object\n\nAfter\n\ncycle                          int64\nstate                         object\nmodeldate                     object\ncandidate_name                object\npct_estimate                 float64\npct_trend_adjusted           float64\ndate                  datetime64[ns]\ndtype: object\n\n\n/var/folders/7v/zl9mv52s3ls94kntlt_l9ryh0000gq/T/ipykernel_41367/3228356118.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_texas_bt['date'] = pd.to_datetime(df_texas_bt.loc[:,'modeldate'], format='%m/%d/%Y').copy()\n\n\nCreate our line plot.\n\nalt.Chart(df_texas_bt).mark_line().encode(\n    y=alt.Y('pct_estimate', scale=alt.Scale(domain=[42,53])),\n    x='date',\n    color='candidate_name')\n\n\n\n\n\n\n\nSometimes multiple axis are used for each line, or in a combined line and bar plot.\nThe example here uses a dataframe with a column for each line. Our data does not have that.\n\ndf_texas_bt\nour_df = df_texas_bt[['candidate_name', 'pct_estimate', 'date']]\nour_df\n\n\n\n\n\n\n\n\ncandidate_name\npct_estimate\ndate\n\n\n\n\n7\nJoseph R. Biden Jr.\n47.46643\n2020-11-03\n\n\n63\nDonald Trump\n48.57118\n2020-11-03\n\n\n231\nJoseph R. Biden Jr.\n47.46643\n2020-11-02\n\n\n287\nDonald Trump\n48.57118\n2020-11-02\n\n\n455\nJoseph R. Biden Jr.\n47.45590\n2020-11-01\n\n\n...\n...\n...\n...\n\n\n28931\nDonald Trump\n49.09724\n2020-02-29\n\n\n28963\nJoseph R. Biden Jr.\n45.30901\n2020-02-28\n\n\n28995\nDonald Trump\n49.09676\n2020-02-28\n\n\n29027\nJoseph R. Biden Jr.\n45.30089\n2020-02-27\n\n\n29058\nDonald Trump\n49.07925\n2020-02-27\n\n\n\n\n502 rows × 3 columns\n\n\n\nPivot table allows us to reshape our dataframe.\n\nour_df = pd.pivot_table(our_df, index=['date'], columns = 'candidate_name')\nour_df.columns = our_df.columns.to_series().str.join('_')\nour_df.head()\n\n\n\n\n\n\n\n\npct_estimate_Donald Trump\npct_estimate_Joseph R. Biden Jr.\n\n\ndate\n\n\n\n\n\n\n2020-02-27\n49.07925\n45.30089\n\n\n2020-02-28\n49.09676\n45.30901\n\n\n2020-02-29\n49.09724\n45.30896\n\n\n2020-03-01\n49.09724\n45.30895\n\n\n2020-03-02\n48.91861\n45.37694\n\n\n\n\n\n\n\nDate here is the dataframe index. We want it to be a column.\n\nour_df['date1'] = our_df.index\nour_df.columns = ['Trump', 'Biden', 'date1']\nour_df.head()\n\n\n\n\n\n\n\n\nTrump\nBiden\ndate1\n\n\ndate\n\n\n\n\n\n\n\n2020-02-27\n49.07925\n45.30089\n2020-02-27\n\n\n2020-02-28\n49.09676\n45.30901\n2020-02-28\n\n\n2020-02-29\n49.09724\n45.30896\n2020-02-29\n\n\n2020-03-01\n49.09724\n45.30895\n2020-03-01\n\n\n2020-03-02\n48.91861\n45.37694\n2020-03-02\n\n\n\n\n\n\n\nCreating our new plot, to fool all those people who expect Trump to win in Texas.\n\nbase = alt.Chart(our_df).encode(\n        alt.X('date1')\n)\n\nline_A = base.mark_line(color='#5276A7').encode(\n    alt.Y('Trump', axis=alt.Axis(titleColor='#5276A7'), scale=alt.Scale(domain=[42,53]))\n)\n\nline_B = base.mark_line(color='#F18727').encode(\n    alt.Y('Biden', axis=alt.Axis(titleColor='#F18727'), scale=alt.Scale(domain=[35,53]))\n)\n\nalt.layer(line_A, line_B).resolve_scale(y='independent')\n\n\n\n\n\n\n\nDid you see what I did there?\nOf course, mixed axis plots are rarely purely line plots. Instead they can be mixes of different axis. For these and other plotting mistakes, the economist has a nice article here. You may want to try some of these plots with this data set or the world indicators dataset from a few weeks ago."
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_3-Choropleths.html#data-preparations",
    "href": "content/labs/Lab_6/IM939_Lab_6_3-Choropleths.html#data-preparations",
    "title": "21  Lab: Choropleth Maps",
    "section": "21.1 Data preparations",
    "text": "21.1 Data preparations\nLoad in two datasets. One (geo_states) contains the geospatial polygons of the states in America, but does not contain any data about USA elections; and the other (df_polls) is the polling data we used in the last notebook, but does not have any geospatial polygons.\n\nimport geopandas as gpd \nimport pandas as pd\nimport altair as alt\n\ngeo_states = gpd.read_file('data/gz_2010_us_040_00_500k.json')\ndf_polls = pd.read_csv('data/presidential_poll_averages_2020.csv')\n\nLet’s explore the data first:\n\ngeo_states.head()\n\n\n\n\n\n\n\n\nGEO_ID\nSTATE\nNAME\nLSAD\nCENSUSAREA\ngeometry\n\n\n\n\n0\n0400000US23\n23\nMaine\n\n30842.923\nMULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n\n\n1\n0400000US25\n25\nMassachusetts\n\n7800.058\nMULTIPOLYGON (((-70.83204 41.60650, -70.82373 ...\n\n\n2\n0400000US26\n26\nMichigan\n\n56538.901\nMULTIPOLYGON (((-88.68443 48.11579, -88.67563 ...\n\n\n3\n0400000US30\n30\nMontana\n\n145545.801\nPOLYGON ((-104.05770 44.99743, -104.25015 44.9...\n\n\n4\n0400000US32\n32\nNevada\n\n109781.180\nPOLYGON ((-114.05060 37.00040, -114.04999 36.9...\n\n\n\n\n\n\n\nThis seems like a regular data frame, but there’s a feature that stands out from the others: geometry. This feature contains the coordinates thar define the polygons (or multipolygons) for every region in the map, in this case, every State in the USA. This is also an indicator that we are not using a regular dataframe, but a particular type of dataframe called GeoDataFrame:\n\ntype(geo_states)\n\ngeopandas.geodataframe.GeoDataFrame\n\n\nBecause this is a geospatial dataframe, we can visualise it as a map. In this case, we are going to use Altair to create a map using the AlbersUsa projection.\n\nalt.Chart(geo_states, title='US states').mark_geoshape().encode(\n).properties(\n    width=500,\n    height=300\n).project(\n    type='albersUsa'\n)\n\n\n\n\n\n\n\nAnd now the polls’ result:\n\ndf_polls\n\n\n\n\n\n\n\n\ncycle\nstate\nmodeldate\ncandidate_name\npct_estimate\npct_trend_adjusted\n\n\n\n\n0\n2020\nWyoming\n11/3/2020\nJoseph R. Biden Jr.\n30.81486\n30.82599\n\n\n1\n2020\nWisconsin\n11/3/2020\nJoseph R. Biden Jr.\n52.12642\n52.09584\n\n\n2\n2020\nWest Virginia\n11/3/2020\nJoseph R. Biden Jr.\n33.49125\n33.51517\n\n\n3\n2020\nWashington\n11/3/2020\nJoseph R. Biden Jr.\n59.34201\n59.39408\n\n\n4\n2020\nVirginia\n11/3/2020\nJoseph R. Biden Jr.\n53.74120\n53.72101\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n29080\n2020\nConnecticut\n2/27/2020\nDonald Trump\n33.66370\n34.58325\n\n\n29081\n2020\nColorado\n2/27/2020\nDonald Trump\n44.27899\n44.07662\n\n\n29082\n2020\nCalifornia\n2/27/2020\nDonald Trump\n34.66504\n34.69761\n\n\n29083\n2020\nArizona\n2/27/2020\nDonald Trump\n47.79450\n48.07208\n\n\n29084\n2020\nAlabama\n2/27/2020\nDonald Trump\n59.15000\n59.14228\n\n\n\n\n29085 rows × 6 columns\n\n\n\nAs you can see, modeldate has different dates. Let’s double check that:\n\ndf_polls.modeldate.unique()\n\narray(['11/3/2020', '11/2/2020', '11/1/2020', '10/31/2020', '10/30/2020',\n       '10/29/2020', '10/28/2020', '10/27/2020', '10/26/2020',\n       '10/25/2020', '10/24/2020', '10/23/2020', '10/22/2020',\n       '10/21/2020', '10/20/2020', '10/19/2020', '10/18/2020',\n       '10/17/2020', '10/16/2020', '10/15/2020', '10/14/2020',\n       '10/13/2020', '10/12/2020', '10/11/2020', '10/10/2020',\n       '10/9/2020', '10/8/2020', '10/7/2020', '10/6/2020', '10/5/2020',\n       '10/4/2020', '10/3/2020', '10/2/2020', '10/1/2020', '9/30/2020',\n       '9/29/2020', '9/28/2020', '9/27/2020', '9/26/2020', '9/25/2020',\n       '9/24/2020', '9/23/2020', '9/22/2020', '9/21/2020', '9/20/2020',\n       '9/19/2020', '9/18/2020', '9/17/2020', '9/16/2020', '9/15/2020',\n       '9/14/2020', '9/13/2020', '9/12/2020', '9/11/2020', '9/10/2020',\n       '9/9/2020', '9/8/2020', '9/7/2020', '9/6/2020', '9/5/2020',\n       '9/4/2020', '9/3/2020', '9/2/2020', '9/1/2020', '8/31/2020',\n       '8/30/2020', '8/29/2020', '8/28/2020', '8/27/2020', '8/26/2020',\n       '8/25/2020', '8/24/2020', '8/23/2020', '8/22/2020', '8/21/2020',\n       '8/20/2020', '8/19/2020', '8/18/2020', '8/17/2020', '8/16/2020',\n       '8/15/2020', '8/14/2020', '8/13/2020', '8/12/2020', '8/11/2020',\n       '8/10/2020', '8/9/2020', '8/8/2020', '8/7/2020', '8/6/2020',\n       '8/5/2020', '8/4/2020', '8/3/2020', '8/2/2020', '8/1/2020',\n       '7/31/2020', '7/30/2020', '7/29/2020', '7/28/2020', '7/27/2020',\n       '7/26/2020', '7/25/2020', '7/24/2020', '7/23/2020', '7/22/2020',\n       '7/21/2020', '7/20/2020', '7/19/2020', '7/18/2020', '7/17/2020',\n       '7/16/2020', '7/15/2020', '7/14/2020', '7/13/2020', '7/12/2020',\n       '7/11/2020', '7/10/2020', '7/9/2020', '7/8/2020', '7/7/2020',\n       '7/6/2020', '7/5/2020', '7/4/2020', '7/3/2020', '7/2/2020',\n       '7/1/2020', '6/30/2020', '6/29/2020', '6/28/2020', '6/27/2020',\n       '6/26/2020', '6/25/2020', '6/24/2020', '6/23/2020', '6/22/2020',\n       '6/21/2020', '6/20/2020', '6/19/2020', '6/18/2020', '6/17/2020',\n       '6/16/2020', '6/15/2020', '6/14/2020', '6/13/2020', '6/12/2020',\n       '6/11/2020', '6/10/2020', '6/9/2020', '6/8/2020', '6/7/2020',\n       '6/6/2020', '6/5/2020', '6/4/2020', '6/3/2020', '6/2/2020',\n       '6/1/2020', '5/31/2020', '5/30/2020', '5/29/2020', '5/28/2020',\n       '5/27/2020', '5/26/2020', '5/25/2020', '5/24/2020', '5/23/2020',\n       '5/22/2020', '5/21/2020', '5/20/2020', '5/19/2020', '5/18/2020',\n       '5/17/2020', '5/16/2020', '5/15/2020', '5/14/2020', '5/13/2020',\n       '5/12/2020', '5/11/2020', '5/10/2020', '5/9/2020', '5/8/2020',\n       '5/7/2020', '5/6/2020', '5/5/2020', '5/4/2020', '5/3/2020',\n       '5/2/2020', '5/1/2020', '4/30/2020', '4/29/2020', '4/28/2020',\n       '4/27/2020', '4/26/2020', '4/25/2020', '4/24/2020', '4/23/2020',\n       '4/22/2020', '4/21/2020', '4/20/2020', '4/19/2020', '4/18/2020',\n       '4/17/2020', '4/16/2020', '4/15/2020', '4/14/2020', '4/13/2020',\n       '4/12/2020', '4/11/2020', '4/10/2020', '4/9/2020', '4/8/2020',\n       '4/7/2020', '4/6/2020', '4/5/2020', '4/4/2020', '4/3/2020',\n       '4/2/2020', '4/1/2020', '3/31/2020', '3/30/2020', '3/29/2020',\n       '3/28/2020', '3/27/2020', '3/26/2020', '3/25/2020', '3/24/2020',\n       '3/23/2020', '3/22/2020', '3/21/2020', '3/20/2020', '3/19/2020',\n       '3/18/2020', '3/17/2020', '3/16/2020', '3/15/2020', '3/14/2020',\n       '3/13/2020', '3/12/2020', '3/11/2020', '3/10/2020', '3/9/2020',\n       '3/8/2020', '3/7/2020', '3/6/2020', '3/5/2020', '3/4/2020',\n       '3/3/2020', '3/2/2020', '3/1/2020', '2/29/2020', '2/28/2020',\n       '2/27/2020'], dtype=object)\n\n\n\n21.1.1 Filtering\nThat means, that we will need to filter our poll data to a specific date, in this case 11/2/2020\n\ndf_nov = df_polls[\n    (df_polls.modeldate == '11/3/2020')\n]\n\ndf_nov_states = df_nov[\n    (df_nov.candidate_name == 'Donald Trump') |\n    (df_nov.candidate_name == 'Joseph R. Biden Jr.')\n]\n\ndf_nov_states\n\n\n\n\n\n\n\n\ncycle\nstate\nmodeldate\ncandidate_name\npct_estimate\npct_trend_adjusted\n\n\n\n\n0\n2020\nWyoming\n11/3/2020\nJoseph R. Biden Jr.\n30.81486\n30.82599\n\n\n1\n2020\nWisconsin\n11/3/2020\nJoseph R. Biden Jr.\n52.12642\n52.09584\n\n\n2\n2020\nWest Virginia\n11/3/2020\nJoseph R. Biden Jr.\n33.49125\n33.51517\n\n\n3\n2020\nWashington\n11/3/2020\nJoseph R. Biden Jr.\n59.34201\n59.39408\n\n\n4\n2020\nVirginia\n11/3/2020\nJoseph R. Biden Jr.\n53.74120\n53.72101\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n107\n2020\nCalifornia\n11/3/2020\nDonald Trump\n32.28521\n32.43615\n\n\n108\n2020\nArkansas\n11/3/2020\nDonald Trump\n58.39097\n58.94886\n\n\n109\n2020\nArizona\n11/3/2020\nDonald Trump\n46.11074\n46.10181\n\n\n110\n2020\nAlaska\n11/3/2020\nDonald Trump\n50.99835\n51.23236\n\n\n111\n2020\nAlabama\n11/3/2020\nDonald Trump\n57.36153\n57.36126\n\n\n\n\n112 rows × 6 columns\n\n\n\n\n\n21.1.2 Computing percentages\nWe want to put the percentage estimates for each candidate onto the map. First, let us create a dataframe containing the data for each candidate.\n\n# Create seperate date frame for trump and biden\ntrump_data = df_nov_states[\n    df_nov_states.candidate_name == 'Donald Trump'\n]\n\nbiden_data = df_nov_states[\n    df_nov_states.candidate_name == 'Joseph R. Biden Jr.'\n]\n\n\n\n21.1.3 Joining data\nAs we have seen before, we have two datasets that partially address our needs: geo_states contains the geospatial polygons of the states in America, but lacks data about USA elections; df_polls contains data about USA elections but lacks geometry.\nWe will need to combine both (joining) to create a (geospatial)dataframe that contains geometry AND polling data so we can create a choropleth map capable of answering our question: who is winning the elections?\nTo do so, we need to join both dataframes using a common feature. Our spatial and poll data have the name of the state in common, but their columns have different names.\nOption A: We could rename the column so it is the same in all cases and then merge (see commented code below)\n\n# Uncomment below to see the effect. This produces an almost identical geodataframe to code cell below, but more verbose. (Can you spot the difference?)\n\n# Rename column names.\n# trump_data.columns = ['cycle', 'NAME', 'modeldate', 'candidate_name', 'pct_estimate', 'pct_trend_adjusted']\n# biden_data.columns = ['cycle', 'NAME', 'modeldate', 'candidate_name', 'pct_estimate', 'pct_trend_adjusted']\n\n# We can join the geospatial and poll data using the NAME column (the name of the state).\n# geo_states_trump = geo_states.merge(trump_data, on = 'NAME')\n# geo_states_biden = geo_states.merge(biden_data, left_on = 'NAME', right_on = 'state')\n\nOption B: We can join the geospatial and poll data using different column names by using left_on for the left data (usually the geodataframe) and right_on for the right dataframe. We will be using this method, as it doesn’t require to rename columns.\n\n# Add the poll data (divided in two data frames) to a single geospatial dataframe.\ngeo_states_trump = geo_states.merge(\n    trump_data, left_on = 'NAME', right_on = 'state')\n\ngeo_states_biden = geo_states.merge(\n    biden_data, left_on = 'NAME', right_on = 'state')\n\n\ngeo_states_trump.head()\n\n\n\n\n\n\n\n\nGEO_ID\nSTATE\nNAME\nLSAD\nCENSUSAREA\ngeometry\ncycle\nstate\nmodeldate\ncandidate_name\npct_estimate\npct_trend_adjusted\n\n\n\n\n0\n0400000US23\n23\nMaine\n\n30842.923\nMULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n2020\nMaine\n11/3/2020\nDonald Trump\n40.34410\n40.31588\n\n\n1\n0400000US25\n25\nMassachusetts\n\n7800.058\nMULTIPOLYGON (((-70.83204 41.60650, -70.82373 ...\n2020\nMassachusetts\n11/3/2020\nDonald Trump\n28.56164\n28.86275\n\n\n2\n0400000US26\n26\nMichigan\n\n56538.901\nMULTIPOLYGON (((-88.68443 48.11579, -88.67563 ...\n2020\nMichigan\n11/3/2020\nDonald Trump\n43.20577\n43.23326\n\n\n3\n0400000US30\n30\nMontana\n\n145545.801\nPOLYGON ((-104.05770 44.99743, -104.25015 44.9...\n2020\nMontana\n11/3/2020\nDonald Trump\n49.74744\n49.78661\n\n\n4\n0400000US32\n32\nNevada\n\n109781.180\nPOLYGON ((-114.05060 37.00040, -114.04999 36.9...\n2020\nNevada\n11/3/2020\nDonald Trump\n44.32982\n44.36094\n\n\n\n\n\n\n\n\ngeo_states_biden.head()\n\n\n\n\n\n\n\n\nGEO_ID\nSTATE\nNAME\nLSAD\nCENSUSAREA\ngeometry\ncycle\nstate\nmodeldate\ncandidate_name\npct_estimate\npct_trend_adjusted\n\n\n\n\n0\n0400000US23\n23\nMaine\n\n30842.923\nMULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n2020\nMaine\n11/3/2020\nJoseph R. Biden Jr.\n53.31518\n53.32106\n\n\n1\n0400000US25\n25\nMassachusetts\n\n7800.058\nMULTIPOLYGON (((-70.83204 41.60650, -70.82373 ...\n2020\nMassachusetts\n11/3/2020\nJoseph R. Biden Jr.\n64.36328\n64.62505\n\n\n2\n0400000US26\n26\nMichigan\n\n56538.901\nMULTIPOLYGON (((-88.68443 48.11579, -88.67563 ...\n2020\nMichigan\n11/3/2020\nJoseph R. Biden Jr.\n51.17806\n51.15482\n\n\n3\n0400000US30\n30\nMontana\n\n145545.801\nPOLYGON ((-104.05770 44.99743, -104.25015 44.9...\n2020\nMontana\n11/3/2020\nJoseph R. Biden Jr.\n45.34418\n45.36695\n\n\n4\n0400000US32\n32\nNevada\n\n109781.180\nPOLYGON ((-114.05060 37.00040, -114.04999 36.9...\n2020\nNevada\n11/3/2020\nJoseph R. Biden Jr.\n49.62386\n49.65657\n\n\n\n\n\n\n\nJoe Biden is clearly winning. Can we make it look like he is not?"
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_3-Choropleths.html#data-visualisation",
    "href": "content/labs/Lab_6/IM939_Lab_6_3-Choropleths.html#data-visualisation",
    "title": "21  Lab: Choropleth Maps",
    "section": "21.2 Data visualisation",
    "text": "21.2 Data visualisation\nWe can plot this specifying the feature to use for our colour.\n\nalt.Chart(geo_states_trump, title='Poll estimate for Donald Trump on 11/3/2020').mark_geoshape().encode(\n    color='pct_estimate',\n    tooltip=['NAME', 'pct_estimate']\n).properties(\n    width=500,\n    height=300\n).project(\n    type='albersUsa'\n)\n\n\n\n\n\n\n\n\n21.2.1 Binning\nTo smooth out any differences we can bin our data.\n\nalt.Chart(geo_states_trump, title='Poll estimate for Donald Trump on 11/3/2020').mark_geoshape().encode(\n    alt.Color('pct_estimate', bin=alt.Bin(step=35)),\n    tooltip=['NAME', 'pct_estimate']\n).properties(\n    width=500,\n    height=300\n).project(\n    type='albersUsa'\n)\n\n\n\n\n\n\n\nHow would you interpret the plot above?\nWhat about if we increase the binstep so we have more bins?\n\nalt.Chart(geo_states_trump, title='Poll estimate for Donald Trump on 11/3/2020').mark_geoshape().encode(\n    alt.Color('pct_estimate', bin=alt.Bin(step=5)),\n    tooltip=['NAME', 'pct_estimate']\n).properties(\n    width=500,\n    height=300\n).project(\n    type='albersUsa'\n)\n\n\n\n\n\n\n\nPerhaps try different step sizes for the bins and consider how bins can shape our interpretation of the data. What would happen if plots with different bin sizes were placed side to side.\nTo add further confusion, what happens when we log scale the data?\n\nalt.Chart(geo_states_trump, title='Poll estimate for Donald Trump on 11/3/2020').mark_geoshape().encode(\n    alt.Color('pct_estimate', bin=alt.Bin(step=5), scale=alt.Scale(type='log')),\n    tooltip=['NAME', 'pct_estimate']\n).properties(\n    width=500,\n    height=300\n).project(\n    type='albersUsa'\n)\n\n\n\n\n\n\n\nvs\n\nalt.Chart(geo_states_biden, title='Poll estimate for Joe Biden on 11/3/2020').mark_geoshape().encode(\n    alt.Color('pct_estimate', bin=alt.Bin(step=5), scale=alt.Scale(type='log')),\n    tooltip=['NAME', 'pct_estimate']\n).properties(\n    width=500,\n    height=300\n).project(\n    type='albersUsa'\n)\n\n\n\n\n\n\n\nWhat is happening here?!?!\n\n\n21.2.2 Colour palettes\nNext up, what about the colours we use and the range of values assigned to each color? Code inspired by/taken from here.\n\nalt.Chart(geo_states_trump, title='Poll estimate for Donal Trump on 11/3/2020').mark_geoshape().encode(\n    alt.Color('pct_estimate',\n    scale=alt.Scale(type=\"linear\",\n              domain=[10, 40, 50, 55, 60, 61, 62],\n                          range=[\"#414487\",\"#414487\",\n                                 \"#355f8d\",\"#355f8d\",\n                                 \"#2a788e\",\n                                 \"#fde725\",\"#fde725\"])),\n    tooltip=['NAME', 'pct_estimate']\n).properties(\n    width=500,\n    height=300\n).project(\n    type='albersUsa'\n)\n\n\n\n\n\n\n\nCompare that with\n\nalt.Chart(geo_states_trump, title='Poll estimate for Donald Trump on 11/3/2020').mark_geoshape().encode(\n    alt.Color('pct_estimate',\n    scale=alt.Scale(type=\"linear\",\n              domain=[10, 20, 30, 35, 68, 70, 100],\n                          range=[\"#414487\",\"#414487\",\n                                 \"#7ad151\",\"#7ad151\",\n                                 \"#bddf26\",\n                                 \"#fde725\",\"#fde725\"])),\n    tooltip=['NAME', 'pct_estimate']\n).properties(\n    width=500,\n    height=300\n).project(\n    type='albersUsa'\n)\n\n\n\n\n\n\n\n\n\n21.2.3 Legends\nMy goodness! So what have we played around with?\n\nTransforming our scale using log\nBinning our data to smooth out variances\nAltering our colour scheme and the ranges for each colour\n\n… what about if we remove the legend?\n\nalt.Chart(geo_states_trump, title='Poll estimate for Donald Trump on 11/3/2020').mark_geoshape().encode(\n    alt.Color('pct_estimate',\n    scale=alt.Scale(type=\"linear\",\n              domain=[10, 20, 30, 35, 68, 70, 100],\n                          range=[\"#414487\",\"#414487\",\n                                 \"#7ad151\",\"#7ad151\",\n                                 \"#bddf26\",\n                                 \"#fde725\",\"#fde725\"]),\n                                 legend=None),\n    tooltip=['NAME', 'pct_estimate']\n).properties(\n    width=500,\n    height=300\n).project(\n    type='albersUsa'\n)\n\n\n\n\n\n\n\nGood luck trying to interpret that. Though we often see maps without legends and with questionable colour schemes on TV.\nHow do you think choropleths should be displayed? What information does a use need to understand the message communicated in these plots?"
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_4-Exercises.html#setup",
    "href": "content/labs/Lab_6/IM939_Lab_6_4-Exercises.html#setup",
    "title": "22  Exercise: Data visualisation",
    "section": "22.1 Setup",
    "text": "22.1 Setup\nLoad any libraries and datasets needed for your visualisation."
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_4-Exercises.html#data-wrangling",
    "href": "content/labs/Lab_6/IM939_Lab_6_4-Exercises.html#data-wrangling",
    "title": "22  Exercise: Data visualisation",
    "section": "22.2 Data wrangling",
    "text": "22.2 Data wrangling\nSubset or clean your data"
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_4-Exercises.html#analysis",
    "href": "content/labs/Lab_6/IM939_Lab_6_4-Exercises.html#analysis",
    "title": "22  Exercise: Data visualisation",
    "section": "22.3 Analysis",
    "text": "22.3 Analysis\nCarry out an analysis if required. E.g., are you running a PCA or other dimension reduction, or a linear regression to plot?"
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_4-Exercises.html#visualisation",
    "href": "content/labs/Lab_6/IM939_Lab_6_4-Exercises.html#visualisation",
    "title": "22  Exercise: Data visualisation",
    "section": "22.4 Visualisation",
    "text": "22.4 Visualisation\nCreate a visualisation which clearly show the trend you would like to show in your data."
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_4-Exercises.html#discussion",
    "href": "content/labs/Lab_6/IM939_Lab_6_4-Exercises.html#discussion",
    "title": "22  Exercise: Data visualisation",
    "section": "22.5 Discussion",
    "text": "22.5 Discussion\nWhy did you choose this visualisation? Do you think other will clearly see the trend you have identified?\nCreate a visualisation of this trend which you think will mislead the user?\nHow do you think this will mislead the user?"
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_5-Simpsons_Paradox.html",
    "href": "content/labs/Lab_6/IM939_Lab_6_5-Simpsons_Paradox.html",
    "title": "23  IM939 - Lab 6 Part 5 Simpson’s Paradox",
    "section": "",
    "text": "24 5 In-depth Analysis - Outliers\nLet’s try to go a bit more in-depth . We know that gender doesn’t show many differences. In age there are not big differences except for one case. Let’s focus on ethnicity then.\nWe are going to use Seaborn’s ‘catplot’. In the documentation we can read what are the error bars here: “In seaborn, the barplot() function operates on a full dataset and applies a function to obtain the estimate (taking the mean by default). When there are multiple observations in each category, it also uses bootstrapping to compute a confidence interval around the estimate, which is plotted using error bars.”\nsns.catplot(x=\"Ethnicity\", y='Expenditures', \n            kind=\"bar\", data=df)\n\n#you can also run a nested table, but the chart might be more straightforward in analysis.\n#np.round(df.pivot_table(index=['cat_AgeCohort','Ethnicity'], values=['Expenditures']), 2)\nnp.round(df.pivot_table(index=['Ethnicity'], values=['Expenditures']), 2)\n\n\n\n\n\n\n\n\nExpenditures\n\n\nEthnicity\n\n\n\n\n\nAmerican Indian\n36438.25\n\n\nAsian\n18392.37\n\n\nBlack\n20884.59\n\n\nHispanic\n11065.57\n\n\nMulti Race\n4456.73\n\n\nNative Hawaiian\n42782.33\n\n\nOther\n3316.50\n\n\nWhite not Hispanic\n24697.55\nSo there are big differences in the averages between ethnicities. Does it mean there is discrimination?\ndf.groupby('Ethnicity').count()\n\n\n\n\n\n\n\n\nId\nAgeCohort\nAge\nGender\nExpenditures\ncat_AgeCohort\n\n\nEthnicity\n\n\n\n\n\n\n\n\n\n\nAmerican Indian\n4\n4\n4\n4\n4\n2\n\n\nAsian\n129\n129\n129\n129\n129\n108\n\n\nBlack\n59\n59\n59\n59\n59\n49\n\n\nHispanic\n376\n376\n376\n376\n376\n315\n\n\nMulti Race\n26\n26\n26\n26\n26\n19\n\n\nNative Hawaiian\n3\n3\n3\n3\n3\n2\n\n\nOther\n2\n2\n2\n2\n2\n2\n\n\nWhite not Hispanic\n401\n401\n401\n401\n401\n315\nAs you can see there are big sample size differences between ethnic groups.\nWhat conclusions does it bring? There are 3 major ethnicities within the dataset: White non-Hispanic (40%), Hispanic (38%), Asian (13%). The sample sizes of other ethnicites are very small.\nPlease also remember that 1). We know it is representative data of the population of residents. So based on this data we can use inferential statistics (look up Week 03 slides if you need a reminder) and estimate results for the whole population of beneficiaries of California DDS.\n2). Also, if you look into actual demographics of California State here\nYou will notce that the proportions of the state are similar to proportions of this case study. Hispanic and White non-Hispanic constitute a majority of California’s population.\nLet’s focus on the top 2 biggest groups. We can see there is a difference in the average expenditures between the White non-Hispanic and Hispanic groups.\n##selecting cases that are either 'Hispanic' or 'White non Hispanic' \nHispanic = df[(df[\"Ethnicity\"] == 'Hispanic') | (df[\"Ethnicity\"] == 'White not Hispanic')]\nHispanic\n\n\n\n\n\n\n\n\nId\nAgeCohort\nAge\nGender\nExpenditures\nEthnicity\ncat_AgeCohort\n\n\n\n\n0\n10210\n13-17\n17\nFemale\n2113\nWhite not Hispanic\n13-17\n\n\n1\n10409\n22-50\n37\nMale\n41924\nWhite not Hispanic\n22-50\n\n\n2\n10486\n0-5\n3\nMale\n1454\nHispanic\nNaN\n\n\n3\n10538\n18-21\n19\nFemale\n6400\nHispanic\n18-21\n\n\n4\n10568\n13-17\n13\nMale\n4412\nWhite not Hispanic\n13-17\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n992\n99114\n18-21\n18\nMale\n5298\nHispanic\n18-21\n\n\n995\n99622\n51 +\n86\nFemale\n57055\nWhite not Hispanic\nNaN\n\n\n996\n99715\n18-21\n20\nMale\n7494\nHispanic\n18-21\n\n\n998\n99791\n6-12\n10\nMale\n3638\nHispanic\n6-12\n\n\n999\n99898\n22-50\n23\nMale\n26702\nWhite not Hispanic\n22-50\n\n\n\n\n777 rows × 7 columns\nnp.round(Hispanic.pivot_table(index=['Ethnicity', 'cat_AgeCohort'], values=['Expenditures']), 2)\n\n\n\n\n\n\n\n\n\nExpenditures\n\n\nEthnicity\ncat_AgeCohort\n\n\n\n\n\nHispanic\n6-12\n2312.19\n\n\n13-17\n3955.28\n\n\n18-21\n9959.85\n\n\n22-50\n40924.12\n\n\nWhite not Hispanic\n6-12\n2052.26\n\n\n13-17\n3904.36\n\n\n18-21\n10133.06\n\n\n22-50\n40187.62\nsns.catplot(x=\"cat_AgeCohort\", y='Expenditures', hue=\"Ethnicity\", kind=\"bar\", data=Hispanic)\nLet’s get back to our original question : does discrimination exist in this case?\n“Is the typical Hispanic consumer receiving fewer funds (i.e., expenditures) than the typical White non-Hispanic consumer? If a Hispanic consumer was to file for discrimination based upon ethnicity, s/he would more than likely be asked his/her age. Since the typical amount of expenditures for Hispanics (in all but one age cohort) is higher than the typical amount of expenditures for White non-Hispanics in the respective age cohort, the discrimination claim would be refuted”.\nThis case study shows Simpson’s Paradox. You may ask: “Why is the overall average for all consumers significantly different indicating ethnic discrimination of Hispanics, yet in all but one age cohort (18-21) the average of expenditures for Hispanic consumers are greater than those of the White non-Hispanic population?” Look at the table below.\npd.crosstab([Hispanic.cat_AgeCohort],Hispanic.Ethnicity)\n\n\n\n\n\n\n\nEthnicity\nHispanic\nWhite not Hispanic\n\n\ncat_AgeCohort\n\n\n\n\n\n\n6-12\n91\n46\n\n\n13-17\n103\n67\n\n\n18-21\n78\n69\n\n\n22-50\n43\n133\nResults\n“There are more Hispanics in the youngest four age cohorts, while the White non-Hispanics have more consumers in the oldest two age cohorts. The two populations are close in overall counts (376 vs. 401). On top of this, consumers expenditures increase as they age to see the paradox.\nExpenditure average for Hispanic consumers are higher in all but one of the age cohorts, but the trend reverses when the groups are combined resulting in a lower expenditure average for all Hispanic consumers when compared to all White non-Hispanics.”\n“The overall Hispanic consumer population is a relatively younger when compared to the White non-Hispanic consumer population. Since the expenditures for younger consumers is lower, the overall average of expenditures for Hispanics (vs White non-Hispanics) is less.”\npd.crosstab(Hispanic.cat_AgeCohort,Hispanic.Ethnicity, \n            normalize='columns')\n\n# values=Hispanic.Ethnicity,aggfunc=sum,\n\n\n\n\n\n\n\nEthnicity\nHispanic\nWhite not Hispanic\n\n\ncat_AgeCohort\n\n\n\n\n\n\n6-12\n0.288889\n0.146032\n\n\n13-17\n0.326984\n0.212698\n\n\n18-21\n0.247619\n0.219048\n\n\n22-50\n0.136508\n0.422222"
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_5-Simpsons_Paradox.html#data",
    "href": "content/labs/Lab_6/IM939_Lab_6_5-Simpsons_Paradox.html#data",
    "title": "23  IM939 - Lab 6 Part 5 Simpson’s Paradox",
    "section": "23.1 1 Data",
    "text": "23.1 1 Data\nHere are the key information from the dataset documentation file (every time I use “” below it is a cite from the dataset file). ### Abstract: “The State of California Department of Developmental Services (DDS) is responsible for allocating funds that support over 250,000 developmentally-disabled residents (e.g., intellectual disability, cerebral palsy, autism, etc.), called here also consumers. The dataset represents a sample of 1,000 of these consumers. Biographical characteristics and expenditure data (i.e., the dollar amount the State spends on each consumer in supporting these individuals and their families) are included in the data set for each consumer.\n\n23.1.1 Source:\nThe data set originates from DDS’s “Client Master File.” In order to remain in compliance with California State Legislation, the data have been altered to protect the rights and privacy of specific individual consumers. The data set is designed to represent a sample of 1,000 DDS consumers.\n\n\n23.1.2 Variable Descriptions:\nA header line contains the name of the variables. There are no missing values.\nId: 5-digit, unique identification code for each consumer (similar to a social security number)\nAge Cohort: Binned age variable represented as six age cohorts (0-5, 6-12, 13-17, 18-21, 22-50, and 51+)\nAge: Unbinned age variable\nGender: Male or Female\nExpenditures: Dollar amount of annual expenditures spent on each consumer\nEthnicity: Eight ethnic groups (American Indian, Asian, Black, Hispanic, Multi-race, Native Hawaiian, Other, and White non-Hispanic).\n\n\n23.1.3 Research problem\nThe data set and case study are based on a real-life scenario where there was a claim of discrimination based on ethnicity. The exercise highlights the importance of performing rigorous statistical analysis and how data interpretations can accurately inform or misguide decision makers.” (Taylor, Mickel 2014)"
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_5-Simpsons_Paradox.html#reading-the-dataset",
    "href": "content/labs/Lab_6/IM939_Lab_6_5-Simpsons_Paradox.html#reading-the-dataset",
    "title": "23  IM939 - Lab 6 Part 5 Simpson’s Paradox",
    "section": "23.2 2 Reading the dataset",
    "text": "23.2 2 Reading the dataset\nYou should know the Pandas library already from the lab 1 with James. Here we are going to use it to explore the data and for pivot tables. In the folder you downloaded from the Moodle you have a dataset called ‘Lab 6 - Paradox Dataset’.\n\nimport pandas as pd\ndf = pd.read_excel('data/Paradox_Dataset.xlsx')\n\nA reminder: anything with a pd. prefix comes from pandas. This is particulary useful for preventing a module from overwriting inbuilt Python functionality.\nLet’s have a look at our dataset\n\ndf\n\n\n\n\n\n\n\n\nId\nAgeCohort\nAge\nGender\nExpenditures\nEthnicity\n\n\n\n\n0\n10210\n13-17\n17\nFemale\n2113\nWhite not Hispanic\n\n\n1\n10409\n22-50\n37\nMale\n41924\nWhite not Hispanic\n\n\n2\n10486\n0-5\n3\nMale\n1454\nHispanic\n\n\n3\n10538\n18-21\n19\nFemale\n6400\nHispanic\n\n\n4\n10568\n13-17\n13\nMale\n4412\nWhite not Hispanic\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n995\n99622\n51 +\n86\nFemale\n57055\nWhite not Hispanic\n\n\n996\n99715\n18-21\n20\nMale\n7494\nHispanic\n\n\n997\n99718\n13-17\n17\nFemale\n3673\nMulti Race\n\n\n998\n99791\n6-12\n10\nMale\n3638\nHispanic\n\n\n999\n99898\n22-50\n23\nMale\n26702\nWhite not Hispanic\n\n\n\n\n1000 rows × 6 columns\n\n\n\nWe have 6 columns (variables) in 1000 rows. Let’s see what type of object is our dataset and what types of objects are in the dataset.\n\ntype(df)\n\npandas.core.frame.DataFrame"
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_5-Simpsons_Paradox.html#exploring-data",
    "href": "content/labs/Lab_6/IM939_Lab_6_5-Simpsons_Paradox.html#exploring-data",
    "title": "23  IM939 - Lab 6 Part 5 Simpson’s Paradox",
    "section": "23.3 3 Exploring data",
    "text": "23.3 3 Exploring data\n\n23.3.1 Missing values\nLet’s check if we have any missing data\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 6 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n 0   Id            1000 non-null   int64 \n 1   AgeCohort     1000 non-null   object\n 2   Age           1000 non-null   int64 \n 3   Gender        1000 non-null   object\n 4   Expenditures  1000 non-null   int64 \n 5   Ethnicity     1000 non-null   object\ndtypes: int64(3), object(3)\nmemory usage: 47.0+ KB\n\n\nThe above tables shows that we have 1000 observations for each of 6 columns.\nLet’s see if there are any unexpected values.\n\nimport numpy as np\nnp.unique(df.AgeCohort)\n\narray([' 0-5', ' 51 +', '13-17', '18-21', '22-50', '6-12'], dtype=object)\n\n\n\nnp.unique(df.Age)\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n       34, 35, 36, 37, 38, 39, 40, 41, 42, 44, 45, 46, 48, 51, 52, 53, 54,\n       55, 56, 57, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73,\n       74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 88, 89, 90, 91, 94,\n       95])\n\n\n\nnp.unique(df.Gender)\n\narray(['Female', 'Male'], dtype=object)\n\n\n\nnp.unique(df.Ethnicity)\n\narray(['American Indian', 'Asian', 'Black', 'Hispanic', 'Multi Race',\n       'Native Hawaiian', 'Other', 'White not Hispanic'], dtype=object)\n\n\nThere aren’t any unexpected values in neither of these 4 variables. We didn’t run this command for Expenditures on purpose, as this would return us too many values. An easier way to check this variable would be just a boxplot.\n\ndf.boxplot(column=['Age'])\n\n&lt;Axes: &gt;\n\n\n\n\n\n\ndf.boxplot(column=['Expenditures'])\n\n&lt;Axes: &gt;\n\n\n\n\n\nLet’s see a summary of data types we have here.\n\n\n23.3.2 Data types\n\ndf.dtypes\n\nId               int64\nAgeCohort       object\nAge              int64\nGender          object\nExpenditures     int64\nEthnicity       object\ndtype: object\n\n\nWe are creating a new categorical column cat_AgeCohort that would make our work a bit easier later. You can read more here\n\ndf['cat_AgeCohort'] = pd.Categorical(df['AgeCohort'], \n                                     ordered=True, \n                                     categories=['0-5', '6-12', '13-17', '18-21', '22-50', '51 +'])\n\nHere int64 mean ‘a 64-bit integer’ and ‘object’ are strings. This gives you also a hint they are different types of variables. The ‘bit’ refers to how long and precise the number is. Pandas uses data types from numpy (pandas documentation, numpy documentation). In our dataset three variables are numeric: Id, age are ordinal variables, Expenditures is a scale variable. AgeCohort is categorical and Gender and Ethnicity are nominal.\nFor that reason ‘data.describe’ will bring us a summary of numeric variables only.\n\ndf.describe()\n\n\n\n\n\n\n\n\nId\nAge\nExpenditures\n\n\n\n\ncount\n1000.000000\n1000.000000\n1000.000000\n\n\nmean\n54662.846000\n22.800000\n18065.786000\n\n\nstd\n25643.673401\n18.462038\n19542.830884\n\n\nmin\n10210.000000\n0.000000\n222.000000\n\n\n25%\n31808.750000\n12.000000\n2898.750000\n\n\n50%\n55384.500000\n18.000000\n7026.000000\n\n\n75%\n76134.750000\n26.000000\n37712.750000\n\n\nmax\n99898.000000\n95.000000\n75098.000000\n\n\n\n\n\n\n\nIt doesn’t make sense to plot not numeric variables or ids. That’s why we are going to just plot age and expenditures.\n\ndf.plot(x = 'Age', y = 'Expenditures', kind='scatter')\n\n&lt;Axes: xlabel='Age', ylabel='Expenditures'&gt;\n\n\n\n\n\nThe pattern of data is very interesting, expecially around x-values of ca. 25. The research paper can bring us more clarification.\n\n\n23.3.3 Age\nThe crucial factor in this case study is age: “As consumers get older, their financial needs increase as they move out of their parent’s home, etc. Therefore, it is expected that expenditures for older consumers will be higher than for the younger consumers”\nIn the dataset we have two age variables that both refer to the same information - age of consumers. They are saved as two distinct data types: binned ‘AgeCohort’ and unbinned ‘Age’.\nAge categories If you look at the binned one you will notice that the categories are somewhat interesting:\n\ndf[['Age']].plot(kind='hist', ec='black')\n\n&lt;Axes: ylabel='Frequency'&gt;\n\n\n\n\n\n\ndf[\"AgeCohort\"].describe() #we will receive the output for the categorical variable \"AgeCohort\"\ndf['cat_AgeCohort'].describe()\n\ncount       812\nunique        4\ntop       22-50\nfreq        226\nName: cat_AgeCohort, dtype: object\n\n\nHere we will run a bar plot of age categories.\n\ndf['cat_AgeCohort'].value_counts().plot(kind=\"bar\")\n\n&lt;Axes: xlabel='cat_AgeCohort'&gt;\n\n\n\n\n\nThe default order of plot elements is ‘value count’. For the age variable it might be more useful to look at the order chronologically.\n\n#using sns.countplot from seaborn we will plot AgeCohort\n#the order in plotting this variable is really crucial, we want to have sorted by age categories\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#here is without sorting / ordering\n#sns.countplot(x=\"AgeCohort\", data=df)\n\n#here we plot the variable with sorting\nsns.countplot(x=\"cat_AgeCohort\", data=df)\n\n#You can try playing with the commands below too:\n#sns.countplot(x=\"AgeCohort\", data=df, order=df['AgeCohort'].value_counts().index)\n#sns.countplot(x=\"AgeCohort\", data=df, order=['0-5', '6-12', '13-17', '18-21', '22-50','51+'])\n\n&lt;Axes: xlabel='cat_AgeCohort', ylabel='count'&gt;\n\n\n\n\n\nWhy would the data be binned in such “uneven” categories like ‘0-5 years’, ‘6-12’ and ‘22-50’? Instead of even categories e.g. ‘0-10’, ‘11-20’, ‘21-30’ etc. or every 5 years ‘0-5’, ‘6-10’ etc.?\nHere the age cohorts were allocated based on the theory, rather than based on data (this way we would have even number of people in each category) or based on logical age categories, e.g. every 5 or 10 years.\nThe authors explain: “The cohorts are established based on the amount of financial support typically required during a particular life phase (…) The 0-5 cohort (preschool age) has the fewest needs and requires the least amount of funding (…) Those in the 51+ cohort have the most needs and require the most amount of funding”. You can read in more details in the paper."
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_5-Simpsons_Paradox.html#exploratory-analysis",
    "href": "content/labs/Lab_6/IM939_Lab_6_5-Simpsons_Paradox.html#exploratory-analysis",
    "title": "23  IM939 - Lab 6 Part 5 Simpson’s Paradox",
    "section": "23.4 4 Exploratory analysis",
    "text": "23.4 4 Exploratory analysis\nThe research question is: are any demographics discriminated in distributions of the funds?\nFollowing the authors: “Discrimination exists in this case study if the amount of expenditures for a typical person in a group of consumers that share a common attribute (e.g., gender, ethnicity, etc.) is significantly different when compared to a typical person in another group. For example, discrimination based on gender would occur if the expenditures for a typical female are less than the amount for a typical male.”\nWe are going to examine the data using plots for categorical data and pivot tables (cross-tables) with means. “Pivot table reports are particularly useful in narrowing down larger data sets or analyzing relationships between data points.” Pivot tables will help you understand what is Simpson’s Paradox.\n\n23.4.1 Age x expenditures\nLet’s see how expenditures are distributed across age groups.\nWe are going to use a swarm plot which I believe works well here to notice the paradox and “the points are adjusted (only along the categorical axis) so that they don’t overlap. This gives a better representation of the distribution of values, but it does not scale well to large numbers of observations. A swarm plot can be drawn on its own, but it is also a good complement to a box or violin plot in cases where you want to show all observations along with some representation of the underlying distribution.” Read more here\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.catplot(x=\"AgeCohort\", y=\"Expenditures\", kind=\"swarm\", data=df)\n#you can also do a boxplot if you change kind=\"box\"\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 83.5% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 35.4% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 76.8% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 58.8% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 14.2% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 84.0% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 85.8% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 42.5% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 80.5% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 64.8% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 21.7% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 86.9% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n\n\n\n\n\n23.4.2 Ethnicity\nEthnicity could be another discriminating factor. Let’s check this here too by plotting expenditures by ethnicity.\nThese groups reflect the demographic profile of the State of California.\n\nsns.catplot(x=\"Ethnicity\", y=\"Expenditures\", kind=\"swarm\", data=df)\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 50.1% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 64.6% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 10.2% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 11.5% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 25.6% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 58.6% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 69.4% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 18.6% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 23.1% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 34.9% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n\n\n\n\n\n23.4.3 Gender\nGender could have been another discriminating factor (as gender based discrimination is also very common). It is not the case here. See below plots to confirm these. We are plotting expenditures by gender.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#sns.catplot(x=\"Gender\", y=\"Expenditures\", kind=\"swarm\", data=df)\n#you can create even a nicer plot than for ethnicity, using tips here https://seaborn.pydata.org/tutorial/categorical.html\n#It's a combination of swarmplot and violin plot to show each observation along with a summary of the distribution\n\ng = sns.catplot(x=\"Gender\", y=\"Expenditures\", kind=\"violin\", inner=None, data=df)\nsns.swarmplot(x=\"Gender\", y=\"Expenditures\", color=\"k\", size=3, data=df, ax=g.ax)\n\n&lt;Axes: xlabel='Gender', ylabel='Expenditures'&gt;\n\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 7.0% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n\n\n\n\n\n23.4.4 Mean Expenditures\nThis was a quick visual analysis. Let’s check means to see how it looks like by age, ethnicity and gender. Why would it be also good to check medians here?\n\nimport pandas as pd\nimport numpy as np\n\n#By default the aggreggate function is mean\n\nnp.round(df.pivot_table(index=['Ethnicity'], values=['Expenditures']), 2)\n\n\n\n\n\n\n\n\nExpenditures\n\n\nEthnicity\n\n\n\n\n\nAmerican Indian\n36438.25\n\n\nAsian\n18392.37\n\n\nBlack\n20884.59\n\n\nHispanic\n11065.57\n\n\nMulti Race\n4456.73\n\n\nNative Hawaiian\n42782.33\n\n\nOther\n3316.50\n\n\nWhite not Hispanic\n24697.55\n\n\n\n\n\n\n\n\nnp.round(df.pivot_table(index=['Gender'], values=['Expenditures']), 2)\n\n\n\n\n\n\n\n\nExpenditures\n\n\nGender\n\n\n\n\n\nFemale\n18129.61\n\n\nMale\n18001.20\n\n\n\n\n\n\n\n\nnp.round(df.pivot_table(index=['cat_AgeCohort'], values=['Expenditures']), 2)\n\n\n\n\n\n\n\n\nExpenditures\n\n\ncat_AgeCohort\n\n\n\n\n\n6-12\n2226.86\n\n\n13-17\n3922.61\n\n\n18-21\n9888.54\n\n\n22-50\n40209.28\n\n\n\n\n\n\n\nWhat do these tables tell us? There is much discrepnacy in average results for ethnicity and age cohort. If we look at gender - there aren’t many differences.\nPlease remember that in this case study “the needs for consumers increase as they become older which results in higher expenditures”. This would explain age discrepancies a bit, but what about ethnicity?"
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_5-Simpsons_Paradox.html#explanation",
    "href": "content/labs/Lab_6/IM939_Lab_6_5-Simpsons_Paradox.html#explanation",
    "title": "23  IM939 - Lab 6 Part 5 Simpson’s Paradox",
    "section": "25.1 Explanation",
    "text": "25.1 Explanation\n“This exercise is based on a real-life case in California. The situation involved an alleged case of discrimination privileging White non-Hispanics over Hispanics in the allocation of funds to over 250,000 developmentally-disabled California residents.\nA number of years ago, an allegation of discrimination was made and supported by a univariate analysis that examined average annual expenditures on consumers by ethnicity. The analysis revealed that the average annual expenditures on Hispanic consumers was approximately one-third (⅓) of the average expenditures on White non-Hispanic consumers. (…) A bivariate analysis examining ethnicity and age (divided into six age cohorts) revealed that ethnic discrimination did not exist. Moreover, in all but one of the age cohorts, the trend reversed where the average annual expenditures on White non-Hispanic consumers were less than the expenditures on Hispanic consumers.”(Taylor, Mickel 2014)\nWhen running the simple table with aggregated data, the discrimination in this case appared evident. After running a few more detailed tables, it appears to be no evidence of discrimination based on this sample and the variables collected."
  },
  {
    "objectID": "content/labs/Lab_6/IM939_Lab_6_5-Simpsons_Paradox.html#takeaways",
    "href": "content/labs/Lab_6/IM939_Lab_6_5-Simpsons_Paradox.html#takeaways",
    "title": "23  IM939 - Lab 6 Part 5 Simpson’s Paradox",
    "section": "25.2 Takeaways",
    "text": "25.2 Takeaways\nThe example above concerns a crucial topic of discrimination. As you can see, data and statistics alone won’t give us the anwser. First results might give us a confusing result. Critical thinking is essential when working with data, in order to account for reasons not evident at the first sight. The authors remind us the following: 1) “outcome of important decisions (such as discrimination claims) are often heavily influenced by statistics and how an incomplete analysis may lead to poor decision making” 2) “importance of identifying and analyzing all sources of specific variation (i.e., potential influential factors) in statistical analyses”. This is something we already discussed in previous weeks, but it is never enough to stress it out”\n\n25.2.1 *Additional Links\nSome links regarding categorical data in Python for those interested:\nhttps://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html#description\nhttps://pandas.pydata.org/pandas-docs/version/0.23.1/generated/pandas.DataFrame.plot.bar.html\nhttps://seaborn.pydata.org/tutorial/categorical.html\nhttps://seaborn.pydata.org/generated/seaborn.countplot.html"
  },
  {
    "objectID": "content/labs/Lab_7/IM939_Lab7-Part1.html#datasets",
    "href": "content/labs/Lab_7/IM939_Lab7-Part1.html#datasets",
    "title": "24  Lab: Hate crimes",
    "section": "24.1 Datasets",
    "text": "24.1 Datasets\n\nHate crimes: a csv file\nOECD Poverty gap: a csv file\nPoverty & Equity Data Portal: From Organisation for Economic Co-operation and Development (OECD) or from worldbank\n\n\n24.1.1 Further datasets\n\nNHS: multiple files. The NHS inequality challenge https://www.nuffieldtrust.org.uk/project/nhs-visual-data-challenge \nOffice for National Statistics (ONS):\n\nGender Pay Gap\nHealth state life expectancies by Index of Multiple Deprivation (IMD 2015 and IMD 2019): England, all ages multiple publications\n\n\n\n\n24.1.2 Additional Readings\n\nIndicators - critical reviews: The Poverty of Statistics and the Statistics of Poverty: https://www.tandfonline.com/doi/full/10.1080/01436590903321844?src=recsys\nIndicators in global health: arguments: indicators are usually comprehensible to a small group of experts. Why use indicators then? „Because indicators used in global HIV finance offer openings for engagement to promote accountability (…) some indicators and data truly are better than others, and as they were all created by humans, they all can be deconstructed and remade in other forms” Davis, S. (2020). The Uncounted: Politics of Data in Global Health, Cambridge. doi:10.1017/9781108649544\n\nIndicators - conceptualization"
  },
  {
    "objectID": "content/labs/Lab_7/IM939_Lab7-Part1.html#hate-crimes",
    "href": "content/labs/Lab_7/IM939_Lab7-Part1.html#hate-crimes",
    "title": "24  Lab: Hate crimes",
    "section": "24.2 Hate Crimes",
    "text": "24.2 Hate Crimes\n\n24.2.1 Source:\nhttps://github.com/fivethirtyeight/data/tree/master/hate-crimes\n\n\n24.2.2 Variables:\n\n\n\n\n\n\n\nHeader\nDefinition\n\n\n\n\nstate\nState name\n\n\nmedian_household_income\nMedian household income, 2016\n\n\nshare_unemployed_seasonal\nShare of the population that is unemployed (seasonally adjusted), Sept. 2016\n\n\nshare_population_in_metro_areas\nShare of the population that lives in metropolitan areas, 2015\n\n\nshare_population_with_high_school_degree\nShare of adults 25 and older with a high-school degree, 2009\n\n\nshare_non_citizen\nShare of the population that are not U.S. citizens, 2015\n\n\nshare_white_poverty\nShare of white residents who are living in poverty, 2015\n\n\ngini_index\nGini Index, 2015\n\n\nshare_non_white\nShare of the population that is not white, 2015\n\n\nshare_voters_voted_trump\nShare of 2016 U.S. presidential voters who voted for Donald Trump\n\n\nhate_crimes_per_100k_splc\nHate crimes per 100,000 population, Southern Poverty Law Center, Nov. 9-18, 2016\n\n\navg_hatecrimes_per_100k_fbi\nAverage annual hate crimes per 100,000 population, FBI, 2010-2015"
  },
  {
    "objectID": "content/labs/Lab_7/IM939_Lab7-Part1.html#data-exploration",
    "href": "content/labs/Lab_7/IM939_Lab7-Part1.html#data-exploration",
    "title": "24  Lab: Hate crimes",
    "section": "24.3 Data exploration",
    "text": "24.3 Data exploration\n\nimport pandas as pd\ndf = pd.read_excel('data/hate_Crimes_v2.xlsx')\n\nA reminder: anything with a pd. prefix comes from pandas. This is particulary useful for preventing a module from overwriting inbuilt Python functionality.\nLet’s have a look at our dataset\n\ndf.tail()\n\n\n\n\n\n\n\n\nNAME\nmedian_household_income\nshare_unemployed_seasonal\nshare_population_in_metro_areas\nshare_population_with_high_school_degree\nshare_non_citizen\nshare_white_poverty\ngini_index\nshare_non_white\nshare_voters_voted_trump\nhate_crimes_per_100k_splc\navg_hatecrimes_per_100k_fbi\n\n\n\n\n46\nVirginia\n66155\n0.043\n0.89\n0.866\n0.06\n0.07\n0.459\n0.38\n0.45\n0.36\n1.72\n\n\n47\nWashington\n59068\n0.052\n0.86\n0.897\n0.08\n0.09\n0.441\n0.31\n0.38\n0.67\n3.81\n\n\n48\nWest Virginia\n39552\n0.073\n0.55\n0.828\n0.01\n0.14\n0.451\n0.07\n0.69\n0.32\n2.03\n\n\n49\nWisconsin\n58080\n0.043\n0.69\n0.898\n0.03\n0.09\n0.430\n0.22\n0.48\n0.22\n1.12\n\n\n50\nWyoming\n55690\n0.040\n0.31\n0.918\n0.02\n0.09\n0.423\n0.15\n0.70\n0.00\n0.26\n\n\n\n\n\n\n\n\ntype(df)\n\npandas.core.frame.DataFrame\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 51 entries, 0 to 50\nData columns (total 12 columns):\n #   Column                                    Non-Null Count  Dtype  \n---  ------                                    --------------  -----  \n 0   NAME                                      51 non-null     object \n 1   median_household_income                   51 non-null     int64  \n 2   share_unemployed_seasonal                 51 non-null     float64\n 3   share_population_in_metro_areas           51 non-null     float64\n 4   share_population_with_high_school_degree  51 non-null     float64\n 5   share_non_citizen                         48 non-null     float64\n 6   share_white_poverty                       51 non-null     float64\n 7   gini_index                                51 non-null     float64\n 8   share_non_white                           51 non-null     float64\n 9   share_voters_voted_trump                  51 non-null     float64\n 10  hate_crimes_per_100k_splc                 51 non-null     float64\n 11  avg_hatecrimes_per_100k_fbi               51 non-null     float64\ndtypes: float64(10), int64(1), object(1)\nmemory usage: 4.9+ KB\n\n\n\n24.3.1 Missing values\nLet’s explore the dataset\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 51 entries, 0 to 50\nData columns (total 12 columns):\n #   Column                                    Non-Null Count  Dtype  \n---  ------                                    --------------  -----  \n 0   NAME                                      51 non-null     object \n 1   median_household_income                   51 non-null     int64  \n 2   share_unemployed_seasonal                 51 non-null     float64\n 3   share_population_in_metro_areas           51 non-null     float64\n 4   share_population_with_high_school_degree  51 non-null     float64\n 5   share_non_citizen                         48 non-null     float64\n 6   share_white_poverty                       51 non-null     float64\n 7   gini_index                                51 non-null     float64\n 8   share_non_white                           51 non-null     float64\n 9   share_voters_voted_trump                  51 non-null     float64\n 10  hate_crimes_per_100k_splc                 51 non-null     float64\n 11  avg_hatecrimes_per_100k_fbi               51 non-null     float64\ndtypes: float64(10), int64(1), object(1)\nmemory usage: 4.9+ KB\n\n\nThe above tables shows that we have some missing data for some of states. See below too.\n\ndf.isna().sum()\n\nNAME                                        0\nmedian_household_income                     0\nshare_unemployed_seasonal                   0\nshare_population_in_metro_areas             0\nshare_population_with_high_school_degree    0\nshare_non_citizen                           3\nshare_white_poverty                         0\ngini_index                                  0\nshare_non_white                             0\nshare_voters_voted_trump                    0\nhate_crimes_per_100k_splc                   0\navg_hatecrimes_per_100k_fbi                 0\ndtype: int64\n\n\n\nimport numpy as np\nnp.unique(df.NAME)\n\narray(['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California',\n       'Colorado', 'Connecticut', 'Delaware', 'District of Columbia',\n       'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana',\n       'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland',\n       'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi',\n       'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire',\n       'New Jersey', 'New Mexico', 'New York', 'North Carolina',\n       'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania',\n       'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee',\n       'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington',\n       'West Virginia', 'Wisconsin', 'Wyoming'], dtype=object)\n\n\nThere aren’t any unexpected values in ‘state’."
  },
  {
    "objectID": "content/labs/Lab_7/IM939_Lab7-Part1.html#mapping-hate-crime-across-the-usa",
    "href": "content/labs/Lab_7/IM939_Lab7-Part1.html#mapping-hate-crime-across-the-usa",
    "title": "24  Lab: Hate crimes",
    "section": "24.4 Mapping hate crime across the USA",
    "text": "24.4 Mapping hate crime across the USA\n\n#using James' code from the last lab: we need  the geospatial polygons of the states in America  \nimport geopandas as gpd \nimport pandas as pd\nimport altair as alt\n\ngeo_states = gpd.read_file('data/gz_2010_us_040_00_500k.json')\n#df = pd.read_excel('data/hate_Crimes_v2.xlsx')\ngeo_states.head()\n\n\n\n\n\n\n\n\nGEO_ID\nSTATE\nNAME\nLSAD\nCENSUSAREA\ngeometry\n\n\n\n\n0\n0400000US23\n23\nMaine\n\n30842.923\nMULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n\n\n1\n0400000US25\n25\nMassachusetts\n\n7800.058\nMULTIPOLYGON (((-70.83204 41.60650, -70.82373 ...\n\n\n2\n0400000US26\n26\nMichigan\n\n56538.901\nMULTIPOLYGON (((-88.68443 48.11579, -88.67563 ...\n\n\n3\n0400000US30\n30\nMontana\n\n145545.801\nPOLYGON ((-104.05770 44.99743, -104.25015 44.9...\n\n\n4\n0400000US32\n32\nNevada\n\n109781.180\nPOLYGON ((-114.05060 37.00040, -114.04999 36.9...\n\n\n\n\n\n\n\n\nalt.Chart(geo_states, title='US states').mark_geoshape().encode(\n).properties(\n    width=500,\n    height=300\n).project(\n    type='albersUsa'\n)\n\n\n\n\n\n\n\n\n# Add the data\n#should i rename 'state' to 'NAME'?\ngeo_states = geo_states.merge(df, on='NAME')\n\n\ngeo_states.head()\n\n\n\n\n\n\n\n\nGEO_ID\nSTATE\nNAME\nLSAD\nCENSUSAREA\ngeometry\nmedian_household_income\nshare_unemployed_seasonal\nshare_population_in_metro_areas\nshare_population_with_high_school_degree\nshare_non_citizen\nshare_white_poverty\ngini_index\nshare_non_white\nshare_voters_voted_trump\nhate_crimes_per_100k_splc\navg_hatecrimes_per_100k_fbi\n\n\n\n\n0\n0400000US23\n23\nMaine\n\n30842.923\nMULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n51710\n0.044\n0.54\n0.902\nNaN\n0.12\n0.437\n0.09\n0.45\n0.61\n2.62\n\n\n1\n0400000US25\n25\nMassachusetts\n\n7800.058\nMULTIPOLYGON (((-70.83204 41.60650, -70.82373 ...\n63151\n0.046\n0.97\n0.890\n0.09\n0.08\n0.475\n0.27\n0.34\n0.63\n4.80\n\n\n2\n0400000US26\n26\nMichigan\n\n56538.901\nMULTIPOLYGON (((-88.68443 48.11579, -88.67563 ...\n52005\n0.050\n0.87\n0.879\n0.04\n0.09\n0.451\n0.24\n0.48\n0.40\n3.20\n\n\n3\n0400000US30\n30\nMontana\n\n145545.801\nPOLYGON ((-104.05770 44.99743, -104.25015 44.9...\n51102\n0.041\n0.34\n0.908\n0.01\n0.10\n0.435\n0.10\n0.57\n0.49\n2.95\n\n\n4\n0400000US32\n32\nNevada\n\n109781.180\nPOLYGON ((-114.05060 37.00040, -114.04999 36.9...\n49875\n0.067\n0.87\n0.839\n0.10\n0.08\n0.448\n0.50\n0.46\n0.14\n2.11\n\n\n\n\n\n\n\n\nalt.Chart(geo_states, title='PRE-election Hate crime per 100k').mark_geoshape().encode(\n    color='avg_hatecrimes_per_100k_fbi',\n    tooltip=['NAME', 'avg_hatecrimes_per_100k_fbi']\n).properties(\n    width=500,\n    height=300\n).project(\n    type='albersUsa'\n)\n\n\n\n\n\n\n\n\nalt.Chart(geo_states, title='POST-election Hate crime per 100k').mark_geoshape().encode(\n    color='hate_crimes_per_100k_splc',\n    tooltip=['NAME', 'hate_crimes_per_100k_splc']\n).properties(\n    width=500,\n    height=300\n).project(\n    type='albersUsa'\n)\n\n\n\n\n\n\n\n\n24.4.1 Exploring data\n\nimport seaborn as sns\nsns.pairplot(data = df.iloc[:,1:])\n\n\n\n\n\ndf.boxplot(column=['median_household_income'])\n\n&lt;Axes: &gt;\n\n\n\n\n\n\ndf.boxplot(column=['avg_hatecrimes_per_100k_fbi'])\n\n&lt;Axes: &gt;\n\n\n\n\n\nWe may want to drop columns (remove them). Details are here.\nLet us drop Hawaii.\n\ndf[df.NAME == 'Hawaii']\n\n\n\n\n\n\n\n\nNAME\nmedian_household_income\nshare_unemployed_seasonal\nshare_population_in_metro_areas\nshare_population_with_high_school_degree\nshare_non_citizen\nshare_white_poverty\ngini_index\nshare_non_white\nshare_voters_voted_trump\nhate_crimes_per_100k_splc\navg_hatecrimes_per_100k_fbi\n\n\n\n\n11\nHawaii\n71223\n0.034\n0.76\n0.904\n0.08\n0.07\n0.433\n0.81\n0.3\n0.0\n0.0\n\n\n\n\n\n\n\n\ndf = df.drop(df.index[11])\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nmedian_household_income\nshare_unemployed_seasonal\nshare_population_in_metro_areas\nshare_population_with_high_school_degree\nshare_non_citizen\nshare_white_poverty\ngini_index\nshare_non_white\nshare_voters_voted_trump\nhate_crimes_per_100k_splc\navg_hatecrimes_per_100k_fbi\n\n\n\n\ncount\n50.000000\n50.000000\n50.000000\n50.000000\n47.000000\n50.000000\n50.000000\n50.000000\n50.00000\n50.000000\n50.000000\n\n\nmean\n54903.620000\n0.049880\n0.750000\n0.868420\n0.054043\n0.092200\n0.454180\n0.305800\n0.49380\n0.281200\n2.363200\n\n\nstd\n9010.994814\n0.010571\n0.183425\n0.034049\n0.031184\n0.024767\n0.020889\n0.150551\n0.11674\n0.255779\n1.714502\n\n\nmin\n35521.000000\n0.028000\n0.310000\n0.799000\n0.010000\n0.040000\n0.419000\n0.060000\n0.04000\n0.000000\n0.260000\n\n\n25%\n48358.500000\n0.042250\n0.630000\n0.839750\n0.030000\n0.080000\n0.440000\n0.192500\n0.42000\n0.130000\n1.290000\n\n\n50%\n54613.000000\n0.051000\n0.790000\n0.874000\n0.040000\n0.090000\n0.454500\n0.275000\n0.49500\n0.215000\n1.980000\n\n\n75%\n60652.750000\n0.057750\n0.897500\n0.897750\n0.080000\n0.100000\n0.466750\n0.420000\n0.57750\n0.345000\n3.182500\n\n\nmax\n76165.000000\n0.073000\n1.000000\n0.918000\n0.130000\n0.170000\n0.532000\n0.630000\n0.70000\n1.520000\n10.950000\n\n\n\n\n\n\n\n\ndf.plot(x = 'avg_hatecrimes_per_100k_fbi', y = 'median_household_income', kind='scatter')\n\n&lt;Axes: xlabel='avg_hatecrimes_per_100k_fbi', ylabel='median_household_income'&gt;\n\n\n\n\n\n\ndf.plot(x = 'hate_crimes_per_100k_splc', y = 'median_household_income', kind='scatter')\n\n&lt;Axes: xlabel='hate_crimes_per_100k_splc', ylabel='median_household_income'&gt;\n\n\n\n\n\n\ndf[df.hate_crimes_per_100k_splc &gt; (np.std(df.hate_crimes_per_100k_splc) * 2.5)]\n\n\n\n\n\n\n\n\nNAME\nmedian_household_income\nshare_unemployed_seasonal\nshare_population_in_metro_areas\nshare_population_with_high_school_degree\nshare_non_citizen\nshare_white_poverty\ngini_index\nshare_non_white\nshare_voters_voted_trump\nhate_crimes_per_100k_splc\navg_hatecrimes_per_100k_fbi\n\n\n\n\n8\nDistrict of Columbia\n68277\n0.067\n1.00\n0.871\n0.11\n0.04\n0.532\n0.63\n0.04\n1.52\n10.95\n\n\n37\nOregon\n58875\n0.062\n0.87\n0.891\n0.07\n0.10\n0.449\n0.26\n0.41\n0.83\n3.39\n\n\n47\nWashington\n59068\n0.052\n0.86\n0.897\n0.08\n0.09\n0.441\n0.31\n0.38\n0.67\n3.81\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\noutliers_df = df[df.hate_crimes_per_100k_splc &gt; (np.std(df.hate_crimes_per_100k_splc) * 2.5)]\ndf.plot(x = 'hate_crimes_per_100k_splc', y = 'median_household_income', kind='scatter')\n\nplt.scatter(outliers_df.hate_crimes_per_100k_splc, outliers_df.median_household_income ,c='red')\n\n&lt;matplotlib.collections.PathCollection at 0x16c96c850&gt;\n\n\n\n\n\n\ndf_pivot = df.pivot_table(index=['NAME'], values=['hate_crimes_per_100k_splc', 'avg_hatecrimes_per_100k_fbi', 'median_household_income'])\ndf_pivot\n\n##sort by values\n#df_pivot = pd.pivot_table(df, index=['state'], columns = ['hate_crimes_per_100k_splc'], fill_value=0)\n#df_pivot\n#df2 = df_pivot.reindex(df_pivot['hate_crimes_per_100k_splc'].sort_values(by='hate_crimes_per_100k_splc', ascending=False).index)\n\n\n\n\n\n\n\n\navg_hatecrimes_per_100k_fbi\nhate_crimes_per_100k_splc\nmedian_household_income\n\n\nNAME\n\n\n\n\n\n\n\nAlabama\n1.80\n0.12\n42278\n\n\nAlaska\n1.65\n0.14\n67629\n\n\nArizona\n3.41\n0.22\n49254\n\n\nArkansas\n0.86\n0.06\n44922\n\n\nCalifornia\n2.39\n0.25\n60487\n\n\nColorado\n2.80\n0.39\n60940\n\n\nConnecticut\n3.77\n0.33\n70161\n\n\nDelaware\n1.46\n0.32\n57522\n\n\nDistrict of Columbia\n10.95\n1.52\n68277\n\n\nFlorida\n0.69\n0.18\n46140\n\n\nGeorgia\n0.41\n0.12\n49555\n\n\nIdaho\n1.89\n0.12\n53438\n\n\nIllinois\n1.04\n0.19\n54916\n\n\nIndiana\n1.75\n0.24\n48060\n\n\nIowa\n0.56\n0.45\n57810\n\n\nKansas\n2.14\n0.10\n53444\n\n\nKentucky\n4.20\n0.32\n42786\n\n\nLouisiana\n1.34\n0.10\n42406\n\n\nMaine\n2.62\n0.61\n51710\n\n\nMaryland\n1.32\n0.37\n76165\n\n\nMassachusetts\n4.80\n0.63\n63151\n\n\nMichigan\n3.20\n0.40\n52005\n\n\nMinnesota\n3.61\n0.62\n67244\n\n\nMississippi\n0.62\n0.06\n35521\n\n\nMissouri\n1.90\n0.18\n56630\n\n\nMontana\n2.95\n0.49\n51102\n\n\nNebraska\n2.68\n0.15\n56870\n\n\nNevada\n2.11\n0.14\n49875\n\n\nNew Hampshire\n2.10\n0.15\n73397\n\n\nNew Jersey\n4.41\n0.07\n65243\n\n\nNew Mexico\n1.88\n0.29\n46686\n\n\nNew York\n3.10\n0.35\n54310\n\n\nNorth Carolina\n1.26\n0.24\n46784\n\n\nNorth Dakota\n4.74\n0.00\n60730\n\n\nOhio\n3.24\n0.19\n49644\n\n\nOklahoma\n1.08\n0.13\n47199\n\n\nOregon\n3.39\n0.83\n58875\n\n\nPennsylvania\n0.43\n0.28\n55173\n\n\nRhode Island\n1.28\n0.09\n58633\n\n\nSouth Carolina\n1.93\n0.20\n44929\n\n\nSouth Dakota\n3.30\n0.00\n53053\n\n\nTennessee\n3.13\n0.19\n43716\n\n\nTexas\n0.75\n0.21\n53875\n\n\nUtah\n2.38\n0.13\n63383\n\n\nVermont\n1.90\n0.32\n60708\n\n\nVirginia\n1.72\n0.36\n66155\n\n\nWashington\n3.81\n0.67\n59068\n\n\nWest Virginia\n2.03\n0.32\n39552\n\n\nWisconsin\n1.12\n0.22\n58080\n\n\nWyoming\n0.26\n0.00\n55690\n\n\n\n\n\n\n\n\ndf_pivot.sort_values(by=['avg_hatecrimes_per_100k_fbi'], ascending=False)\n\n\n\n\n\n\n\n\navg_hatecrimes_per_100k_fbi\nhate_crimes_per_100k_splc\nmedian_household_income\n\n\nNAME\n\n\n\n\n\n\n\nDistrict of Columbia\n10.95\n1.52\n68277\n\n\nMassachusetts\n4.80\n0.63\n63151\n\n\nNorth Dakota\n4.74\n0.00\n60730\n\n\nNew Jersey\n4.41\n0.07\n65243\n\n\nKentucky\n4.20\n0.32\n42786\n\n\nWashington\n3.81\n0.67\n59068\n\n\nConnecticut\n3.77\n0.33\n70161\n\n\nMinnesota\n3.61\n0.62\n67244\n\n\nArizona\n3.41\n0.22\n49254\n\n\nOregon\n3.39\n0.83\n58875\n\n\nSouth Dakota\n3.30\n0.00\n53053\n\n\nOhio\n3.24\n0.19\n49644\n\n\nMichigan\n3.20\n0.40\n52005\n\n\nTennessee\n3.13\n0.19\n43716\n\n\nNew York\n3.10\n0.35\n54310\n\n\nMontana\n2.95\n0.49\n51102\n\n\nColorado\n2.80\n0.39\n60940\n\n\nNebraska\n2.68\n0.15\n56870\n\n\nMaine\n2.62\n0.61\n51710\n\n\nCalifornia\n2.39\n0.25\n60487\n\n\nUtah\n2.38\n0.13\n63383\n\n\nKansas\n2.14\n0.10\n53444\n\n\nNevada\n2.11\n0.14\n49875\n\n\nNew Hampshire\n2.10\n0.15\n73397\n\n\nWest Virginia\n2.03\n0.32\n39552\n\n\nSouth Carolina\n1.93\n0.20\n44929\n\n\nVermont\n1.90\n0.32\n60708\n\n\nMissouri\n1.90\n0.18\n56630\n\n\nIdaho\n1.89\n0.12\n53438\n\n\nNew Mexico\n1.88\n0.29\n46686\n\n\nAlabama\n1.80\n0.12\n42278\n\n\nIndiana\n1.75\n0.24\n48060\n\n\nVirginia\n1.72\n0.36\n66155\n\n\nAlaska\n1.65\n0.14\n67629\n\n\nDelaware\n1.46\n0.32\n57522\n\n\nLouisiana\n1.34\n0.10\n42406\n\n\nMaryland\n1.32\n0.37\n76165\n\n\nRhode Island\n1.28\n0.09\n58633\n\n\nNorth Carolina\n1.26\n0.24\n46784\n\n\nWisconsin\n1.12\n0.22\n58080\n\n\nOklahoma\n1.08\n0.13\n47199\n\n\nIllinois\n1.04\n0.19\n54916\n\n\nArkansas\n0.86\n0.06\n44922\n\n\nTexas\n0.75\n0.21\n53875\n\n\nFlorida\n0.69\n0.18\n46140\n\n\nMississippi\n0.62\n0.06\n35521\n\n\nIowa\n0.56\n0.45\n57810\n\n\nPennsylvania\n0.43\n0.28\n55173\n\n\nGeorgia\n0.41\n0.12\n49555\n\n\nWyoming\n0.26\n0.00\n55690\n\n\n\n\n\n\n\n\n#This is code for standarization  \nfrom sklearn import preprocessing\nimport numpy as np\n\n#Get column names first\n#names = df.columns\n#df_stand = df[['median_household_income','share_unemployed_seasonal']]\ndf_stand = df[['median_household_income','share_unemployed_seasonal', 'share_population_in_metro_areas'\n               , 'share_population_with_high_school_degree', 'share_non_citizen', 'share_white_poverty', 'gini_index'\n               , 'share_non_white', 'share_voters_voted_trump', 'hate_crimes_per_100k_splc', 'avg_hatecrimes_per_100k_fbi']]\nnames = df_stand.columns\n#Create the Scaler object\nscaler = preprocessing.StandardScaler()\n#Fit your data on the scaler object\ndf2 = scaler.fit_transform(df_stand)\ndf2 = pd.DataFrame(df2, columns=names)\ndf2.tail()\n\n\n\n\n\n\n\n\nmedian_household_income\nshare_unemployed_seasonal\nshare_population_in_metro_areas\nshare_population_with_high_school_degree\nshare_non_citizen\nshare_white_poverty\ngini_index\nshare_non_white\nshare_voters_voted_trump\nhate_crimes_per_100k_splc\navg_hatecrimes_per_100k_fbi\n\n\n\n\n45\n1.261305\n-0.657461\n0.771002\n-0.071795\n0.193108\n-0.905436\n0.233085\n0.497859\n-0.379003\n0.311206\n-0.378961\n\n\n46\n0.466836\n0.202590\n0.605787\n0.847894\n0.841399\n-0.089728\n-0.637357\n0.028181\n-0.984716\n1.535493\n0.852428\n\n\n47\n-1.720951\n2.209376\n-1.101431\n-1.199157\n-1.427620\n1.949543\n-0.153778\n-1.582146\n1.697727\n0.153233\n-0.196315\n\n\n48\n0.356079\n-0.657461\n-0.330429\n0.877562\n-0.779329\n-0.089728\n-1.169293\n-0.575692\n-0.119412\n-0.241698\n-0.732470\n\n\n49\n0.088155\n-0.944145\n-2.423149\n1.470910\n-1.103475\n-0.089728\n-1.507798\n-1.045370\n1.784258\n-1.110547\n-1.239166\n\n\n\n\n\n\n\n\nax = sns.boxplot(data=df2, orient=\"h\")\n\n\n\n\n\n#wanted to remove row with Hawaii (row nr 11) following https://chrisalbon.com/python/data_wrangling/pandas_dropping_column_and_rows/\n\ndf2 = df.copy()\ndf2\n#df2.drop('Hawaii')\n#df2.drop(11) #drop Hawaii row\ndf2.drop(df.index[11])\ndf2.tail()\n\n\n\n\n\n\n\n\nNAME\nmedian_household_income\nshare_unemployed_seasonal\nshare_population_in_metro_areas\nshare_population_with_high_school_degree\nshare_non_citizen\nshare_white_poverty\ngini_index\nshare_non_white\nshare_voters_voted_trump\nhate_crimes_per_100k_splc\navg_hatecrimes_per_100k_fbi\n\n\n\n\n46\nVirginia\n66155\n0.043\n0.89\n0.866\n0.06\n0.07\n0.459\n0.38\n0.45\n0.36\n1.72\n\n\n47\nWashington\n59068\n0.052\n0.86\n0.897\n0.08\n0.09\n0.441\n0.31\n0.38\n0.67\n3.81\n\n\n48\nWest Virginia\n39552\n0.073\n0.55\n0.828\n0.01\n0.14\n0.451\n0.07\n0.69\n0.32\n2.03\n\n\n49\nWisconsin\n58080\n0.043\n0.69\n0.898\n0.03\n0.09\n0.430\n0.22\n0.48\n0.22\n1.12\n\n\n50\nWyoming\n55690\n0.040\n0.31\n0.918\n0.02\n0.09\n0.423\n0.15\n0.70\n0.00\n0.26\n\n\n\n\n\n\n\n\nimport scipy.stats\n#instead of running it one by one for every pair of variables, like:\n#scipy.stats.pearsonr(st_wine.quality.values, st_wine.alcohol.values) \n\ncorrMatrix = df2.corr(numeric_only=True).round(2)\nprint (corrMatrix)\n\n                                          median_household_income  \\\nmedian_household_income                                      1.00   \nshare_unemployed_seasonal                                   -0.34   \nshare_population_in_metro_areas                              0.29   \nshare_population_with_high_school_degree                     0.64   \nshare_non_citizen                                            0.28   \nshare_white_poverty                                         -0.82   \ngini_index                                                  -0.15   \nshare_non_white                                             -0.00   \nshare_voters_voted_trump                                    -0.57   \nhate_crimes_per_100k_splc                                    0.33   \navg_hatecrimes_per_100k_fbi                                  0.32   \n\n                                          share_unemployed_seasonal  \\\nmedian_household_income                                       -0.34   \nshare_unemployed_seasonal                                      1.00   \nshare_population_in_metro_areas                                0.37   \nshare_population_with_high_school_degree                      -0.61   \nshare_non_citizen                                              0.31   \nshare_white_poverty                                            0.19   \ngini_index                                                     0.53   \nshare_non_white                                                0.59   \nshare_voters_voted_trump                                      -0.21   \nhate_crimes_per_100k_splc                                      0.18   \navg_hatecrimes_per_100k_fbi                                    0.07   \n\n                                          share_population_in_metro_areas  \\\nmedian_household_income                                              0.29   \nshare_unemployed_seasonal                                            0.37   \nshare_population_in_metro_areas                                      1.00   \nshare_population_with_high_school_degree                            -0.27   \nshare_non_citizen                                                    0.75   \nshare_white_poverty                                                 -0.39   \ngini_index                                                           0.52   \nshare_non_white                                                      0.60   \nshare_voters_voted_trump                                            -0.58   \nhate_crimes_per_100k_splc                                            0.26   \navg_hatecrimes_per_100k_fbi                                          0.21   \n\n                                          share_population_with_high_school_degree  \\\nmedian_household_income                                                       0.64   \nshare_unemployed_seasonal                                                    -0.61   \nshare_population_in_metro_areas                                              -0.27   \nshare_population_with_high_school_degree                                      1.00   \nshare_non_citizen                                                            -0.30   \nshare_white_poverty                                                          -0.48   \ngini_index                                                                   -0.58   \nshare_non_white                                                              -0.56   \nshare_voters_voted_trump                                                     -0.13   \nhate_crimes_per_100k_splc                                                     0.21   \navg_hatecrimes_per_100k_fbi                                                   0.16   \n\n                                          share_non_citizen  \\\nmedian_household_income                                0.28   \nshare_unemployed_seasonal                              0.31   \nshare_population_in_metro_areas                        0.75   \nshare_population_with_high_school_degree              -0.30   \nshare_non_citizen                                      1.00   \nshare_white_poverty                                   -0.38   \ngini_index                                             0.51   \nshare_non_white                                        0.76   \nshare_voters_voted_trump                              -0.62   \nhate_crimes_per_100k_splc                              0.28   \navg_hatecrimes_per_100k_fbi                            0.30   \n\n                                          share_white_poverty  gini_index  \\\nmedian_household_income                                 -0.82       -0.15   \nshare_unemployed_seasonal                                0.19        0.53   \nshare_population_in_metro_areas                         -0.39        0.52   \nshare_population_with_high_school_degree                -0.48       -0.58   \nshare_non_citizen                                       -0.38        0.51   \nshare_white_poverty                                      1.00        0.01   \ngini_index                                               0.01        1.00   \nshare_non_white                                         -0.24        0.59   \nshare_voters_voted_trump                                 0.54       -0.46   \nhate_crimes_per_100k_splc                               -0.26        0.38   \navg_hatecrimes_per_100k_fbi                             -0.26        0.42   \n\n                                          share_non_white  \\\nmedian_household_income                             -0.00   \nshare_unemployed_seasonal                            0.59   \nshare_population_in_metro_areas                      0.60   \nshare_population_with_high_school_degree            -0.56   \nshare_non_citizen                                    0.76   \nshare_white_poverty                                 -0.24   \ngini_index                                           0.59   \nshare_non_white                                      1.00   \nshare_voters_voted_trump                            -0.44   \nhate_crimes_per_100k_splc                            0.12   \navg_hatecrimes_per_100k_fbi                          0.08   \n\n                                          share_voters_voted_trump  \\\nmedian_household_income                                      -0.57   \nshare_unemployed_seasonal                                    -0.21   \nshare_population_in_metro_areas                              -0.58   \nshare_population_with_high_school_degree                     -0.13   \nshare_non_citizen                                            -0.62   \nshare_white_poverty                                           0.54   \ngini_index                                                   -0.46   \nshare_non_white                                              -0.44   \nshare_voters_voted_trump                                      1.00   \nhate_crimes_per_100k_splc                                    -0.69   \navg_hatecrimes_per_100k_fbi                                  -0.50   \n\n                                          hate_crimes_per_100k_splc  \\\nmedian_household_income                                        0.33   \nshare_unemployed_seasonal                                      0.18   \nshare_population_in_metro_areas                                0.26   \nshare_population_with_high_school_degree                       0.21   \nshare_non_citizen                                              0.28   \nshare_white_poverty                                           -0.26   \ngini_index                                                     0.38   \nshare_non_white                                                0.12   \nshare_voters_voted_trump                                      -0.69   \nhate_crimes_per_100k_splc                                      1.00   \navg_hatecrimes_per_100k_fbi                                    0.68   \n\n                                          avg_hatecrimes_per_100k_fbi  \nmedian_household_income                                          0.32  \nshare_unemployed_seasonal                                        0.07  \nshare_population_in_metro_areas                                  0.21  \nshare_population_with_high_school_degree                         0.16  \nshare_non_citizen                                                0.30  \nshare_white_poverty                                             -0.26  \ngini_index                                                       0.42  \nshare_non_white                                                  0.08  \nshare_voters_voted_trump                                        -0.50  \nhate_crimes_per_100k_splc                                        0.68  \navg_hatecrimes_per_100k_fbi                                      1.00  \n\n\n\nimport pandas as pd\nimport seaborn as sn\nimport matplotlib.pyplot as plt\n\ncorrMatrix = df2.corr(numeric_only=True).round(1)  #I added here \".round(1)\" so that's easier to read given number of variables\nsn.heatmap(corrMatrix, annot=True)\nplt.show()\n\n\n\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\n\nx = df2[['median_household_income', 'share_population_with_high_school_degree', 'share_voters_voted_trump']]\ny = df2[['avg_hatecrimes_per_100k_fbi']]\n#what if we change the y variable\n#y = df2[['hate_crimes_per_100k_splc']]\n\nest = LinearRegression(fit_intercept = True) \nest.fit(x, y)\n\nprint(\"Coefficients:\", est.coef_)\nprint (\"Intercept:\", est.intercept_)\n\nmodel = LinearRegression()\nmodel.fit(x, y)\ny_hat = model.predict(x)\nprint (\"MSE:\", metrics.mean_squared_error(y, y_hat))\nprint (\"R^2:\", metrics.r2_score(y, y_hat))\nprint (\"var:\", y.var())\n\nCoefficients: [[-1.63935828e-05  7.65352737e+00 -7.85302986e+00]]\nIntercept: [0.49461694]\nMSE: 2.1105276140605045\nR^2: 0.26736253642536767\nvar: avg_hatecrimes_per_100k_fbi    2.939516\ndtype: float64"
  },
  {
    "objectID": "content/labs/Lab_7/IM939_Lab7-Part2.html#source",
    "href": "content/labs/Lab_7/IM939_Lab7-Part2.html#source",
    "title": "25  Lab: Poverty and Inequality",
    "section": "25.1 Source",
    "text": "25.1 Source\nIn this lab, we will be using World Development Indicators dataset from worldbank, which contains the following features:\n\n\n\nFeatures’ definitions"
  },
  {
    "objectID": "content/labs/Lab_7/IM939_Lab7-Part2.html#reading-the-dataset",
    "href": "content/labs/Lab_7/IM939_Lab7-Part2.html#reading-the-dataset",
    "title": "25  Lab: Poverty and Inequality",
    "section": "25.2 Reading the dataset",
    "text": "25.2 Reading the dataset\n\nimport pandas as pd\ndf = pd.read_excel('data/WDI_countries_v2.xlsx', sheet_name='Data4')\n\nLet’s have a look at our dataset\n\ndf.head()\n\n\n\n\n\n\n\n\nCountry Code\nbirthrate\nDeathrate\nGNI\nLifeexp_female\nLifeexp_male\nNeonatal_death\n\n\n\n\n0\nAFG\n32.487\n6.423\n2260.0\n66.026\n63.047\n44503.0\n\n\n1\nALB\n11.780\n7.898\n13820.0\n80.167\n76.816\n243.0\n\n\n2\nDZA\n24.282\n4.716\n11450.0\n77.938\n75.494\n16407.0\n\n\n3\nAND\n7.200\n4.400\nNaN\nNaN\nNaN\n1.0\n\n\n4\nAGO\n40.729\n8.190\n6550.0\n63.666\n58.064\n35489.0\n\n\n\n\n\n\n\n\n25.2.1 Missing values\nLet’s check if we have any missing data\n\ndf.info()\ndf.isna().sum()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 216 entries, 0 to 215\nData columns (total 7 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   Country Code    216 non-null    object \n 1   birthrate       205 non-null    float64\n 2   Deathrate       205 non-null    float64\n 3   GNI             187 non-null    float64\n 4   Lifeexp_female  198 non-null    float64\n 5   Lifeexp_male    198 non-null    float64\n 6   Neonatal_death  193 non-null    float64\ndtypes: float64(6), object(1)\nmemory usage: 11.9+ KB\n\n\nCountry Code       0\nbirthrate         11\nDeathrate         11\nGNI               29\nLifeexp_female    18\nLifeexp_male      18\nNeonatal_death    23\ndtype: int64\n\n\n\ndf.boxplot(column=['birthrate', 'Deathrate'])\n\n&lt;Axes: &gt;\n\n\n\n\n\n\ndf.boxplot(column=['Lifeexp_female', 'Lifeexp_male'])\n\n&lt;Axes: &gt;\n\n\n\n\n\n\ndf.boxplot(column=['GNI'])\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n#This is code for standarization  \nfrom sklearn import preprocessing\nimport numpy as np\nimport seaborn as sns\n\n#Get column names first\n#names = df.columns\ndf_stand = df[['birthrate', 'Deathrate', 'GNI', 'Lifeexp_female', 'Lifeexp_male', 'Neonatal_death']]\nnames = df_stand.columns\n#Create the Scaler object\nscaler = preprocessing.StandardScaler()\n#Fit your data on the scaler object\ndf2 = scaler.fit_transform(df_stand)\ndf2 = pd.DataFrame(df2, columns=names)\ndf2.tail()\n\nax = sns.boxplot(data=df2, orient=\"h\")\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nbirthrate\nDeathrate\nGNI\nLifeexp_female\nLifeexp_male\nNeonatal_death\n\n\n\n\ncount\n205.000000\n205.000000\n187.000000\n198.000000\n198.000000\n193.000000\n\n\nmean\n19.637580\n7.573941\n20630.427807\n75.193288\n70.323854\n12948.031088\n\n\nstd\n9.839573\n2.636414\n21044.240160\n7.870933\n7.419214\n48782.770706\n\n\nmin\n5.900000\n1.202000\n780.000000\n54.991000\n50.582000\n0.000000\n\n\n25%\n10.900000\n5.800000\n5090.000000\n69.497250\n65.533500\n163.000000\n\n\n50%\n17.545000\n7.163000\n13280.000000\n77.193000\n71.140500\n1288.000000\n\n\n75%\n27.100000\n9.100000\n28360.000000\n80.776500\n76.047500\n7316.000000\n\n\nmax\n46.079000\n15.400000\n123290.000000\n87.700000\n82.300000\n546427.000000\n\n\n\n\n\n\n\n\ndf[['birthrate']].plot(kind='hist', ec='black')\n\n&lt;Axes: ylabel='Frequency'&gt;\n\n\n\n\n\n\nimport seaborn as sns\nsns.pairplot(data = df.iloc[:,1:])\n\n\n\n\n\ndf.plot(x = 'birthrate', y = 'Deathrate', kind='scatter')\n\n&lt;Axes: xlabel='birthrate', ylabel='Deathrate'&gt;\n\n\n\n\n\n\nimport pandas as pd\nimport seaborn as sn\nimport matplotlib.pyplot as plt\n\ncorrMatrix = df.corr(numeric_only=True).round(1)  #I added here \".round(1)\" so that's easier to read given number of variables\nsn.heatmap(corrMatrix, annot=True)\nplt.show()"
  },
  {
    "objectID": "content/labs/Lab_7/IM939_Lab7-Part2.html#how-quickly-are-populations-growing",
    "href": "content/labs/Lab_7/IM939_Lab7-Part2.html#how-quickly-are-populations-growing",
    "title": "25  Lab: Poverty and Inequality",
    "section": "25.3 How quickly are populations growing?",
    "text": "25.3 How quickly are populations growing?\nThis question can be investigated by calculating birth rate minus death rate. Results range from -?? (a decreasing population) to +?? (an increasing population). The mean is around ??, but what does it signify?\n\ndf['pop'] = df['birthrate'] - df['Deathrate']\ndf.head()\n\n\n\n\n\n\n\n\nCountry Code\nbirthrate\nDeathrate\nGNI\nLifeexp_female\nLifeexp_male\nNeonatal_death\npop\n\n\n\n\n0\nAFG\n32.487\n6.423\n2260.0\n66.026\n63.047\n44503.0\n26.064\n\n\n1\nALB\n11.780\n7.898\n13820.0\n80.167\n76.816\n243.0\n3.882\n\n\n2\nDZA\n24.282\n4.716\n11450.0\n77.938\n75.494\n16407.0\n19.566\n\n\n3\nAND\n7.200\n4.400\nNaN\nNaN\nNaN\n1.0\n2.800\n\n\n4\nAGO\n40.729\n8.190\n6550.0\n63.666\n58.064\n35489.0\n32.539\n\n\n\n\n\n\n\n\ndf.boxplot(column=['pop'])\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\n\nRouncefield, Mary. 1995. “The Statistics of Poverty and Inequality.” Journal of Statistics Education 3 (2): 8. https://doi.org/10.1080/10691898.1995.11910491."
  },
  {
    "objectID": "content/labs/Lab_7/IM939_Lab7-Part3.html#source",
    "href": "content/labs/Lab_7/IM939_Lab7-Part3.html#source",
    "title": "26  Lab: Gender gaps",
    "section": "26.1 Source",
    "text": "26.1 Source\n“Annual gender pay gap estimates for UK employees by age, occupation, industry, full-time and part-time, region and other geographies, and public and private sector. Compiled from the Annual Survey of Hours and Earnings.” https://www.ons.gov.uk/employmentandlabourmarket/peopleinwork/earningsandworkinghours/datasets/annualsurveyofhoursandearningsashegenderpaygaptables"
  },
  {
    "objectID": "content/labs/Lab_7/IM939_Lab7-Part3.html#explanations",
    "href": "content/labs/Lab_7/IM939_Lab7-Part3.html#explanations",
    "title": "26  Lab: Gender gaps",
    "section": "26.2 Explanations",
    "text": "26.2 Explanations\n^ Gender pay gap defined as the difference between men’s and women’s hourly earnings as a percentage of men’s earnings.\n^^ Employees on adult rates whose pay for the survey pay-period was not affected by absence. Estimates for 2020 include employees who have been furloughed under the Coronavirus Job Retention Scheme (CJRS).\nSource: Annual Survey of Hours and Earnings, Office for National Statistics.\nTable 2.12 Gender pay gap (%)^ - For all employee jobs^^: United Kingdom, 2020\n\n26.2.1 Definitions\n• Gender pay gap (GPG) - calculated as the difference between average hourly earnings (excluding overtime) of men and women as a proportion of average hourly earnings (excluding overtime) of men. For example, a 4% GPG denotes that women earn 4% less, on average, than men. Conversely, a -4% GPG denotes that women earn 4% more, on average, than men.\n• Mean - a measure of the average which is derived by summing the values for a given sample, and then dividing the sum by the number of observations (i.e. jobs) in the sample. In earnings distributions, the mean can be disproportionately influenced by a relatively small number of high-paying jobs.\n• Median - the value below which 50% of jobs fall. It is ONS’s preferred measure of average earnings as it is less affected by a relatively small number of very high earners and the skewed distribution of earnings. It therefore gives a better indication of typical pay than the mean.\n• Full-time - employees working more than 30 paid hours per week (or 25 or more for the teaching professions).\n\n\n26.2.2 Coverage and timeliness\nASHE covers employee jobs in the United Kingdom. It does not cover the self-employed, nor does it cover employees not paid during the reference period.\nGPG estimates are provided for the pay period that included a specified date in April. They relate to employees on adult rates of pay, whose earnings for the survey pay period were not affected by absence. Estimates for 2020 include employees who have been furloughed under the Coronavirus Job Retention Scheme (CJRS).\nASHE is based on a 1% sample of jobs taken from HM Revenue and Customs’ Pay As You Earn (PAYE) records. Consequently, individuals with more than one job may appear in the sample more than once.\n#### Quality measures\nThe colour coding within the tables indicates the quality of each estimate and is based upon the coefficient of variation (CV) values for the corresponding male and female earnings estimates. The CV is the ratio of the standard error of an estimate to the estimate itself and is expressed as a percentage. The smaller the CV the greater the accuracy of the estimate. The colour coding for the GPG estimates is derived as follows:\n• If the CV values of both the male and female earnings estimates are less than or equal to 5% then the GPG estimate is considered good quality.\n• If the CV value of either (or both) the male or female earnings estimate is greater than 5% and less than or equal to 10% (and the CV of the other estimate is less than or equal to 10%) then the GPG estimate is considered reasonable quality.\n• If the CV value of either (or both) the male or female earnings estimate is greater than 10% and less than or equal to 20% (and the CV of the other estimate is less than or equal to 20%) then the GPG estimate is considered lower quality and should be used with caution.\n• If the CV value of at least one of the earnings estimates is greater than 20% then the GPG estimate is considered unreliable for practical purposes and is suppressed."
  },
  {
    "objectID": "content/labs/Lab_7/IM939_Lab7-Part3.html#reading-the-dataset",
    "href": "content/labs/Lab_7/IM939_Lab7-Part3.html#reading-the-dataset",
    "title": "26  Lab: Gender gaps",
    "section": "26.3 Reading the dataset",
    "text": "26.3 Reading the dataset\n\nimport pandas as pd\n\ndf = pd.read_excel('data/genderpaygap.xlsx', sheet_name='All')\ndf2 = pd.read_excel('data/genderpaygap.xlsx', sheet_name='Main')\n\nLet’s have a look at our dataset\n\ndf\n\n\n\n\n\n\n\n\nProfessional_occupations\nCode\nGPGmedian\nGPGmean\n\n\n\n\n0\nAll employees\n0\n15.5\n14.6\n\n\n1\nManagers, directors and senior officials\n1\n11.3\n15.3\n\n\n2\nCorporate managers and directors\n11\n10.9\n12.6\n\n\n3\nOther managers and proprietors\n12\n6.7\n12.3\n\n\n4\nProfessional occupations\n2\n9.1\n12.8\n\n\n5\nScience, research, engineering and technolog...\n21\n7.8\n8.6\n\n\n6\nHealth professionals\n22\n14.6\n26.5\n\n\n7\nTeaching and educational professionals\n23\n6.3\n12.7\n\n\n8\nBusiness, media and public service professio...\n24\n8.5\n11.9\n\n\n9\nAssociate professional and technical occupations\n3\n12.9\n14.1\n\n\n10\nScience, engineering and technology associat...\n31\n16.6\n11.3\n\n\n11\nHealth and social care associate professionals\n32\n7.4\n11.3\n\n\n12\nProtective service occupations\n33\n6.5\n6.9\n\n\n13\nCulture, media and sports occupations\n34\n1.3\n23.2\n\n\n14\nBusiness and public service associate profes...\n35\n14.0\n16.8\n\n\n15\nAdministrative and secretarial occupations\n4\n7.5\n9.5\n\n\n16\nAdministrative occupations\n41\n6.3\n8.2\n\n\n17\nSecretarial and related occupations\n42\n-4.1\n1.1\n\n\n18\nSkilled trades occupations\n5\n22.2\n19.2\n\n\n19\nSkilled agricultural and related trades\n51\n11.3\n-4.2\n\n\n20\nSkilled metal, electrical and electronic trades\n52\n12.9\n7.5\n\n\n21\nSkilled construction and building trades\n53\n7.5\n6.2\n\n\n22\nTextiles, printing and other skilled trades\n54\n5.2\n8.4\n\n\n23\nCaring, leisure and other service occupations\n6\n4.3\n7.9\n\n\n24\nCaring personal service occupations\n61\n4.0\n6.6\n\n\n25\nLeisure, travel and related personal service...\n62\n11.1\n15.7\n\n\n26\nSales and customer service occupations\n7\n2.5\n7.2\n\n\n27\nSales occupations\n71\n2.7\n8.0\n\n\n28\nCustomer service occupations\n72\n1.5\n6.1\n\n\n29\nProcess, plant and machine operatives\n8\n14.6\n12.4\n\n\n30\nProcess, plant and machine operatives\n81\n16.2\n15.2\n\n\n31\nTransport and mobile machine drivers and ope...\n82\n11.6\n-0.3\n\n\n32\nElementary occupations\n9\n5.3\n9.0\n\n\n33\nElementary trades and related occupations\n91\n8.2\n8.4\n\n\n34\nElementary administration and service occupa...\n92\n4.5\n8.9\n\n\n\n\n\n\n\n\ndf2\n\n\n\n\n\n\n\n\nProfessional_occupations\nCode\nGPGmedian\nGPGmean\n\n\n\n\n0\nAll employees\n0\n15.5\n14.6\n\n\n1\nManagers, directors and senior officials\n1\n11.3\n15.3\n\n\n2\nProfessional occupations\n2\n9.1\n12.8\n\n\n3\nAssociate professional and technical occupations\n3\n12.9\n14.1\n\n\n4\nAdministrative and secretarial occupations\n4\n7.5\n9.5\n\n\n5\nSkilled trades occupations\n5\n22.2\n19.2\n\n\n6\nCaring, leisure and other service occupations\n6\n4.3\n7.9\n\n\n7\nSales and customer service occupations\n7\n2.5\n7.2\n\n\n8\nProcess, plant and machine operatives\n8\n14.6\n12.4\n\n\n9\nElementary occupations\n9\n5.3\n9.0\n\n\n\n\n\n\n\n\n26.3.1 Missing values\nLet’s check if we have any missing data\n\ndf.info()\ndf.isna().sum()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 35 entries, 0 to 34\nData columns (total 4 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   Professional_occupations  35 non-null     object \n 1   Code                      35 non-null     int64  \n 2   GPGmedian                 35 non-null     float64\n 3   GPGmean                   35 non-null     float64\ndtypes: float64(2), int64(1), object(1)\nmemory usage: 1.2+ KB\n\n\nProfessional_occupations    0\nCode                        0\nGPGmedian                   0\nGPGmean                     0\ndtype: int64\n\n\n\ndf.boxplot(column=['GPGmedian', 'GPGmean'])\n\n&lt;Axes: &gt;\n\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nCode\nGPGmedian\nGPGmean\n\n\n\n\ncount\n35.000000\n35.000000\n35.000000\n\n\nmean\n35.085714\n8.705714\n10.625714\n\n\nstd\n28.307525\n5.332031\n5.946395\n\n\nmin\n0.000000\n-4.100000\n-4.200000\n\n\n25%\n8.500000\n5.250000\n7.700000\n\n\n50%\n32.000000\n7.800000\n9.500000\n\n\n75%\n53.500000\n12.250000\n13.450000\n\n\nmax\n92.000000\n22.200000\n26.500000\n\n\n\n\n\n\n\n\ndf[['GPGmedian']].plot(kind='hist', ec='black')\n\n&lt;Axes: ylabel='Frequency'&gt;\n\n\n\n\n\n\ndf2.plot.bar(x = 'Professional_occupations', y = 'GPGmedian')\n\n&lt;Axes: xlabel='Professional_occupations'&gt;\n\n\n\n\n\n\ndf2.GPGmedian\n\n0    15.5\n1    11.3\n2     9.1\n3    12.9\n4     7.5\n5    22.2\n6     4.3\n7     2.5\n8    14.6\n9     5.3\nName: GPGmedian, dtype: float64\n\n\n\nimport pandas as pd\nimport numpy as np\n\n#By default the aggreggate function is mean\n\ndf.pivot_table(index=['Professional_occupations'], values=['GPGmean'])\n\n\n\n\n\n\n\n\nGPGmean\n\n\nProfessional_occupations\n\n\n\n\n\nAdministrative occupations\n8.2\n\n\nBusiness and public service associate professionals\n16.8\n\n\nBusiness, media and public service professionals\n11.9\n\n\nCaring personal service occupations\n6.6\n\n\nCorporate managers and directors\n12.6\n\n\nCulture, media and sports occupations\n23.2\n\n\nCustomer service occupations\n6.1\n\n\nElementary administration and service occupations\n8.9\n\n\nElementary trades and related occupations\n8.4\n\n\nHealth and social care associate professionals\n11.3\n\n\nHealth professionals\n26.5\n\n\nLeisure, travel and related personal service occupations\n15.7\n\n\nOther managers and proprietors\n12.3\n\n\nProcess, plant and machine operatives\n15.2\n\n\nProtective service occupations\n6.9\n\n\nSales occupations\n8.0\n\n\nScience, engineering and technology associate professionals\n11.3\n\n\nScience, research, engineering and technology professionals\n8.6\n\n\nSecretarial and related occupations\n1.1\n\n\nSkilled agricultural and related trades\n-4.2\n\n\nSkilled construction and building trades\n6.2\n\n\nSkilled metal, electrical and electronic trades\n7.5\n\n\nTeaching and educational professionals\n12.7\n\n\nTextiles, printing and other skilled trades\n8.4\n\n\nTransport and mobile machine drivers and operatives\n-0.3\n\n\nAdministrative and secretarial occupations\n9.5\n\n\nAll employees\n14.6\n\n\nAssociate professional and technical occupations\n14.1\n\n\nCaring, leisure and other service occupations\n7.9\n\n\nElementary occupations\n9.0\n\n\nManagers, directors and senior officials\n15.3\n\n\nProcess, plant and machine operatives\n12.4\n\n\nProfessional occupations\n12.8\n\n\nSales and customer service occupations\n7.2\n\n\nSkilled trades occupations\n19.2\n\n\n\n\n\n\n\n\n\n26.3.2 Interpreting the estimates\n“It should be noted that the figures do not show differences in rates of pay for comparable jobs, as these are affected by factors such as the proportion of men and women working part time or in different occupations. For example, a higher proportion of women work in occupations such as administration and caring, which tend to offer lower salaries.”"
  },
  {
    "objectID": "content/labs/Lab_7/IM939_Lab7-Simpsons_Paradox2.html",
    "href": "content/labs/Lab_7/IM939_Lab7-Simpsons_Paradox2.html",
    "title": "27  IM939 - Lab 7 Simpson’s Paradox",
    "section": "",
    "text": "28 5 In-depth Analysis - Outliers\nLet’s try to go a bit more in-depth . We know that gender doesn’t show many differences. In age there are not big differences except for one case. Let’s focus on ethnicity then.\nWe are going to use Seaborn’s ‘catplot’. In the documentation we can read what are the error bars here: “In seaborn, the barplot() function operates on a full dataset and applies a function to obtain the estimate (taking the mean by default). When there are multiple observations in each category, it also uses bootstrapping to compute a confidence interval around the estimate, which is plotted using error bars.”\nsns.catplot(y=\"Ethnicity\", x='Expenditures', \n            kind=\"bar\", data=df)\n\n#you can also run a nested table, but the chart might be more straightforward in analysis.\n#np.round(df.pivot_table(index=['cat_AgeCohort','Ethnicity'], values=['Expenditures']), 2)\nnp.round(df.pivot_table(index=['Ethnicity'], values=['Expenditures']), 2)\n\n\n\n\n\n\n\n\nExpenditures\n\n\nEthnicity\n\n\n\n\n\nAmerican Indian\n36438.25\n\n\nAsian\n18392.37\n\n\nBlack\n20884.59\n\n\nHispanic\n11065.57\n\n\nMulti Race\n4456.73\n\n\nNative Hawaiian\n42782.33\n\n\nOther\n3316.50\n\n\nWhite not Hispanic\n24697.55\nSo there are big differences in the averages between ethnicities. Does it mean there is discrimination?\ndf.groupby('Ethnicity').count()\n\n\n\n\n\n\n\n\nId\nAgeCohort\nAge\nGender\nExpenditures\ncat_AgeCohort\n\n\nEthnicity\n\n\n\n\n\n\n\n\n\n\nAmerican Indian\n4\n4\n4\n4\n4\n2\n\n\nAsian\n129\n129\n129\n129\n129\n108\n\n\nBlack\n59\n59\n59\n59\n59\n49\n\n\nHispanic\n376\n376\n376\n376\n376\n315\n\n\nMulti Race\n26\n26\n26\n26\n26\n19\n\n\nNative Hawaiian\n3\n3\n3\n3\n3\n2\n\n\nOther\n2\n2\n2\n2\n2\n2\n\n\nWhite not Hispanic\n401\n401\n401\n401\n401\n315\nAs you can see there are big sample size differences between ethnic groups.\nWhat conclusions does it bring? There are 3 major ethnicities within the dataset: White non-Hispanic (40%), Hispanic (38%), Asian (13%). The sample sizes of other ethnicites are very small.\nPlease also remember that 1). We know it is representative data of the population of residents. So based on this data we can use inferential statistics (look up Week 03 slides if you need a reminder) and estimate results for the whole population of beneficiaries of California DDS.\n2). Also, if you look into actual demographics of California State here\nYou will notce that the proportions of the state are similar to proportions of this case study. Hispanic and White non-Hispanic constitute a majority of California’s population.\nLet’s focus on the top 2 biggest groups. We can see there is a difference in the average expenditures between the White non-Hispanic and Hispanic groups.\n##selecting cases that are either 'Hispanic' or 'White non Hispanic' \nHispanic = df[(df[\"Ethnicity\"] == 'Hispanic') | (df[\"Ethnicity\"] == 'White not Hispanic')]\nHispanic\n\n\n\n\n\n\n\n\nId\nAgeCohort\nAge\nGender\nExpenditures\nEthnicity\ncat_AgeCohort\n\n\n\n\n0\n10210\n13-17\n17\nFemale\n2113\nWhite not Hispanic\n13-17\n\n\n1\n10409\n22-50\n37\nMale\n41924\nWhite not Hispanic\n22-50\n\n\n2\n10486\n0-5\n3\nMale\n1454\nHispanic\nNaN\n\n\n3\n10538\n18-21\n19\nFemale\n6400\nHispanic\n18-21\n\n\n4\n10568\n13-17\n13\nMale\n4412\nWhite not Hispanic\n13-17\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n992\n99114\n18-21\n18\nMale\n5298\nHispanic\n18-21\n\n\n995\n99622\n51 +\n86\nFemale\n57055\nWhite not Hispanic\nNaN\n\n\n996\n99715\n18-21\n20\nMale\n7494\nHispanic\n18-21\n\n\n998\n99791\n6-12\n10\nMale\n3638\nHispanic\n6-12\n\n\n999\n99898\n22-50\n23\nMale\n26702\nWhite not Hispanic\n22-50\n\n\n\n\n777 rows × 7 columns\nnp.round(Hispanic.pivot_table(index=['Ethnicity', 'cat_AgeCohort'], values=['Expenditures']), 2)\n\n\n\n\n\n\n\n\n\nExpenditures\n\n\nEthnicity\ncat_AgeCohort\n\n\n\n\n\nHispanic\n6-12\n2312.19\n\n\n13-17\n3955.28\n\n\n18-21\n9959.85\n\n\n22-50\n40924.12\n\n\nWhite not Hispanic\n6-12\n2052.26\n\n\n13-17\n3904.36\n\n\n18-21\n10133.06\n\n\n22-50\n40187.62\nsns.catplot(x=\"cat_AgeCohort\", y='Expenditures', hue=\"Ethnicity\", kind=\"bar\", data=Hispanic)\nLet’s get back to our original question : does discrimination exist in this case?\n“Is the typical Hispanic consumer receiving fewer funds (i.e., expenditures) than the typical White non-Hispanic consumer? If a Hispanic consumer was to file for discrimination based upon ethnicity, s/he would more than likely be asked his/her age. Since the typical amount of expenditures for Hispanics (in all but one age cohort) is higher than the typical amount of expenditures for White non-Hispanics in the respective age cohort, the discrimination claim would be refuted”.\nThis case study shows Simpson’s Paradox. You may ask: “Why is the overall average for all consumers significantly different indicating ethnic discrimination of Hispanics, yet in all but one age cohort (18-21) the average of expenditures for Hispanic consumers are greater than those of the White non-Hispanic population?” Look at the table below.\npd.crosstab([Hispanic.cat_AgeCohort],Hispanic.Ethnicity)\n\n\n\n\n\n\n\nEthnicity\nHispanic\nWhite not Hispanic\n\n\ncat_AgeCohort\n\n\n\n\n\n\n6-12\n91\n46\n\n\n13-17\n103\n67\n\n\n18-21\n78\n69\n\n\n22-50\n43\n133\nResults\n“There are more Hispanics in the youngest four age cohorts, while the White non-Hispanics have more consumers in the oldest two age cohorts. The two populations are close in overall counts (376 vs. 401). On top of this, consumers expenditures increase as they age to see the paradox.\nExpenditure average for Hispanic consumers are higher in all but one of the age cohorts, but the trend reverses when the groups are combined resulting in a lower expenditure average for all Hispanic consumers when compared to all White non-Hispanics.”\n“The overall Hispanic consumer population is a relatively younger when compared to the White non-Hispanic consumer population. Since the expenditures for younger consumers is lower, the overall average of expenditures for Hispanics (vs White non-Hispanics) is less.”\npd.crosstab(Hispanic.cat_AgeCohort,Hispanic.Ethnicity, \n            normalize='columns')\n\n# values=Hispanic.Ethnicity,aggfunc=sum,\n\n\n\n\n\n\n\nEthnicity\nHispanic\nWhite not Hispanic\n\n\ncat_AgeCohort\n\n\n\n\n\n\n6-12\n0.288889\n0.146032\n\n\n13-17\n0.326984\n0.212698\n\n\n18-21\n0.247619\n0.219048\n\n\n22-50\n0.136508\n0.422222"
  },
  {
    "objectID": "content/labs/Lab_7/IM939_Lab7-Simpsons_Paradox2.html#data",
    "href": "content/labs/Lab_7/IM939_Lab7-Simpsons_Paradox2.html#data",
    "title": "27  IM939 - Lab 7 Simpson’s Paradox",
    "section": "27.1 1 Data",
    "text": "27.1 1 Data\nHere are the key information from the dataset documentation file (every time I use “” below it is a cite from the dataset file). ### Abstract: “The State of California Department of Developmental Services (DDS) is responsible for allocating funds that support over 250,000 developmentally-disabled residents (e.g., intellectual disability, cerebral palsy, autism, etc.), called here also consumers. The dataset represents a sample of 1,000 of these consumers. Biographical characteristics and expenditure data (i.e., the dollar amount the State spends on each consumer in supporting these individuals and their families) are included in the data set for each consumer.\n\n27.1.1 Source:\nThe data set originates from DDS’s “Client Master File.” In order to remain in compliance with California State Legislation, the data have been altered to protect the rights and privacy of specific individual consumers. The data set is designed to represent a sample of 1,000 DDS consumers.\n\n\n27.1.2 Variable Descriptions:\nA header line contains the name of the variables. There are no missing values.\nId: 5-digit, unique identification code for each consumer (similar to a social security number)\nAge Cohort: Binned age variable represented as six age cohorts (0-5, 6-12, 13-17, 18-21, 22-50, and 51+)\nAge: Unbinned age variable\nGender: Male or Female\nExpenditures: Dollar amount of annual expenditures spent on each consumer\nEthnicity: Eight ethnic groups (American Indian, Asian, Black, Hispanic, Multi-race, Native Hawaiian, Other, and White non-Hispanic).\n\n\n27.1.3 Research problem\nThe data set and case study are based on a real-life scenario where there was a claim of discrimination based on ethnicity. The exercise highlights the importance of performing rigorous statistical analysis and how data interpretations can accurately inform or misguide decision makers.” (Taylor, Mickel 2014)"
  },
  {
    "objectID": "content/labs/Lab_7/IM939_Lab7-Simpsons_Paradox2.html#reading-the-dataset",
    "href": "content/labs/Lab_7/IM939_Lab7-Simpsons_Paradox2.html#reading-the-dataset",
    "title": "27  IM939 - Lab 7 Simpson’s Paradox",
    "section": "27.2 2 Reading the dataset",
    "text": "27.2 2 Reading the dataset\nYou should know the Pandas library already from the lab 1 with James. Here we are going to use it to explore the data and for pivot tables. In the folder you downloaded from the Moodle you have a dataset called ‘Lab 6 - Paradox Dataset’.\n\nimport pandas as pd\ndf = pd.read_excel('data/Paradox_Dataset.xlsx')\n\nA reminder: anything with a pd. prefix comes from pandas. This is particulary useful for preventing a module from overwriting inbuilt Python functionality.\nLet’s have a look at our dataset\n\ndf\n\n\n\n\n\n\n\n\nId\nAgeCohort\nAge\nGender\nExpenditures\nEthnicity\n\n\n\n\n0\n10210\n13-17\n17\nFemale\n2113\nWhite not Hispanic\n\n\n1\n10409\n22-50\n37\nMale\n41924\nWhite not Hispanic\n\n\n2\n10486\n0-5\n3\nMale\n1454\nHispanic\n\n\n3\n10538\n18-21\n19\nFemale\n6400\nHispanic\n\n\n4\n10568\n13-17\n13\nMale\n4412\nWhite not Hispanic\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n995\n99622\n51 +\n86\nFemale\n57055\nWhite not Hispanic\n\n\n996\n99715\n18-21\n20\nMale\n7494\nHispanic\n\n\n997\n99718\n13-17\n17\nFemale\n3673\nMulti Race\n\n\n998\n99791\n6-12\n10\nMale\n3638\nHispanic\n\n\n999\n99898\n22-50\n23\nMale\n26702\nWhite not Hispanic\n\n\n\n\n1000 rows × 6 columns\n\n\n\nWe have 6 columns (variables) in 1000 rows. Let’s see what type of object is our dataset and what types of objects are in the dataset.\n\ntype(df)\n\npandas.core.frame.DataFrame"
  },
  {
    "objectID": "content/labs/Lab_7/IM939_Lab7-Simpsons_Paradox2.html#exploring-data",
    "href": "content/labs/Lab_7/IM939_Lab7-Simpsons_Paradox2.html#exploring-data",
    "title": "27  IM939 - Lab 7 Simpson’s Paradox",
    "section": "27.3 3 Exploring data",
    "text": "27.3 3 Exploring data\n\n27.3.1 Missing values\nLet’s check if we have any missing data\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 6 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n 0   Id            1000 non-null   int64 \n 1   AgeCohort     1000 non-null   object\n 2   Age           1000 non-null   int64 \n 3   Gender        1000 non-null   object\n 4   Expenditures  1000 non-null   int64 \n 5   Ethnicity     1000 non-null   object\ndtypes: int64(3), object(3)\nmemory usage: 47.0+ KB\n\n\nThe above tables shows that we have 1000 observations for each of 6 columns.\nLet’s see if there are any unexpected values.\n\nimport numpy as np\nnp.unique(df.AgeCohort)\n\narray([' 0-5', ' 51 +', '13-17', '18-21', '22-50', '6-12'], dtype=object)\n\n\n\nnp.unique(df.Age)\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n       34, 35, 36, 37, 38, 39, 40, 41, 42, 44, 45, 46, 48, 51, 52, 53, 54,\n       55, 56, 57, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73,\n       74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 88, 89, 90, 91, 94,\n       95])\n\n\n\nnp.unique(df.Gender)\n\narray(['Female', 'Male'], dtype=object)\n\n\n\nnp.unique(df.Ethnicity)\n\narray(['American Indian', 'Asian', 'Black', 'Hispanic', 'Multi Race',\n       'Native Hawaiian', 'Other', 'White not Hispanic'], dtype=object)\n\n\nThere aren’t any unexpected values in neither of these 4 variables. We didn’t run this command for Expenditures on purpose, as this would return us too many values. An easier way to check this variable would be just a boxplot.\n\ndf.boxplot(column=['Age'])\n\n&lt;Axes: &gt;\n\n\n\n\n\n\ndf.boxplot(column=['Expenditures'])\n\n&lt;Axes: &gt;\n\n\n\n\n\nLet’s see a summary of data types we have here.\n\n\n27.3.2 Data types\n\ndf.dtypes\n\nId               int64\nAgeCohort       object\nAge              int64\nGender          object\nExpenditures     int64\nEthnicity       object\ndtype: object\n\n\nWe are creating a new categorical column cat_AgeCohort that would make our work a bit easier later. You can read more here\n\ndf['cat_AgeCohort'] = pd.Categorical(df['AgeCohort'], \n                                     ordered=True, \n                                     categories=['0-5', '6-12', '13-17', '18-21', '22-50', '51 +'])\n\nHere int64 mean ‘a 64-bit integer’ and ‘object’ are strings. This gives you also a hint they are different types of variables. The ‘bit’ refers to how long and precise the number is. Pandas uses data types from numpy (pandas documentation, numpy documentation). In our dataset three variables are numeric: Id, age are ordinal variables, Expenditures is a scale variable. AgeCohort is categorical and Gender and Ethnicity are nominal.\nFor that reason ‘data.describe’ will bring us a summary of numeric variables only.\n\ndf.describe()\n\n\n\n\n\n\n\n\nId\nAge\nExpenditures\n\n\n\n\ncount\n1000.000000\n1000.000000\n1000.000000\n\n\nmean\n54662.846000\n22.800000\n18065.786000\n\n\nstd\n25643.673401\n18.462038\n19542.830884\n\n\nmin\n10210.000000\n0.000000\n222.000000\n\n\n25%\n31808.750000\n12.000000\n2898.750000\n\n\n50%\n55384.500000\n18.000000\n7026.000000\n\n\n75%\n76134.750000\n26.000000\n37712.750000\n\n\nmax\n99898.000000\n95.000000\n75098.000000\n\n\n\n\n\n\n\nIt doesn’t make sense to plot not numeric variables or ids. That’s why we are going to just plot age and expenditures.\n\ndf.plot(x = 'Age', y = 'Expenditures', kind='scatter')\n\n&lt;Axes: xlabel='Age', ylabel='Expenditures'&gt;\n\n\n\n\n\nThe pattern of data is very interesting, expecially around x-values of ca. 25. The research paper can bring us more clarification.\n\n\n27.3.3 Age\nThe crucial factor in this case study is age: “As consumers get older, their financial needs increase as they move out of their parent’s home, etc. Therefore, it is expected that expenditures for older consumers will be higher than for the younger consumers”\nIn the dataset we have two age variables that both refer to the same information - age of consumers. They are saved as two distinct data types: binned ‘AgeCohort’ and unbinned ‘Age’.\nAge categories If you look at the binned one you will notice that the categories are somewhat interesting:\n\ndf[['Age']].plot(kind='hist', ec='black')\n\n&lt;Axes: ylabel='Frequency'&gt;\n\n\n\n\n\n\ndf[\"AgeCohort\"].describe() #we will receive the output for the categorical variable \"AgeCohort\"\ndf['cat_AgeCohort'].describe()\n\ncount       812\nunique        4\ntop       22-50\nfreq        226\nName: cat_AgeCohort, dtype: object\n\n\nHere we will run a bar plot of age categories.\n\ndf['cat_AgeCohort'].value_counts().plot(kind=\"bar\")\n\n&lt;Axes: xlabel='cat_AgeCohort'&gt;\n\n\n\n\n\nThe default order of plot elements is ‘value count’. For the age variable it might be more useful to look at the order chronologically.\n\n#using sns.countplot from seaborn we will plot AgeCohort\n#the order in plotting this variable is really crucial, we want to have sorted by age categories\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#here is without sorting / ordering\n#sns.countplot(x=\"AgeCohort\", data=df)\n\n#here we plot the variable with sorting\nsns.countplot(x=\"cat_AgeCohort\", data=df)\n\n\n\n#You can try playing with the commands below too:\n#sns.countplot(x=\"AgeCohort\", data=df, order=df['AgeCohort'].value_counts().index)\n#sns.countplot(x=\"AgeCohort\", data=df, order=['0-5', '6-12', '13-17', '18-21', '22-50','51+'])\n\n&lt;Axes: xlabel='cat_AgeCohort', ylabel='count'&gt;\n\n\n\n\n\nWhy would the data be binned in such “uneven” categories like ‘0-5 years’, ‘6-12’ and ‘22-50’? Instead of even categories e.g. ‘0-10’, ‘11-20’, ‘21-30’ etc. or every 5 years ‘0-5’, ‘6-10’ etc.?\nHere the age cohorts were allocated based on the theory, rather than based on data (this way we would have even number of people in each category) or based on logical age categories, e.g. every 5 or 10 years.\nThe authors explain: “The cohorts are established based on the amount of financial support typically required during a particular life phase (…) The 0-5 cohort (preschool age) has the fewest needs and requires the least amount of funding (…) Those in the 51+ cohort have the most needs and require the most amount of funding”. You can read in more details in the paper."
  },
  {
    "objectID": "content/labs/Lab_7/IM939_Lab7-Simpsons_Paradox2.html#exploratory-analysis",
    "href": "content/labs/Lab_7/IM939_Lab7-Simpsons_Paradox2.html#exploratory-analysis",
    "title": "27  IM939 - Lab 7 Simpson’s Paradox",
    "section": "27.4 4 Exploratory analysis",
    "text": "27.4 4 Exploratory analysis\nThe research question is: are any demographics discriminated in distributions of the funds?\nFollowing the authors: “Discrimination exists in this case study if the amount of expenditures for a typical person in a group of consumers that share a common attribute (e.g., gender, ethnicity, etc.) is significantly different when compared to a typical person in another group. For example, discrimination based on gender would occur if the expenditures for a typical female are less than the amount for a typical male.”\nWe are going to examine the data using plots for categorical data and pivot tables (cross-tables) with means. “Pivot table reports are particularly useful in narrowing down larger data sets or analyzing relationships between data points.” Pivot tables will help you understand what is Simpson’s Paradox.\n\n27.4.1 Age x expenditures\nLet’s see how expenditures are distributed across age groups.\nWe are going to use a swarm plot which I believe works well here to notice the paradox and “the points are adjusted (only along the categorical axis) so that they don’t overlap. This gives a better representation of the distribution of values, but it does not scale well to large numbers of observations. A swarm plot can be drawn on its own, but it is also a good complement to a box or violin plot in cases where you want to show all observations along with some representation of the underlying distribution.” Read more here\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.catplot(x=\"AgeCohort\", y=\"Expenditures\", kind=\"swarm\", data=df)\n#you can also do a boxplot if you change kind=\"box\"\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 83.5% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 35.4% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 76.8% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 58.8% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 14.2% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 84.0% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 85.8% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 42.5% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 80.5% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 64.8% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 21.7% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 86.9% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n\n\n\n\n\n27.4.2 Ethnicity\nEthnicity could be another discriminating factor. Let’s check this here too by plotting expenditures by ethnicity.\nThese groups reflect the demographic profile of the State of California.\n\nsns.catplot(y=\"Ethnicity\", x=\"Expenditures\", kind=\"swarm\", data=df)\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 50.1% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 65.2% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 11.9% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 11.5% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 25.6% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 62.6% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 71.3% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 15.3% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 34.6% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 36.4% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n\n\n\n\n#g=sns.catplot(y=\"Ethnicity\", x=\"Expenditures\", hue='Age', kind=\"swarm\", data=df)\n#g._legend.remove()\n\nsns.catplot(y=\"Ethnicity\", x=\"Expenditures\", hue='cat_AgeCohort', kind=\"swarm\", data=df)\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 39.0% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 58.1% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 17.6% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 53.3% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 65.4% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 10.2% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 5.3% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 28.7% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 46.3% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 61.9% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 6.1% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 22.2% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 56.2% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 68.3% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 12.2% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 10.5% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n\n\n\n\n\n27.4.3 Gender\nGender could have been another discriminating factor (as gender based discrimination is also very common). It is not the case here. See below plots to confirm these. We are plotting expenditures by gender.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#sns.catplot(x=\"Gender\", y=\"Expenditures\", kind=\"swarm\", data=df)\n#you can create even a nicer plot than for ethnicity, using tips here https://seaborn.pydata.org/tutorial/categorical.html\n#It's a combination of swarmplot and violin plot to show each observation along with a summary of the distribution\n\ng = sns.catplot(x=\"Gender\", y=\"Expenditures\", kind=\"violin\", inner=None, data=df)\nsns.swarmplot(x=\"Gender\", y=\"Expenditures\", color=\"k\", size=3, data=df, ax=g.ax)\n\n&lt;Axes: xlabel='Gender', ylabel='Expenditures'&gt;\n\n\n/Users/u2071219/anaconda3/envs/IM939/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 7.0% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n\n\n\n\n\n27.4.4 Mean Expenditures\nThis was a quick visual analysis. Let’s check means to see how it looks like by age, ethnicity and gender. Why would it be also good to check medians here?\n\nimport pandas as pd\nimport numpy as np\n\n#By default the aggreggate function is mean\n\nnp.round(df.pivot_table(index=['Ethnicity'], values=['Expenditures']), 2)\n\n\n\n\n\n\n\n\nExpenditures\n\n\nEthnicity\n\n\n\n\n\nAmerican Indian\n36438.25\n\n\nAsian\n18392.37\n\n\nBlack\n20884.59\n\n\nHispanic\n11065.57\n\n\nMulti Race\n4456.73\n\n\nNative Hawaiian\n42782.33\n\n\nOther\n3316.50\n\n\nWhite not Hispanic\n24697.55\n\n\n\n\n\n\n\n\nnp.round(df.pivot_table(index=['Gender'], values=['Expenditures']), 2)\n\n\n\n\n\n\n\n\nExpenditures\n\n\nGender\n\n\n\n\n\nFemale\n18129.61\n\n\nMale\n18001.20\n\n\n\n\n\n\n\n\nnp.round(df.pivot_table(index=['cat_AgeCohort'], values=['Expenditures']), 2)\n\n\n\n\n\n\n\n\nExpenditures\n\n\ncat_AgeCohort\n\n\n\n\n\n6-12\n2226.86\n\n\n13-17\n3922.61\n\n\n18-21\n9888.54\n\n\n22-50\n40209.28\n\n\n\n\n\n\n\nWhat do these tables tell us? There is much discrepnacy in average results for ethnicity and age cohort. If we look at gender - there aren’t many differences.\nPlease remember that in this case study “the needs for consumers increase as they become older which results in higher expenditures”. This would explain age discrepancies a bit, but what about ethnicity?"
  },
  {
    "objectID": "content/labs/Lab_7/IM939_Lab7-Simpsons_Paradox2.html#explanation",
    "href": "content/labs/Lab_7/IM939_Lab7-Simpsons_Paradox2.html#explanation",
    "title": "27  IM939 - Lab 7 Simpson’s Paradox",
    "section": "29.1 Explanation",
    "text": "29.1 Explanation\n“This exercise is based on a real-life case in California. The situation involved an alleged case of discrimination privileging White non-Hispanics over Hispanics in the allocation of funds to over 250,000 developmentally-disabled California residents.\nA number of years ago, an allegation of discrimination was made and supported by a univariate analysis that examined average annual expenditures on consumers by ethnicity. The analysis revealed that the average annual expenditures on Hispanic consumers was approximately one-third (⅓) of the average expenditures on White non-Hispanic consumers. (…) A bivariate analysis examining ethnicity and age (divided into six age cohorts) revealed that ethnic discrimination did not exist. Moreover, in all but one of the age cohorts, the trend reversed where the average annual expenditures on White non-Hispanic consumers were less than the expenditures on Hispanic consumers.”(Taylor, Mickel 2014)\nWhen running the simple table with aggregated data, the discrimination in this case appared evident. After running a few more detailed tables, it appears to be no evidence of discrimination based on this sample and the variables collected."
  },
  {
    "objectID": "content/labs/Lab_7/IM939_Lab7-Simpsons_Paradox2.html#takeaways",
    "href": "content/labs/Lab_7/IM939_Lab7-Simpsons_Paradox2.html#takeaways",
    "title": "27  IM939 - Lab 7 Simpson’s Paradox",
    "section": "29.2 Takeaways",
    "text": "29.2 Takeaways\nThe example above concerns a crucial topic of discrimination. As you can see, data and statistics alone won’t give us the anwser. First results might give us a confusing result. Critical thinking is essential when working with data, in order to account for reasons not evident at the first sight. The authors remind us the following: 1) “outcome of important decisions (such as discrimination claims) are often heavily influenced by statistics and how an incomplete analysis may lead to poor decision making” 2) “importance of identifying and analyzing all sources of specific variation (i.e., potential influential factors) in statistical analyses”. This is something we already discussed in previous weeks, but it is never enough to stress it out”\n\n29.2.1 *Additional Links\nSome links regarding categorical data in Python for those interested:\nhttps://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html#description\nhttps://pandas.pydata.org/pandas-docs/version/0.23.1/generated/pandas.DataFrame.plot.bar.html\nhttps://seaborn.pydata.org/tutorial/categorical.html\nhttps://seaborn.pydata.org/generated/seaborn.countplot.html"
  },
  {
    "objectID": "content/references.html",
    "href": "content/references.html",
    "title": "References",
    "section": "",
    "text": "Calero Valdez, André, Martina Ziefle, and Michael Sedlmair. 2018.\n“Studying Biases in Visualization\nResearch: Framework and\nMethods.” In Cognitive Biases in\nVisualizations, edited by Geoffrey Ellis, 13–27. Cham:\nSpringer International Publishing. https://doi.org/10.1007/978-3-319-95831-6_2.\n\n\nFisher, R. A. 1936. “Iris.” UCI Machine Learning\nRepository. https://doi.org/10.24432/C56C76.\n\n\nIgual, Laura, and Santi Seguí. 2017. “Regression\nAnalysis.” In Introduction to Data\nScience: A Python\nApproach to Concepts, Techniques\nand Applications, 97–114. Cham: Springer International\nPublishing. https://doi.org/10.1007/978-3-319-50017-1_6.\n\n\nRedmond, Michael. 2009. “Communities and\nCrime.” UCI Machine Learning Repository. https://doi.org/10.24432/C53W3X.\n\n\nRouncefield, Mary. 1995. “The Statistics of\nPoverty and Inequality.” Journal of\nStatistics Education 3 (2): 8. https://doi.org/10.1080/10691898.1995.11910491."
  },
  {
    "objectID": "README_setup.html#setting-up-the-environment",
    "href": "README_setup.html#setting-up-the-environment",
    "title": "Appendix A — Setup & Usage",
    "section": "A.1 Setting up the environment",
    "text": "A.1 Setting up the environment\nVirtual environments are a way to install all the dependencies (and their right version) required for a certain project by isolating python and libraries’ specific versions. Every person who recreatesthe virtual environment will be using the same packages and versions, reducing errors and increasing reproducibility.\nThis project, uses a conda environment called IM939, which will install all the packages and versions (as well as their dependencies) as defined in the file environment.yml at the root of this project.\n\nA.1.1 Installing Anaconda\nYou will need to install Anaconda distribution your machine if it is not already installed. You can run the following command to see if it is already present in your system:\nconda --help\nIf you get a command not found error, you will need to install Anaconda following these instructions on their website).\n\n\nA.1.2 Creating the virtual environment\n\n\n\n\n\n\nNote\n\n\n\nYou only need to do this once in the same machine. After the virtual environment is created we can always update it to follow any change in the file environment.yml\n\n\nTo recreate the virtual environment from environment.yml, run the following command:\nconda env create -f environment.yml\nor, if we want to install the environment within the project:\nconda env create --prefix env -f environment.yml\n\n\nA.1.3 Activating the virtual environment\nOnce the environment has been created, it needs to be activated by typing the following command\nconda activate IM939\nor, if it is stored in env/ folder:\nconda activate env/\n\n\n\n\n\n\nOnce per session\n\n\n\nYou will need to activate the environment every time you open your editor anew.\n\n\nDeactivate virtual environment:\nIf you want, you can always deactivate your environment (and, actually open the default one, called base) by running:\nconda deactivate\nUpdate virtual environment from environment.yml:\nconda env update -f environment.yml\nFreeze used dependencies into a file\nWe can create a file (in this case environment.yml) containing the exact libraries and versions used in the current environment. This can be useful to update the versions used in the environment in the future.\nconda env export &gt; environment.yml"
  },
  {
    "objectID": "README_setup.html#recreating-the-handbook",
    "href": "README_setup.html#recreating-the-handbook",
    "title": "Appendix A — Setup & Usage",
    "section": "A.2 Recreating the handbook",
    "text": "A.2 Recreating the handbook\n\n\n\n\n\n\nTip\n\n\n\nQuarto has extensive documentation at their website (specifically in this page about authoring document and this other on managing books).\n\n\nThe workflow can be summarised as follows (detailed instructions below):\n\nCreate new content or edit existing one\n\nCreate a .md, .iypnb or .qmd file and put it in the /content/ folder\nAdd an entry to to the table of contents in _quarto.yml (more info in quarto\n\nEdit existing content in /content/ folder\nRender book locally to see changes\nCommit & Push changes to the source documents in /content/ folder\ngit commit -a -m \"My fancy message\"\ngit push origin main\nPublish book online\n\n\nA.2.1 Rendering the book locally:\nRendering the book will convert jupyternotebooks, qmd files or md files listed in _quarto.yml’s table of contents into a a book, applying the styles and configurations from _quarto.yml. Do this to preview how your changes would look like as a book. From the repo’s root, run:\nquarto render\n\n\n\n\n\n\nFirst time render\n\n\n\nOn its first run, this command will take several minutes to process. This is because quarto will run and execute the computations in every cell within every jupyter notebook, some of which are really time consuming. The good news, is that quarto will create a cached version of it (stored in the /_freeze/ folder) , which means that further runs of quarto render will not need to execute the cells again (unless the original jupyternotebook is changed or the corresponding folder is deleted).\nIf you want the cache to be regenerated:\nquarto render --cache-refresh\n\n\n\n\nA.2.2 Publishing book to github pages\nThis book is published using GitHub pages, and assumes that your rendered book will be located on a dedicated branch called gh-pages which needs to be created in the repo if not present already. Also, the repository needs to be configured as to use that branch’s root for publishing a GithubPage.\nOnce gh-pagesbranch has been created, run the following command:\nquarto publish gh-pages   \nThis command will render the book in the branch gh-pages and will push it to the corresponding branch in our repo and then checking out again to the previous branch (usually, main). More info about it here: https://quarto.org/docs/publishing/github-pages.html\n\n\n\n\n\n\nOther useful resources\n\n\n\n\nNotebook embedding: https://quarto.org/docs/authoring/notebook-embed.html"
  },
  {
    "objectID": "content/files-and-folders.html",
    "href": "content/files-and-folders.html",
    "title": "Appendix B — Working with files and directories",
    "section": "",
    "text": "This is a placeholder\n# SETTING THE WORKING DIRECTORY: if you have not downloaded both the .ipynb file and the Data folder into the same \n# location, then put the path to the folder containing all of your data files (necessary for this lab) inside \n# the os.chdir() function as a STRING\nos.chdir(os.path.join(os.getcwd(), 'data')) \n# READ COMMENT ABOVE or Download the Data folder as well into the same folder as this notebook"
  }
]