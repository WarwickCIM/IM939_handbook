{
  "hash": "fad5cac4334164b40a7b36e3615443ae",
  "result": {
    "markdown": "# Lab: Dimension Reduction\n\nSo far, we have worked with relatively small data sets, but often times, datasets may have a high number of dimensions. In those cases, appliying the methods we have covered in past sessions (i.e., visual exploration using data visualisations, correlations between two variables...) may be really difficult, if not impossible to do.\n\nIn this notebook we are going to explore two different techniques for reducing the dimensionality of our data: Clustering and Principle Component Analysis (PCA).\n\n## Data preparations\n\nIn this notebook, we are going to use the simple [Iris flower dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set) [@r_a_fisher_iris_1936]. The dataset, which is famously used to introduce these methods, consists of 4 measures or attributes (the length and the width of the sepals and petals, in centimeters) describing 50 samples from three species of flowers (_Iris setosa_, _Iris virginica_ and _Iris versicolor_). \n\n::: aside\n![ Image of a primrose willowherb ''Ludwigia octovalvis'' (Family Onagraceae), flower showing petals and sepals. Photograph made in Hawai'i by Eric Guinther and released under the GNU Free Documentation License.](media/451px-Petal-sepal.jpg)\n\n:::\n\nContrary to previous sessions, in this case, the dataset will not be read from a `csv` file, but it is provided by the `sklearn.datasets` submodule:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.datasets import load_iris\nimport numpy as np\n\niris = load_iris()\n\n# Explore our data.\niris\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\n{'data': array([[5.1, 3.5, 1.4, 0.2],\n        [4.9, 3. , 1.4, 0.2],\n        [4.7, 3.2, 1.3, 0.2],\n        [4.6, 3.1, 1.5, 0.2],\n        [5. , 3.6, 1.4, 0.2],\n        [5.4, 3.9, 1.7, 0.4],\n        [4.6, 3.4, 1.4, 0.3],\n        [5. , 3.4, 1.5, 0.2],\n        [4.4, 2.9, 1.4, 0.2],\n        [4.9, 3.1, 1.5, 0.1],\n        [5.4, 3.7, 1.5, 0.2],\n        [4.8, 3.4, 1.6, 0.2],\n        [4.8, 3. , 1.4, 0.1],\n        [4.3, 3. , 1.1, 0.1],\n        [5.8, 4. , 1.2, 0.2],\n        [5.7, 4.4, 1.5, 0.4],\n        [5.4, 3.9, 1.3, 0.4],\n        [5.1, 3.5, 1.4, 0.3],\n        [5.7, 3.8, 1.7, 0.3],\n        [5.1, 3.8, 1.5, 0.3],\n        [5.4, 3.4, 1.7, 0.2],\n        [5.1, 3.7, 1.5, 0.4],\n        [4.6, 3.6, 1. , 0.2],\n        [5.1, 3.3, 1.7, 0.5],\n        [4.8, 3.4, 1.9, 0.2],\n        [5. , 3. , 1.6, 0.2],\n        [5. , 3.4, 1.6, 0.4],\n        [5.2, 3.5, 1.5, 0.2],\n        [5.2, 3.4, 1.4, 0.2],\n        [4.7, 3.2, 1.6, 0.2],\n        [4.8, 3.1, 1.6, 0.2],\n        [5.4, 3.4, 1.5, 0.4],\n        [5.2, 4.1, 1.5, 0.1],\n        [5.5, 4.2, 1.4, 0.2],\n        [4.9, 3.1, 1.5, 0.2],\n        [5. , 3.2, 1.2, 0.2],\n        [5.5, 3.5, 1.3, 0.2],\n        [4.9, 3.6, 1.4, 0.1],\n        [4.4, 3. , 1.3, 0.2],\n        [5.1, 3.4, 1.5, 0.2],\n        [5. , 3.5, 1.3, 0.3],\n        [4.5, 2.3, 1.3, 0.3],\n        [4.4, 3.2, 1.3, 0.2],\n        [5. , 3.5, 1.6, 0.6],\n        [5.1, 3.8, 1.9, 0.4],\n        [4.8, 3. , 1.4, 0.3],\n        [5.1, 3.8, 1.6, 0.2],\n        [4.6, 3.2, 1.4, 0.2],\n        [5.3, 3.7, 1.5, 0.2],\n        [5. , 3.3, 1.4, 0.2],\n        [7. , 3.2, 4.7, 1.4],\n        [6.4, 3.2, 4.5, 1.5],\n        [6.9, 3.1, 4.9, 1.5],\n        [5.5, 2.3, 4. , 1.3],\n        [6.5, 2.8, 4.6, 1.5],\n        [5.7, 2.8, 4.5, 1.3],\n        [6.3, 3.3, 4.7, 1.6],\n        [4.9, 2.4, 3.3, 1. ],\n        [6.6, 2.9, 4.6, 1.3],\n        [5.2, 2.7, 3.9, 1.4],\n        [5. , 2. , 3.5, 1. ],\n        [5.9, 3. , 4.2, 1.5],\n        [6. , 2.2, 4. , 1. ],\n        [6.1, 2.9, 4.7, 1.4],\n        [5.6, 2.9, 3.6, 1.3],\n        [6.7, 3.1, 4.4, 1.4],\n        [5.6, 3. , 4.5, 1.5],\n        [5.8, 2.7, 4.1, 1. ],\n        [6.2, 2.2, 4.5, 1.5],\n        [5.6, 2.5, 3.9, 1.1],\n        [5.9, 3.2, 4.8, 1.8],\n        [6.1, 2.8, 4. , 1.3],\n        [6.3, 2.5, 4.9, 1.5],\n        [6.1, 2.8, 4.7, 1.2],\n        [6.4, 2.9, 4.3, 1.3],\n        [6.6, 3. , 4.4, 1.4],\n        [6.8, 2.8, 4.8, 1.4],\n        [6.7, 3. , 5. , 1.7],\n        [6. , 2.9, 4.5, 1.5],\n        [5.7, 2.6, 3.5, 1. ],\n        [5.5, 2.4, 3.8, 1.1],\n        [5.5, 2.4, 3.7, 1. ],\n        [5.8, 2.7, 3.9, 1.2],\n        [6. , 2.7, 5.1, 1.6],\n        [5.4, 3. , 4.5, 1.5],\n        [6. , 3.4, 4.5, 1.6],\n        [6.7, 3.1, 4.7, 1.5],\n        [6.3, 2.3, 4.4, 1.3],\n        [5.6, 3. , 4.1, 1.3],\n        [5.5, 2.5, 4. , 1.3],\n        [5.5, 2.6, 4.4, 1.2],\n        [6.1, 3. , 4.6, 1.4],\n        [5.8, 2.6, 4. , 1.2],\n        [5. , 2.3, 3.3, 1. ],\n        [5.6, 2.7, 4.2, 1.3],\n        [5.7, 3. , 4.2, 1.2],\n        [5.7, 2.9, 4.2, 1.3],\n        [6.2, 2.9, 4.3, 1.3],\n        [5.1, 2.5, 3. , 1.1],\n        [5.7, 2.8, 4.1, 1.3],\n        [6.3, 3.3, 6. , 2.5],\n        [5.8, 2.7, 5.1, 1.9],\n        [7.1, 3. , 5.9, 2.1],\n        [6.3, 2.9, 5.6, 1.8],\n        [6.5, 3. , 5.8, 2.2],\n        [7.6, 3. , 6.6, 2.1],\n        [4.9, 2.5, 4.5, 1.7],\n        [7.3, 2.9, 6.3, 1.8],\n        [6.7, 2.5, 5.8, 1.8],\n        [7.2, 3.6, 6.1, 2.5],\n        [6.5, 3.2, 5.1, 2. ],\n        [6.4, 2.7, 5.3, 1.9],\n        [6.8, 3. , 5.5, 2.1],\n        [5.7, 2.5, 5. , 2. ],\n        [5.8, 2.8, 5.1, 2.4],\n        [6.4, 3.2, 5.3, 2.3],\n        [6.5, 3. , 5.5, 1.8],\n        [7.7, 3.8, 6.7, 2.2],\n        [7.7, 2.6, 6.9, 2.3],\n        [6. , 2.2, 5. , 1.5],\n        [6.9, 3.2, 5.7, 2.3],\n        [5.6, 2.8, 4.9, 2. ],\n        [7.7, 2.8, 6.7, 2. ],\n        [6.3, 2.7, 4.9, 1.8],\n        [6.7, 3.3, 5.7, 2.1],\n        [7.2, 3.2, 6. , 1.8],\n        [6.2, 2.8, 4.8, 1.8],\n        [6.1, 3. , 4.9, 1.8],\n        [6.4, 2.8, 5.6, 2.1],\n        [7.2, 3. , 5.8, 1.6],\n        [7.4, 2.8, 6.1, 1.9],\n        [7.9, 3.8, 6.4, 2. ],\n        [6.4, 2.8, 5.6, 2.2],\n        [6.3, 2.8, 5.1, 1.5],\n        [6.1, 2.6, 5.6, 1.4],\n        [7.7, 3. , 6.1, 2.3],\n        [6.3, 3.4, 5.6, 2.4],\n        [6.4, 3.1, 5.5, 1.8],\n        [6. , 3. , 4.8, 1.8],\n        [6.9, 3.1, 5.4, 2.1],\n        [6.7, 3.1, 5.6, 2.4],\n        [6.9, 3.1, 5.1, 2.3],\n        [5.8, 2.7, 5.1, 1.9],\n        [6.8, 3.2, 5.9, 2.3],\n        [6.7, 3.3, 5.7, 2.5],\n        [6.7, 3. , 5.2, 2.3],\n        [6.3, 2.5, 5. , 1.9],\n        [6.5, 3. , 5.2, 2. ],\n        [6.2, 3.4, 5.4, 2.3],\n        [5.9, 3. , 5.1, 1.8]]),\n 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n 'frame': None,\n 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n|details-start|\\n**References**\\n|details-split|\\n\\n- Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n  Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n  Mathematical Statistics\" (John Wiley, NY, 1950).\\n- Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n  (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n- Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n  Structure and Classification Rule for Recognition in Partially Exposed\\n  Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n  Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n- Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n  on Information Theory, May 1972, 431-433.\\n- See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n  conceptual clustering system finds 3 classes in the data.\\n- Many, many more ...\\n\\n|details-end|',\n 'feature_names': ['sepal length (cm)',\n  'sepal width (cm)',\n  'petal length (cm)',\n  'petal width (cm)'],\n 'filename': 'iris.csv',\n 'data_module': 'sklearn.datasets.data'}\n```\n:::\n:::\n\n\nRegretfully, the iris dataset is not a data frame as the previous ones:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ntype(iris)\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nsklearn.utils._bunch.Bunch\n```\n:::\n:::\n\n\nThis means that, if we want to apply all the methods that we are familiar with, we will need to convert this odd data type into a pandas dataframe format we know and love. We can do this following [this stackoverflow answer](https://stackoverflow.com/questions/38105539/how-to-convert-a-scikit-learn-dataset-to-a-pandas-dataset):\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport pandas as pd\n\niris_df = pd.DataFrame(iris.data, columns = iris.feature_names)\niris_df.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal length (cm)</th>\n      <th>sepal width (cm)</th>\n      <th>petal length (cm)</th>\n      <th>petal width (cm)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nAs usual, we may want to see some descriptive measures to get a sense of the data:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\niris_df.describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal length (cm)</th>\n      <th>sepal width (cm)</th>\n      <th>petal length (cm)</th>\n      <th>petal width (cm)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>150.000000</td>\n      <td>150.000000</td>\n      <td>150.000000</td>\n      <td>150.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>5.843333</td>\n      <td>3.057333</td>\n      <td>3.758000</td>\n      <td>1.199333</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.828066</td>\n      <td>0.435866</td>\n      <td>1.765298</td>\n      <td>0.762238</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>4.300000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>0.100000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>5.100000</td>\n      <td>2.800000</td>\n      <td>1.600000</td>\n      <td>0.300000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>5.800000</td>\n      <td>3.000000</td>\n      <td>4.350000</td>\n      <td>1.300000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>6.400000</td>\n      <td>3.300000</td>\n      <td>5.100000</td>\n      <td>1.800000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>7.900000</td>\n      <td>4.400000</td>\n      <td>6.900000</td>\n      <td>2.500000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n\n::: {.callout-warning collapse=\"true\"}\n\n### What is this telling us?\n\nAs can be seen from above, we do not have any missing data (all variables have 150 observations) but the scale (ranges) for every variable differ considerably (look the min and max values of sepal length and compare them with petal width).\n:::\n\n### Normalisation\n\nSome algorithms are senstive to the size of variables. For example, if the sepal widths were in meters and the other variables in cm then an algorithm may underweight sepal widths. This means that we will need to rescale our variables to be in the safe side. There are two ways to put variables into the same scale: normalisation and standarisation.\n\n- **Normalisation:** rescales a dataset so that each **value** falls between 0 and 1. It uses the following formula to do so:\n\n$$xnew = (xi – xmin) / (xmax – xmin)$$\n\n- **Standarisation:** rescales a dataset to have a **mean** of 0 and a **standard deviation** of 1. It uses the following formula to do so: \n\n$$xnew = (xi – x) / s$$\n\n::: callout-caution\n\n### Which one should we use?\n\nIf you cannot choose between them then try it both ways. You could compare the result with your raw data, the normalised data and the standardised data.\n\nThese blog posts may help you: [(Statology) Standardization vs. Normalization: What’s the Difference?](https://www.statology.org/standardization-vs-normalization/) and [(Analytics Vidhya) Feature Engineering: Scaling, Normalization, and Standardization (Updated 2023)](https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/)).\n\n:::\n\n\nIn the code below we will normalise the data between `0` and `1`  by using `.fit_transform()` from sklearn.\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import MinMaxScaler\ncol_names = iris_df.columns\niris_df =  pd.DataFrame(MinMaxScaler().fit_transform(iris_df))\niris_df.columns = col_names # Column names were lost, so we need to re-introduce\niris_df\n\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal length (cm)</th>\n      <th>sepal width (cm)</th>\n      <th>petal length (cm)</th>\n      <th>petal width (cm)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.222222</td>\n      <td>0.625000</td>\n      <td>0.067797</td>\n      <td>0.041667</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.166667</td>\n      <td>0.416667</td>\n      <td>0.067797</td>\n      <td>0.041667</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.111111</td>\n      <td>0.500000</td>\n      <td>0.050847</td>\n      <td>0.041667</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.083333</td>\n      <td>0.458333</td>\n      <td>0.084746</td>\n      <td>0.041667</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.194444</td>\n      <td>0.666667</td>\n      <td>0.067797</td>\n      <td>0.041667</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>0.666667</td>\n      <td>0.416667</td>\n      <td>0.711864</td>\n      <td>0.916667</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>0.555556</td>\n      <td>0.208333</td>\n      <td>0.677966</td>\n      <td>0.750000</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>0.611111</td>\n      <td>0.416667</td>\n      <td>0.711864</td>\n      <td>0.791667</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>0.527778</td>\n      <td>0.583333</td>\n      <td>0.745763</td>\n      <td>0.916667</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>0.444444</td>\n      <td>0.416667</td>\n      <td>0.694915</td>\n      <td>0.708333</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 4 columns</p>\n</div>\n```\n:::\n:::\n\n\nLet's see how the the descriptive measures have changed after the transformation:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\niris_df.describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal length (cm)</th>\n      <th>sepal width (cm)</th>\n      <th>petal length (cm)</th>\n      <th>petal width (cm)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>150.000000</td>\n      <td>150.000000</td>\n      <td>150.000000</td>\n      <td>150.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.428704</td>\n      <td>0.440556</td>\n      <td>0.467458</td>\n      <td>0.458056</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.230018</td>\n      <td>0.181611</td>\n      <td>0.299203</td>\n      <td>0.317599</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.222222</td>\n      <td>0.333333</td>\n      <td>0.101695</td>\n      <td>0.083333</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.416667</td>\n      <td>0.416667</td>\n      <td>0.567797</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.583333</td>\n      <td>0.541667</td>\n      <td>0.694915</td>\n      <td>0.708333</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nGreat.\n\nOur dataset show us the length and width of both the sepal (leaf) and petals of 150 plants. The dataset is quite famous and you can find a [wikipedia page](https://en.wikipedia.org/wiki/Iris_flower_data_set) with details of the dataset.\n\n::: callout-caution\n\n### Questions\n\nTo motivate our exploration of the data, consider the sorts of questions we can ask:\n\n* Are all our plants from the same species?\n* Do some plants have similiar leaf and petal sizes?\n* Can we differentiate between the plants using all 4 variables (dimensions)?\n* Do we need to include both length and width, or can we reduce these dimensions and simplify our analysis?\n\n:::\n\n### Initial exploration\n\nWe can explore a dataset with few variables using plots. \n\n\n#### Distribution (variabilty and density)\n\nWe'd like to see each variable's distributions in terms of variablity and density. We have seen several ways to do this, but in this case we will be using a new plot type (a [violin plot](https://en.wikipedia.org/wiki/Violin_plot)) to visualise the distribution. \n\nTo do so, we will be using seaborn's [`violinplot()`](https://seaborn.pydata.org/generated/seaborn.violinplot.html). Because we want to create a single plot with a violin plot per variable, we will need to transform our data from wide to a long format.\n\n::: aside\n\n![Explanation of Violin plot. ([Chambers 2017](http://dx.doi.org/10.13140/RG.2.2.26587.62244))](media/Explanation-of-Violin-plot-Densities-are-estimated-using-a-Gaussian-kernel-density.png) \n\n:::\n\n\n::: {.callout-tip collapsed=false}\n\n### Wide vs long data\n\nA dataset can be written in two different formats: wide and long.\n\n- **wide**: every row is a unique observation, where columns are variables (or attributes) describing the observation.\n- **long**: single observations are split into multiple rows. Usually, the first column contains the index to the observation, and there are two more columns: the name of the variable, and the actual value of the variable.\n\n![Wide vs long data (Source: [Statology](https://www.statology.org/long-vs-wide-data/))](media/wideLong1-1-768x543.png)\n\n:::\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nimport seaborn as sns\n\n# some plots require a long dataframe structure\niris_df_long = iris_df.melt()\niris_df_long\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>variable</th>\n      <th>value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sepal length (cm)</td>\n      <td>0.222222</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sepal length (cm)</td>\n      <td>0.166667</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sepal length (cm)</td>\n      <td>0.111111</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sepal length (cm)</td>\n      <td>0.083333</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sepal length (cm)</td>\n      <td>0.194444</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>595</th>\n      <td>petal width (cm)</td>\n      <td>0.916667</td>\n    </tr>\n    <tr>\n      <th>596</th>\n      <td>petal width (cm)</td>\n      <td>0.750000</td>\n    </tr>\n    <tr>\n      <th>597</th>\n      <td>petal width (cm)</td>\n      <td>0.791667</td>\n    </tr>\n    <tr>\n      <th>598</th>\n      <td>petal width (cm)</td>\n      <td>0.916667</td>\n    </tr>\n    <tr>\n      <th>599</th>\n      <td>petal width (cm)</td>\n      <td>0.708333</td>\n    </tr>\n  </tbody>\n</table>\n<p>600 rows × 2 columns</p>\n</div>\n```\n:::\n:::\n\n\nAnd now that we have transformed our data into a long data format, we can create the visualisation:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nsns.violinplot(data = iris_df_long, x = 'variable', y = 'value')\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n<Axes: xlabel='variable', ylabel='value'>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_4_1_Iris_files/figure-html/cell-9-output-2.png){width=600 height=429}\n:::\n:::\n\n\n#### Correlations\n\nThe below plots use the wide data structure.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\niris_df\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal length (cm)</th>\n      <th>sepal width (cm)</th>\n      <th>petal length (cm)</th>\n      <th>petal width (cm)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.222222</td>\n      <td>0.625000</td>\n      <td>0.067797</td>\n      <td>0.041667</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.166667</td>\n      <td>0.416667</td>\n      <td>0.067797</td>\n      <td>0.041667</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.111111</td>\n      <td>0.500000</td>\n      <td>0.050847</td>\n      <td>0.041667</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.083333</td>\n      <td>0.458333</td>\n      <td>0.084746</td>\n      <td>0.041667</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.194444</td>\n      <td>0.666667</td>\n      <td>0.067797</td>\n      <td>0.041667</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>0.666667</td>\n      <td>0.416667</td>\n      <td>0.711864</td>\n      <td>0.916667</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>0.555556</td>\n      <td>0.208333</td>\n      <td>0.677966</td>\n      <td>0.750000</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>0.611111</td>\n      <td>0.416667</td>\n      <td>0.711864</td>\n      <td>0.791667</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>0.527778</td>\n      <td>0.583333</td>\n      <td>0.745763</td>\n      <td>0.916667</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>0.444444</td>\n      <td>0.416667</td>\n      <td>0.694915</td>\n      <td>0.708333</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 4 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nsns.scatterplot(data = iris_df, x = 'sepal length (cm)', y = 'sepal width (cm)')\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\n<Axes: xlabel='sepal length (cm)', ylabel='sepal width (cm)'>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_4_1_Iris_files/figure-html/cell-11-output-2.png){width=589 height=429}\n:::\n:::\n\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nsns.scatterplot(data = iris_df, x = 'sepal length (cm)', y = 'petal length (cm)')\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\n<Axes: xlabel='sepal length (cm)', ylabel='petal length (cm)'>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_4_1_Iris_files/figure-html/cell-12-output-2.png){width=589 height=429}\n:::\n:::\n\n\nInteresting. There seem to be two groupings in the data.\n\nIt might be easier to look at all the variables at once.\n\n::: {.cell .column-page-right execution_count=12}\n``` {.python .cell-code}\nsns.pairplot(iris_df)\n```\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_4_1_Iris_files/figure-html/cell-13-output-1.png){width=949 height=945}\n:::\n:::\n\n\nThere seem to be some groupings in the data. Though we cannot easily identify which point corresponds to which row.\n\n## Clustering\n\nA cluster is simply a group based on simliarity. There are several methods and we will use a relatively simple one called K-means clustering.\n\nIn K-means clustering an algorithm tries to group our items (plants in the iris dataset) based on similarity. We decide how many groups we want and the algorithm does the best it can (an accessible introduction to k-means clustering is [here](https://www.analyticsvidhya.com/blog/2020/10/a-simple-explanation-of-k-means-clustering/)).\n\nTo start, we import the KMeans function from sklearn cluster module and turn our data into a matrix.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nfrom sklearn.cluster import KMeans\n\niris = iris_df.values\niris\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\narray([[0.22222222, 0.625     , 0.06779661, 0.04166667],\n       [0.16666667, 0.41666667, 0.06779661, 0.04166667],\n       [0.11111111, 0.5       , 0.05084746, 0.04166667],\n       [0.08333333, 0.45833333, 0.08474576, 0.04166667],\n       [0.19444444, 0.66666667, 0.06779661, 0.04166667],\n       [0.30555556, 0.79166667, 0.11864407, 0.125     ],\n       [0.08333333, 0.58333333, 0.06779661, 0.08333333],\n       [0.19444444, 0.58333333, 0.08474576, 0.04166667],\n       [0.02777778, 0.375     , 0.06779661, 0.04166667],\n       [0.16666667, 0.45833333, 0.08474576, 0.        ],\n       [0.30555556, 0.70833333, 0.08474576, 0.04166667],\n       [0.13888889, 0.58333333, 0.10169492, 0.04166667],\n       [0.13888889, 0.41666667, 0.06779661, 0.        ],\n       [0.        , 0.41666667, 0.01694915, 0.        ],\n       [0.41666667, 0.83333333, 0.03389831, 0.04166667],\n       [0.38888889, 1.        , 0.08474576, 0.125     ],\n       [0.30555556, 0.79166667, 0.05084746, 0.125     ],\n       [0.22222222, 0.625     , 0.06779661, 0.08333333],\n       [0.38888889, 0.75      , 0.11864407, 0.08333333],\n       [0.22222222, 0.75      , 0.08474576, 0.08333333],\n       [0.30555556, 0.58333333, 0.11864407, 0.04166667],\n       [0.22222222, 0.70833333, 0.08474576, 0.125     ],\n       [0.08333333, 0.66666667, 0.        , 0.04166667],\n       [0.22222222, 0.54166667, 0.11864407, 0.16666667],\n       [0.13888889, 0.58333333, 0.15254237, 0.04166667],\n       [0.19444444, 0.41666667, 0.10169492, 0.04166667],\n       [0.19444444, 0.58333333, 0.10169492, 0.125     ],\n       [0.25      , 0.625     , 0.08474576, 0.04166667],\n       [0.25      , 0.58333333, 0.06779661, 0.04166667],\n       [0.11111111, 0.5       , 0.10169492, 0.04166667],\n       [0.13888889, 0.45833333, 0.10169492, 0.04166667],\n       [0.30555556, 0.58333333, 0.08474576, 0.125     ],\n       [0.25      , 0.875     , 0.08474576, 0.        ],\n       [0.33333333, 0.91666667, 0.06779661, 0.04166667],\n       [0.16666667, 0.45833333, 0.08474576, 0.04166667],\n       [0.19444444, 0.5       , 0.03389831, 0.04166667],\n       [0.33333333, 0.625     , 0.05084746, 0.04166667],\n       [0.16666667, 0.66666667, 0.06779661, 0.        ],\n       [0.02777778, 0.41666667, 0.05084746, 0.04166667],\n       [0.22222222, 0.58333333, 0.08474576, 0.04166667],\n       [0.19444444, 0.625     , 0.05084746, 0.08333333],\n       [0.05555556, 0.125     , 0.05084746, 0.08333333],\n       [0.02777778, 0.5       , 0.05084746, 0.04166667],\n       [0.19444444, 0.625     , 0.10169492, 0.20833333],\n       [0.22222222, 0.75      , 0.15254237, 0.125     ],\n       [0.13888889, 0.41666667, 0.06779661, 0.08333333],\n       [0.22222222, 0.75      , 0.10169492, 0.04166667],\n       [0.08333333, 0.5       , 0.06779661, 0.04166667],\n       [0.27777778, 0.70833333, 0.08474576, 0.04166667],\n       [0.19444444, 0.54166667, 0.06779661, 0.04166667],\n       [0.75      , 0.5       , 0.62711864, 0.54166667],\n       [0.58333333, 0.5       , 0.59322034, 0.58333333],\n       [0.72222222, 0.45833333, 0.66101695, 0.58333333],\n       [0.33333333, 0.125     , 0.50847458, 0.5       ],\n       [0.61111111, 0.33333333, 0.61016949, 0.58333333],\n       [0.38888889, 0.33333333, 0.59322034, 0.5       ],\n       [0.55555556, 0.54166667, 0.62711864, 0.625     ],\n       [0.16666667, 0.16666667, 0.38983051, 0.375     ],\n       [0.63888889, 0.375     , 0.61016949, 0.5       ],\n       [0.25      , 0.29166667, 0.49152542, 0.54166667],\n       [0.19444444, 0.        , 0.42372881, 0.375     ],\n       [0.44444444, 0.41666667, 0.54237288, 0.58333333],\n       [0.47222222, 0.08333333, 0.50847458, 0.375     ],\n       [0.5       , 0.375     , 0.62711864, 0.54166667],\n       [0.36111111, 0.375     , 0.44067797, 0.5       ],\n       [0.66666667, 0.45833333, 0.57627119, 0.54166667],\n       [0.36111111, 0.41666667, 0.59322034, 0.58333333],\n       [0.41666667, 0.29166667, 0.52542373, 0.375     ],\n       [0.52777778, 0.08333333, 0.59322034, 0.58333333],\n       [0.36111111, 0.20833333, 0.49152542, 0.41666667],\n       [0.44444444, 0.5       , 0.6440678 , 0.70833333],\n       [0.5       , 0.33333333, 0.50847458, 0.5       ],\n       [0.55555556, 0.20833333, 0.66101695, 0.58333333],\n       [0.5       , 0.33333333, 0.62711864, 0.45833333],\n       [0.58333333, 0.375     , 0.55932203, 0.5       ],\n       [0.63888889, 0.41666667, 0.57627119, 0.54166667],\n       [0.69444444, 0.33333333, 0.6440678 , 0.54166667],\n       [0.66666667, 0.41666667, 0.6779661 , 0.66666667],\n       [0.47222222, 0.375     , 0.59322034, 0.58333333],\n       [0.38888889, 0.25      , 0.42372881, 0.375     ],\n       [0.33333333, 0.16666667, 0.47457627, 0.41666667],\n       [0.33333333, 0.16666667, 0.45762712, 0.375     ],\n       [0.41666667, 0.29166667, 0.49152542, 0.45833333],\n       [0.47222222, 0.29166667, 0.69491525, 0.625     ],\n       [0.30555556, 0.41666667, 0.59322034, 0.58333333],\n       [0.47222222, 0.58333333, 0.59322034, 0.625     ],\n       [0.66666667, 0.45833333, 0.62711864, 0.58333333],\n       [0.55555556, 0.125     , 0.57627119, 0.5       ],\n       [0.36111111, 0.41666667, 0.52542373, 0.5       ],\n       [0.33333333, 0.20833333, 0.50847458, 0.5       ],\n       [0.33333333, 0.25      , 0.57627119, 0.45833333],\n       [0.5       , 0.41666667, 0.61016949, 0.54166667],\n       [0.41666667, 0.25      , 0.50847458, 0.45833333],\n       [0.19444444, 0.125     , 0.38983051, 0.375     ],\n       [0.36111111, 0.29166667, 0.54237288, 0.5       ],\n       [0.38888889, 0.41666667, 0.54237288, 0.45833333],\n       [0.38888889, 0.375     , 0.54237288, 0.5       ],\n       [0.52777778, 0.375     , 0.55932203, 0.5       ],\n       [0.22222222, 0.20833333, 0.33898305, 0.41666667],\n       [0.38888889, 0.33333333, 0.52542373, 0.5       ],\n       [0.55555556, 0.54166667, 0.84745763, 1.        ],\n       [0.41666667, 0.29166667, 0.69491525, 0.75      ],\n       [0.77777778, 0.41666667, 0.83050847, 0.83333333],\n       [0.55555556, 0.375     , 0.77966102, 0.70833333],\n       [0.61111111, 0.41666667, 0.81355932, 0.875     ],\n       [0.91666667, 0.41666667, 0.94915254, 0.83333333],\n       [0.16666667, 0.20833333, 0.59322034, 0.66666667],\n       [0.83333333, 0.375     , 0.89830508, 0.70833333],\n       [0.66666667, 0.20833333, 0.81355932, 0.70833333],\n       [0.80555556, 0.66666667, 0.86440678, 1.        ],\n       [0.61111111, 0.5       , 0.69491525, 0.79166667],\n       [0.58333333, 0.29166667, 0.72881356, 0.75      ],\n       [0.69444444, 0.41666667, 0.76271186, 0.83333333],\n       [0.38888889, 0.20833333, 0.6779661 , 0.79166667],\n       [0.41666667, 0.33333333, 0.69491525, 0.95833333],\n       [0.58333333, 0.5       , 0.72881356, 0.91666667],\n       [0.61111111, 0.41666667, 0.76271186, 0.70833333],\n       [0.94444444, 0.75      , 0.96610169, 0.875     ],\n       [0.94444444, 0.25      , 1.        , 0.91666667],\n       [0.47222222, 0.08333333, 0.6779661 , 0.58333333],\n       [0.72222222, 0.5       , 0.79661017, 0.91666667],\n       [0.36111111, 0.33333333, 0.66101695, 0.79166667],\n       [0.94444444, 0.33333333, 0.96610169, 0.79166667],\n       [0.55555556, 0.29166667, 0.66101695, 0.70833333],\n       [0.66666667, 0.54166667, 0.79661017, 0.83333333],\n       [0.80555556, 0.5       , 0.84745763, 0.70833333],\n       [0.52777778, 0.33333333, 0.6440678 , 0.70833333],\n       [0.5       , 0.41666667, 0.66101695, 0.70833333],\n       [0.58333333, 0.33333333, 0.77966102, 0.83333333],\n       [0.80555556, 0.41666667, 0.81355932, 0.625     ],\n       [0.86111111, 0.33333333, 0.86440678, 0.75      ],\n       [1.        , 0.75      , 0.91525424, 0.79166667],\n       [0.58333333, 0.33333333, 0.77966102, 0.875     ],\n       [0.55555556, 0.33333333, 0.69491525, 0.58333333],\n       [0.5       , 0.25      , 0.77966102, 0.54166667],\n       [0.94444444, 0.41666667, 0.86440678, 0.91666667],\n       [0.55555556, 0.58333333, 0.77966102, 0.95833333],\n       [0.58333333, 0.45833333, 0.76271186, 0.70833333],\n       [0.47222222, 0.41666667, 0.6440678 , 0.70833333],\n       [0.72222222, 0.45833333, 0.74576271, 0.83333333],\n       [0.66666667, 0.45833333, 0.77966102, 0.95833333],\n       [0.72222222, 0.45833333, 0.69491525, 0.91666667],\n       [0.41666667, 0.29166667, 0.69491525, 0.75      ],\n       [0.69444444, 0.5       , 0.83050847, 0.91666667],\n       [0.66666667, 0.54166667, 0.79661017, 1.        ],\n       [0.66666667, 0.41666667, 0.71186441, 0.91666667],\n       [0.55555556, 0.20833333, 0.6779661 , 0.75      ],\n       [0.61111111, 0.41666667, 0.71186441, 0.79166667],\n       [0.52777778, 0.58333333, 0.74576271, 0.91666667],\n       [0.44444444, 0.41666667, 0.69491525, 0.70833333]])\n```\n:::\n:::\n\n\nSpecify our number of clusters.\n\n::: callout-caution\n\n## IMPORTANT: Check if your data features are standardised/normalised!!\n\nBefore you apply techniques such as PCA, clustering or other feature embedding technniques (such as t-SNE, MDS, etc.). It is very important to make sure that the data features that go into these techniques are normalised/standardised:\n- you can bring the value ranges between 0 and 1 for all of them with a MixMax scaling operation - https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n- you can standardise the values to have a mean of 0 and a standard deviation of 1, aka, z-score standardisation -- https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n- Or make use of more specific normalisation operators that might be more suitable for a particular context. The scikit-learn collection is a good place to look for alternatives -- https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing \n\nThese operations ensure that the results are not biased/skewed/dominated by some of the inherent characteristics of the data that is simply due to the *domain* of values.\n\nScikit-learn has some very nice tutorial here: https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py\n\n::: callout-caution\n\n## Do-this-yourself: Check if we need to do any normalisation for this case?\n\nWe have already looked at how the data looks, what are the descriptive statistics look like, see if we need to do anything more?\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nk_means = KMeans(n_clusters = 3, init = 'random',  n_init = 10)\n```\n:::\n\n\nFit our kmeans model to the data\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nk_means.fit(iris)\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```{=html}\n<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(init=&#x27;random&#x27;, n_clusters=3, n_init=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(init=&#x27;random&#x27;, n_clusters=3, n_init=10)</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\nThe algorithm has assigned the a label to each row.\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nk_means.labels_\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1,\n       1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1,\n       1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2], dtype=int32)\n```\n:::\n:::\n\n\nEach row has been assigned a label.\n\nTo tidy things up we should put everything into a dataframe.\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\niris_df['Three clusters'] = pd.Series(k_means.predict(iris_df.values), index = iris_df.index)\n```\n:::\n\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\niris_df\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal length (cm)</th>\n      <th>sepal width (cm)</th>\n      <th>petal length (cm)</th>\n      <th>petal width (cm)</th>\n      <th>Three clusters</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.222222</td>\n      <td>0.625000</td>\n      <td>0.067797</td>\n      <td>0.041667</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.166667</td>\n      <td>0.416667</td>\n      <td>0.067797</td>\n      <td>0.041667</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.111111</td>\n      <td>0.500000</td>\n      <td>0.050847</td>\n      <td>0.041667</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.083333</td>\n      <td>0.458333</td>\n      <td>0.084746</td>\n      <td>0.041667</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.194444</td>\n      <td>0.666667</td>\n      <td>0.067797</td>\n      <td>0.041667</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>0.666667</td>\n      <td>0.416667</td>\n      <td>0.711864</td>\n      <td>0.916667</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>0.555556</td>\n      <td>0.208333</td>\n      <td>0.677966</td>\n      <td>0.750000</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>0.611111</td>\n      <td>0.416667</td>\n      <td>0.711864</td>\n      <td>0.791667</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>0.527778</td>\n      <td>0.583333</td>\n      <td>0.745763</td>\n      <td>0.916667</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>0.444444</td>\n      <td>0.416667</td>\n      <td>0.694915</td>\n      <td>0.708333</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 5 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {.cell .column-page-right execution_count=19}\n``` {.python .cell-code}\nsns.pairplot(iris_df, hue = 'Three clusters')\n```\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_4_1_Iris_files/figure-html/cell-20-output-1.png){width=1060 height=945}\n:::\n:::\n\n\nThat seems quite nice. We can also do individual plots if preferred.\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\nsns.scatterplot(data = iris_df, x = 'sepal length (cm)', y = 'petal width (cm)', hue = 'Three clusters')\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\n<Axes: xlabel='sepal length (cm)', ylabel='petal width (cm)'>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_4_1_Iris_files/figure-html/cell-21-output-2.png){width=589 height=429}\n:::\n:::\n\n\nK-means works by clustering the data around central points (often called centroids, means or cluster centers). We can extract the cluster centres from the kmeans object.\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\nk_means.cluster_centers_\n```\n\n::: {.cell-output .cell-output-display execution_count=21}\n```\narray([[0.19611111, 0.595     , 0.07830508, 0.06083333],\n       [0.70726496, 0.4508547 , 0.79704476, 0.82478632],\n       [0.44125683, 0.30737705, 0.57571548, 0.54918033]])\n```\n:::\n:::\n\n\nIt is tricky to plot these using seaborn but we can use a normal maplotlib scatter plot.\n\nLet us grab the groups.\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\ngroup1 = iris_df[iris_df['Three clusters'] == 0]\ngroup2 = iris_df[iris_df['Three clusters'] == 1]\ngroup3 = iris_df[iris_df['Three clusters'] == 2]\n```\n:::\n\n\nGrab the centroids\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nimport pandas as pd\n\ncentres = k_means.cluster_centers_\n\ndata = {'x': [centres[0][0], centres[1][0], centres[2][0]],\n        'y': [centres[0][3], centres[1][3], centres[2][3]]}\n\ndf = pd.DataFrame (data, columns = ['x', 'y'])\n```\n:::\n\n\nCreate the plot\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\n# Plot each group individually\nplt.scatter(\n    x = group1['sepal length (cm)'], \n    y = group1['petal width (cm)'], \n    alpha = 0.1, color = 'blue'\n)\n\nplt.scatter(\n    x = group2['sepal length (cm)'], \n    y = group2['petal width (cm)'], \n    alpha = 0.1, color = 'orange'\n)\n\nplt.scatter(\n    x = group3['sepal length (cm)'], \n    y = group3['petal width (cm)'], \n    alpha = 0.1, color = 'red'\n)\n\n# Plot cluster centres\nplt.scatter(\n    x = df['x'], \n    y = df['y'], \n    alpha = 1, color = 'black'\n)\n\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```\n<matplotlib.collections.PathCollection at 0x15ae06590>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_4_1_Iris_files/figure-html/cell-25-output-2.png){width=571 height=411}\n:::\n:::\n\n\n### Number of clusters\n\nWhat happens if we change the number of clusters?\n\nTwo groups\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\nk_means_2 = KMeans(n_clusters = 2, init = 'random', n_init = 10)\nk_means_2.fit(iris)\niris_df['Two clusters'] = pd.Series(k_means_2.predict(iris_df.iloc[:,0:4].values), index = iris_df.index)\n```\n:::\n\n\nNote that I have added a new column to the iris dataframe called 'cluster 2 means' and pass only our origonal 4 columns to the predict function (hence me using .iloc[:,0:4]).\n\nHow do our groupings look now (without plotting the cluster column)?\n\n::: {.cell .column-page-right execution_count=26}\n``` {.python .cell-code}\nsns.pairplot(iris_df.loc[:, iris_df.columns != 'Three clusters'], hue = 'Two clusters')\n```\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_4_1_Iris_files/figure-html/cell-27-output-1.png){width=1047 height=945}\n:::\n:::\n\n\nHmm, does the data have more than two groups in it?\n\nPerhaps we should try 5 clusters instead.\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\nk_means_5 = KMeans(n_clusters = 5, init = 'random', n_init = 10)\nk_means_5.fit(iris)\niris_df['Five clusters'] = pd.Series(k_means_5.predict(iris_df.iloc[:,0:4].values), index = iris_df.index)\n```\n:::\n\n\nPlot without the columns called 'cluster' and 'Two cluster'\n\n::: {.cell .column-page-right execution_count=28}\n``` {.python .cell-code}\nsns.pairplot(iris_df.loc[:, (iris_df.columns != 'Three clusters') & (iris_df.columns != 'Two clusters')], hue = 'Five clusters')\n```\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_4_1_Iris_files/figure-html/cell-29-output-1.png){width=1048 height=945}\n:::\n:::\n\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\niris_df\n```\n\n::: {.cell-output .cell-output-display execution_count=29}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal length (cm)</th>\n      <th>sepal width (cm)</th>\n      <th>petal length (cm)</th>\n      <th>petal width (cm)</th>\n      <th>Three clusters</th>\n      <th>Two clusters</th>\n      <th>Five clusters</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.222222</td>\n      <td>0.625000</td>\n      <td>0.067797</td>\n      <td>0.041667</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.166667</td>\n      <td>0.416667</td>\n      <td>0.067797</td>\n      <td>0.041667</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.111111</td>\n      <td>0.500000</td>\n      <td>0.050847</td>\n      <td>0.041667</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.083333</td>\n      <td>0.458333</td>\n      <td>0.084746</td>\n      <td>0.041667</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.194444</td>\n      <td>0.666667</td>\n      <td>0.067797</td>\n      <td>0.041667</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>0.666667</td>\n      <td>0.416667</td>\n      <td>0.711864</td>\n      <td>0.916667</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>0.555556</td>\n      <td>0.208333</td>\n      <td>0.677966</td>\n      <td>0.750000</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>0.611111</td>\n      <td>0.416667</td>\n      <td>0.711864</td>\n      <td>0.791667</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>0.527778</td>\n      <td>0.583333</td>\n      <td>0.745763</td>\n      <td>0.916667</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>0.444444</td>\n      <td>0.416667</td>\n      <td>0.694915</td>\n      <td>0.708333</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 7 columns</p>\n</div>\n```\n:::\n:::\n\n\nWhich did best?\n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\nk_means.inertia_\n```\n\n::: {.cell-output .cell-output-display execution_count=30}\n```\n6.982216473785234\n```\n:::\n:::\n\n\n::: {.cell execution_count=31}\n``` {.python .cell-code}\nk_means_2.inertia_\n```\n\n::: {.cell-output .cell-output-display execution_count=31}\n```\n12.127790750538193\n```\n:::\n:::\n\n\n::: {.cell execution_count=32}\n``` {.python .cell-code}\nk_means_5.inertia_\n```\n\n::: {.cell-output .cell-output-display execution_count=32}\n```\n4.580948640117293\n```\n:::\n:::\n\n\nIt looks like our k = 5 model captures the data well. Intertia, [looking at the sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) as the _Sum of squared distances of samples to their closest cluster center._.\n\nIf you want to dive further into this then Real Python's [practical guide to K-Means Clustering](https://realpython.com/k-means-clustering-python/) is quite good.\n\n## Principal Component Analysis (PCA)\n\n\nPCA reduces the dimension of our data. The method derives point in an n dimentional space from our data which are uncorrelated.\n\nTo carry out a PCA on our Iris dataset where there are only two dimensions.\n\n::: {.cell execution_count=33}\n``` {.python .cell-code}\nfrom sklearn.decomposition import PCA\n\nn_components = 2\n\npca = PCA(n_components=n_components)\niris_pca = pca.fit(iris_df.iloc[:,0:4])\n```\n:::\n\n\nWe can look at the components.\n\n::: {.cell execution_count=34}\n``` {.python .cell-code}\niris_pca.components_\n```\n\n::: {.cell-output .cell-output-display execution_count=34}\n```\narray([[ 0.42494212, -0.15074824,  0.61626702,  0.64568888],\n       [ 0.42320271,  0.90396711, -0.06038308, -0.00983925]])\n```\n:::\n:::\n\n\nThese components are intersting. You may want to look at a [PennState article on interpreting PCA components](https://online.stat.psu.edu/stat505/lesson/11/11.4).\n\nOur second column, 'sepal width (cm)' is positively correlated with our second principle component whereas the first column 'sepal length (cm)' is postively correlated with both.\n\nYou may want to consider:\n\n* Do we need more than two components?\n* Is it useful to keep sepal length (cm) in the dataset?\n\nWe can also examine the explained variance of the each principle component.\n\n::: {.cell execution_count=35}\n``` {.python .cell-code}\niris_pca.explained_variance_\n```\n\n::: {.cell-output .cell-output-display execution_count=35}\n```\narray([0.23245325, 0.0324682 ])\n```\n:::\n:::\n\n\nA nice worked example showing the link between the explained variance and the component is [here](https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html).\n\nOur first principle component explains a lot more of the variance of data then the second. \n\nAnother way to explore these indicators is to look at the `explained_variance_ratio_` values. These present a similar information but provide them as percentage values so they are easier to interpret. You can also create a plot and see how these percentages add up. In this case, the first two components add up to 0.96. Which means the first two features are able to represent around 96% of the variation in the data, not bad. These values are not always this high.\n\nA high value that is close to 100% means that the PCA is able to represent much of the variance and they will be good representations of the data without losing a lot of that variance in the underlying features. This of course is based on an assumption that `variance` is a good proxy about how informative a feature is.\n\n::: {.cell execution_count=36}\n``` {.python .cell-code}\niris_pca.explained_variance_ratio_\n```\n\n::: {.cell-output .cell-output-display execution_count=36}\n```\narray([0.84136038, 0.11751808])\n```\n:::\n:::\n\n\n::: {.cell execution_count=37}\n``` {.python .cell-code}\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');\n```\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_4_1_Iris_files/figure-html/cell-38-output-1.png){width=597 height=429}\n:::\n:::\n\n\n### Dimension reduction\n\nFor our purposes, we are interested in using PCA for reducing the number of dimension in our data whilst preseving the maximal data variance.\n\nWe can extract the projected components from the model.\n\n::: {.cell execution_count=38}\n``` {.python .cell-code}\niris_pca_vals = pca.fit_transform(iris_df.iloc[:,0:4])\n```\n:::\n\n\nThe numpy arrays contains the projected values.\n\n::: {.cell execution_count=39}\n``` {.python .cell-code}\ntype(iris_pca_vals)\n```\n\n::: {.cell-output .cell-output-display execution_count=39}\n```\nnumpy.ndarray\n```\n:::\n:::\n\n\n::: {.cell execution_count=40}\n``` {.python .cell-code}\niris_pca_vals\n```\n\n::: {.cell-output .cell-output-display execution_count=40}\n```\narray([[-6.30702931e-01,  1.07577910e-01],\n       [-6.22904943e-01, -1.04259833e-01],\n       [-6.69520395e-01, -5.14170597e-02],\n       [-6.54152759e-01, -1.02884871e-01],\n       [-6.48788056e-01,  1.33487576e-01],\n       [-5.35272778e-01,  2.89615724e-01],\n       [-6.56537790e-01,  1.07244911e-02],\n       [-6.25780499e-01,  5.71335411e-02],\n       [-6.75643504e-01, -2.00703283e-01],\n       [-6.45644619e-01, -6.72080097e-02],\n       [-5.97408238e-01,  2.17151953e-01],\n       [-6.38943190e-01,  3.25988375e-02],\n       [-6.61612593e-01, -1.15605495e-01],\n       [-7.51967943e-01, -1.71313322e-01],\n       [-6.00371589e-01,  3.80240692e-01],\n       [-5.52157227e-01,  5.15255982e-01],\n       [-5.77053593e-01,  2.93709492e-01],\n       [-6.03799228e-01,  1.07167941e-01],\n       [-5.20483461e-01,  2.87627289e-01],\n       [-6.12197555e-01,  2.19140388e-01],\n       [-5.57674300e-01,  1.02109180e-01],\n       [-5.79012675e-01,  1.81065123e-01],\n       [-7.37784662e-01,  9.05588211e-02],\n       [-5.06093857e-01,  2.79470846e-02],\n       [-6.07607579e-01,  2.95285112e-02],\n       [-5.90210587e-01, -9.45510863e-02],\n       [-5.61527888e-01,  5.52901611e-02],\n       [-6.08453780e-01,  1.18310099e-01],\n       [-6.12617807e-01,  8.16682448e-02],\n       [-6.38184784e-01, -5.44873860e-02],\n       [-6.20099660e-01, -8.03970516e-02],\n       [-5.24757301e-01,  1.03336126e-01],\n       [-6.73044544e-01,  3.44711846e-01],\n       [-6.27455379e-01,  4.18257508e-01],\n       [-6.18740916e-01, -6.76179787e-02],\n       [-6.44553756e-01, -1.51267253e-02],\n       [-5.93932344e-01,  1.55623876e-01],\n       [-6.87495707e-01,  1.22141914e-01],\n       [-6.92369885e-01, -1.62014545e-01],\n       [-6.13976551e-01,  6.88891719e-02],\n       [-6.26048380e-01,  9.64357527e-02],\n       [-6.09693996e-01, -4.14325957e-01],\n       [-7.04932239e-01, -8.66839521e-02],\n       [-5.14001659e-01,  9.21355196e-02],\n       [-5.43513037e-01,  2.14636651e-01],\n       [-6.07805187e-01, -1.16425433e-01],\n       [-6.28656055e-01,  2.18526915e-01],\n       [-6.70879139e-01, -6.41961326e-02],\n       [-6.09212186e-01,  2.05396323e-01],\n       [-6.29944525e-01,  2.04916869e-02],\n       [ 2.79951766e-01,  1.79245790e-01],\n       [ 2.15141376e-01,  1.10348921e-01],\n       [ 3.22223106e-01,  1.27368010e-01],\n       [ 5.94030131e-02, -3.28502275e-01],\n       [ 2.62515235e-01, -2.95800761e-02],\n       [ 1.03831043e-01, -1.21781742e-01],\n       [ 2.44850362e-01,  1.33801733e-01],\n       [-1.71529386e-01, -3.52976762e-01],\n       [ 2.14230599e-01,  2.06607890e-02],\n       [ 1.53249619e-02, -2.12494509e-01],\n       [-1.13710323e-01, -4.93929201e-01],\n       [ 1.37348380e-01, -2.06894998e-02],\n       [ 4.39928190e-02, -3.06159511e-01],\n       [ 1.92559767e-01, -3.95507760e-02],\n       [-8.26091518e-03, -8.66610981e-02],\n       [ 2.19485489e-01,  1.09383928e-01],\n       [ 1.33272148e-01, -5.90267184e-02],\n       [-5.75757060e-04, -1.42367733e-01],\n       [ 2.54345249e-01, -2.89815304e-01],\n       [-5.60800300e-03, -2.39572672e-01],\n       [ 2.68168358e-01,  4.72705335e-02],\n       [ 9.88208151e-02, -6.96420088e-02],\n       [ 2.89086481e-01, -1.69157553e-01],\n       [ 1.45033538e-01, -7.63961345e-02],\n       [ 1.59287093e-01,  2.19853643e-04],\n       [ 2.13962718e-01,  5.99630005e-02],\n       [ 2.91913782e-01,  4.04990109e-03],\n       [ 3.69148997e-01,  6.43480720e-02],\n       [ 1.86769115e-01, -4.96694916e-02],\n       [-6.87697501e-02, -1.85648007e-01],\n       [-2.15759776e-02, -2.87970157e-01],\n       [-5.89248844e-02, -2.86536746e-01],\n       [ 3.23412419e-02, -1.41140786e-01],\n       [ 2.88906394e-01, -1.31550706e-01],\n       [ 1.09664252e-01, -8.25379800e-02],\n       [ 1.82266934e-01,  1.38247021e-01],\n       [ 2.77724803e-01,  1.05903632e-01],\n       [ 1.95615410e-01, -2.38550997e-01],\n       [ 3.76839264e-02, -5.41130122e-02],\n       [ 4.68406593e-02, -2.53171683e-01],\n       [ 5.54365941e-02, -2.19190186e-01],\n       [ 1.75833387e-01, -8.62037590e-04],\n       [ 4.90676225e-02, -1.79829525e-01],\n       [-1.53444261e-01, -3.78886428e-01],\n       [ 6.69726607e-02, -1.68132343e-01],\n       [ 3.30293747e-02, -4.29708545e-02],\n       [ 6.62142547e-02, -8.10461198e-02],\n       [ 1.35679197e-01, -2.32914079e-02],\n       [-1.58634575e-01, -2.89139847e-01],\n       [ 6.20502279e-02, -1.17687974e-01],\n       [ 6.22771338e-01,  1.16807265e-01],\n       [ 3.46009609e-01, -1.56291874e-01],\n       [ 6.17986434e-01,  1.00519741e-01],\n       [ 4.17789309e-01, -2.68903690e-02],\n       [ 5.63621248e-01,  3.05994289e-02],\n       [ 7.50122599e-01,  1.52133800e-01],\n       [ 1.35857804e-01, -3.30462554e-01],\n       [ 6.08945212e-01,  8.35018443e-02],\n       [ 5.11020215e-01, -1.32575915e-01],\n       [ 7.20608541e-01,  3.34580389e-01],\n       [ 4.24135062e-01,  1.13914054e-01],\n       [ 4.37723702e-01, -8.78049736e-02],\n       [ 5.40793776e-01,  6.93466165e-02],\n       [ 3.63226514e-01, -2.42764625e-01],\n       [ 4.74246948e-01, -1.20676423e-01],\n       [ 5.13932631e-01,  9.88816323e-02],\n       [ 4.24670824e-01,  3.53096310e-02],\n       [ 7.49026039e-01,  4.63778390e-01],\n       [ 8.72194272e-01,  9.33798117e-03],\n       [ 2.82963372e-01, -3.18443776e-01],\n       [ 6.14733184e-01,  1.53566018e-01],\n       [ 3.22133832e-01, -1.40500924e-01],\n       [ 7.58030401e-01,  8.79453649e-02],\n       [ 3.57235237e-01, -9.50568671e-02],\n       [ 5.31036706e-01,  1.68539991e-01],\n       [ 5.46962123e-01,  1.87812429e-01],\n       [ 3.28704908e-01, -6.81237595e-02],\n       [ 3.14783811e-01, -5.57223965e-03],\n       [ 5.16585543e-01, -5.40299414e-02],\n       [ 4.84826663e-01,  1.15348658e-01],\n       [ 6.33043632e-01,  5.92290940e-02],\n       [ 6.87490917e-01,  4.91179916e-01],\n       [ 5.43489246e-01, -5.44399104e-02],\n       [ 2.91133358e-01, -5.82085481e-02],\n       [ 3.05410131e-01, -1.61757644e-01],\n       [ 7.63507935e-01,  1.68186703e-01],\n       [ 5.47805644e-01,  1.58976299e-01],\n       [ 4.06585699e-01,  6.12192966e-02],\n       [ 2.92534659e-01, -1.63044284e-02],\n       [ 5.35871344e-01,  1.19790986e-01],\n       [ 6.13864965e-01,  9.30029331e-02],\n       [ 5.58343139e-01,  1.22041374e-01],\n       [ 3.46009609e-01, -1.56291874e-01],\n       [ 6.23819644e-01,  1.39763503e-01],\n       [ 6.38651518e-01,  1.66900115e-01],\n       [ 5.51461624e-01,  5.98413741e-02],\n       [ 4.07146497e-01, -1.71820871e-01],\n       [ 4.47142619e-01,  3.75600193e-02],\n       [ 4.88207585e-01,  1.49677521e-01],\n       [ 3.12066323e-01, -3.11303854e-02]])\n```\n:::\n:::\n\n\nEach row corresponds to a row in our data.\n\n::: {.cell execution_count=41}\n``` {.python .cell-code}\niris_pca_vals.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=41}\n```\n(150, 2)\n```\n:::\n:::\n\n\n::: {.cell execution_count=42}\n``` {.python .cell-code}\niris_df.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=42}\n```\n(150, 7)\n```\n:::\n:::\n\n\nWe can add the component to our dataset. I prefer to keep everything in one table and it is not at all required. You can just assign the values whichever variables you prefer.\n\n::: {.cell execution_count=43}\n``` {.python .cell-code}\niris_df['c1'] = [item[0] for item in iris_pca_vals]\niris_df['c2'] = [item[1] for item in iris_pca_vals]\n```\n:::\n\n\n::: {.cell execution_count=44}\n``` {.python .cell-code}\niris_df\n```\n\n::: {.cell-output .cell-output-display execution_count=44}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal length (cm)</th>\n      <th>sepal width (cm)</th>\n      <th>petal length (cm)</th>\n      <th>petal width (cm)</th>\n      <th>Three clusters</th>\n      <th>Two clusters</th>\n      <th>Five clusters</th>\n      <th>c1</th>\n      <th>c2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.222222</td>\n      <td>0.625000</td>\n      <td>0.067797</td>\n      <td>0.041667</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>-0.630703</td>\n      <td>0.107578</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.166667</td>\n      <td>0.416667</td>\n      <td>0.067797</td>\n      <td>0.041667</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>-0.622905</td>\n      <td>-0.104260</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.111111</td>\n      <td>0.500000</td>\n      <td>0.050847</td>\n      <td>0.041667</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>-0.669520</td>\n      <td>-0.051417</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.083333</td>\n      <td>0.458333</td>\n      <td>0.084746</td>\n      <td>0.041667</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>-0.654153</td>\n      <td>-0.102885</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.194444</td>\n      <td>0.666667</td>\n      <td>0.067797</td>\n      <td>0.041667</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>-0.648788</td>\n      <td>0.133488</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>0.666667</td>\n      <td>0.416667</td>\n      <td>0.711864</td>\n      <td>0.916667</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0.551462</td>\n      <td>0.059841</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>0.555556</td>\n      <td>0.208333</td>\n      <td>0.677966</td>\n      <td>0.750000</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0.407146</td>\n      <td>-0.171821</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>0.611111</td>\n      <td>0.416667</td>\n      <td>0.711864</td>\n      <td>0.791667</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0.447143</td>\n      <td>0.037560</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>0.527778</td>\n      <td>0.583333</td>\n      <td>0.745763</td>\n      <td>0.916667</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0.488208</td>\n      <td>0.149678</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>0.444444</td>\n      <td>0.416667</td>\n      <td>0.694915</td>\n      <td>0.708333</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0.312066</td>\n      <td>-0.031130</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 9 columns</p>\n</div>\n```\n:::\n:::\n\n\nPlotting out our data on our new two component space.\n\n::: {.cell execution_count=45}\n``` {.python .cell-code}\nsns.scatterplot(data = iris_df, x = 'c1', y = 'c2')\n```\n\n::: {.cell-output .cell-output-display execution_count=45}\n```\n<Axes: xlabel='c1', ylabel='c2'>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_4_1_Iris_files/figure-html/cell-46-output-2.png){width=600 height=429}\n:::\n:::\n\n\nWe have reduced our three dimensions to two.\n\nWe can also colour by our clusters. What does this show us and is it useful?\n\n::: {.cell execution_count=46}\n``` {.python .cell-code}\nsns.scatterplot(data = iris_df, x = 'c1', y = 'c2', hue = 'Three clusters')\n```\n\n::: {.cell-output .cell-output-display execution_count=46}\n```\n<Axes: xlabel='c1', ylabel='c2'>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_4_1_Iris_files/figure-html/cell-47-output-2.png){width=600 height=429}\n:::\n:::\n\n\n::: {.cell execution_count=47}\n``` {.python .cell-code}\niris_df\n```\n\n::: {.cell-output .cell-output-display execution_count=47}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal length (cm)</th>\n      <th>sepal width (cm)</th>\n      <th>petal length (cm)</th>\n      <th>petal width (cm)</th>\n      <th>Three clusters</th>\n      <th>Two clusters</th>\n      <th>Five clusters</th>\n      <th>c1</th>\n      <th>c2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.222222</td>\n      <td>0.625000</td>\n      <td>0.067797</td>\n      <td>0.041667</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>-0.630703</td>\n      <td>0.107578</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.166667</td>\n      <td>0.416667</td>\n      <td>0.067797</td>\n      <td>0.041667</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>-0.622905</td>\n      <td>-0.104260</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.111111</td>\n      <td>0.500000</td>\n      <td>0.050847</td>\n      <td>0.041667</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>-0.669520</td>\n      <td>-0.051417</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.083333</td>\n      <td>0.458333</td>\n      <td>0.084746</td>\n      <td>0.041667</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>-0.654153</td>\n      <td>-0.102885</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.194444</td>\n      <td>0.666667</td>\n      <td>0.067797</td>\n      <td>0.041667</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>-0.648788</td>\n      <td>0.133488</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>0.666667</td>\n      <td>0.416667</td>\n      <td>0.711864</td>\n      <td>0.916667</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0.551462</td>\n      <td>0.059841</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>0.555556</td>\n      <td>0.208333</td>\n      <td>0.677966</td>\n      <td>0.750000</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0.407146</td>\n      <td>-0.171821</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>0.611111</td>\n      <td>0.416667</td>\n      <td>0.711864</td>\n      <td>0.791667</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0.447143</td>\n      <td>0.037560</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>0.527778</td>\n      <td>0.583333</td>\n      <td>0.745763</td>\n      <td>0.916667</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0.488208</td>\n      <td>0.149678</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>0.444444</td>\n      <td>0.416667</td>\n      <td>0.694915</td>\n      <td>0.708333</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0.312066</td>\n      <td>-0.031130</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 9 columns</p>\n</div>\n```\n:::\n:::\n\n\n### PCA to Clusters\n\nWe have reduced our 4D dataset to 2D whilst keeping the data variance. Reducing the data to fewer dimensions can help with the 'curse of dimensionality', reduce the change of overfitting a machine learning model (see [here](https://en.wikipedia.org/wiki/Dimensionality_reduction)) and reduce the computational complexity of a model fit.\n\nPutting our new dimensions into a kMeans model\n\n::: {.cell execution_count=48}\n``` {.python .cell-code}\nk_means_pca = KMeans(n_clusters = 3, init = 'random', n_init = 10)\niris_pca_kmeans = k_means_pca.fit(iris_df.iloc[:,-2:])\n```\n:::\n\n\n::: {.cell execution_count=49}\n``` {.python .cell-code}\ntype(iris_df.iloc[:,-2:].values)\n```\n\n::: {.cell-output .cell-output-display execution_count=49}\n```\nnumpy.ndarray\n```\n:::\n:::\n\n\n::: {.cell execution_count=50}\n``` {.python .cell-code}\niris_df['PCA 3 clusters'] = pd.Series(k_means_pca.predict(iris_df.iloc[:,-2:].values), index = iris_df.index)\niris_df\n```\n\n::: {.cell-output .cell-output-display execution_count=50}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal length (cm)</th>\n      <th>sepal width (cm)</th>\n      <th>petal length (cm)</th>\n      <th>petal width (cm)</th>\n      <th>Three clusters</th>\n      <th>Two clusters</th>\n      <th>Five clusters</th>\n      <th>c1</th>\n      <th>c2</th>\n      <th>PCA 3 clusters</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.222222</td>\n      <td>0.625000</td>\n      <td>0.067797</td>\n      <td>0.041667</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>-0.630703</td>\n      <td>0.107578</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.166667</td>\n      <td>0.416667</td>\n      <td>0.067797</td>\n      <td>0.041667</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>-0.622905</td>\n      <td>-0.104260</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.111111</td>\n      <td>0.500000</td>\n      <td>0.050847</td>\n      <td>0.041667</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>-0.669520</td>\n      <td>-0.051417</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.083333</td>\n      <td>0.458333</td>\n      <td>0.084746</td>\n      <td>0.041667</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>-0.654153</td>\n      <td>-0.102885</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.194444</td>\n      <td>0.666667</td>\n      <td>0.067797</td>\n      <td>0.041667</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>-0.648788</td>\n      <td>0.133488</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>0.666667</td>\n      <td>0.416667</td>\n      <td>0.711864</td>\n      <td>0.916667</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0.551462</td>\n      <td>0.059841</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>0.555556</td>\n      <td>0.208333</td>\n      <td>0.677966</td>\n      <td>0.750000</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0.407146</td>\n      <td>-0.171821</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>0.611111</td>\n      <td>0.416667</td>\n      <td>0.711864</td>\n      <td>0.791667</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0.447143</td>\n      <td>0.037560</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>0.527778</td>\n      <td>0.583333</td>\n      <td>0.745763</td>\n      <td>0.916667</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0.488208</td>\n      <td>0.149678</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>0.444444</td>\n      <td>0.416667</td>\n      <td>0.694915</td>\n      <td>0.708333</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0.312066</td>\n      <td>-0.031130</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 10 columns</p>\n</div>\n```\n:::\n:::\n\n\nAs we only have two dimensions we can easily plot this on a single scatterplot.\n\n::: {.cell execution_count=51}\n``` {.python .cell-code}\n# a different seaborn theme\n# see https://python-graph-gallery.com/104-seaborn-themes/\nsns.set_style(\"darkgrid\")\nsns.scatterplot(data = iris_df, x = 'c1', y = 'c2', hue = 'PCA 3 clusters')\n```\n\n::: {.cell-output .cell-output-display execution_count=51}\n```\n<Axes: xlabel='c1', ylabel='c2'>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_4_1_Iris_files/figure-html/cell-52-output-2.png){width=593 height=427}\n:::\n:::\n\n\nI suspect having two clusters would work better. We should try a few different models.\n\nCopying the code from [here](https://medium.com/@dmitriy.kavyazin/principal-component-analysis-and-k-means-clustering-to-visualize-a-high-dimensional-dataset-577b2a7a5fe2) we can fit multiple numbers of clusters.\n\n::: {.cell execution_count=52}\n``` {.python .cell-code}\nks = range(1, 10)\ninertias = [] # Create an empty list (will be populated later)\nfor k in ks:\n    # Create a KMeans instance with k clusters: model\n    model = KMeans(n_clusters=k, n_init = 10)\n    \n    # Fit model to samples\n    model.fit(iris_df.iloc[:,-2:])\n    \n    # Append the inertia to the list of inertias\n    inertias.append(model.inertia_)\n    \nplt.plot(ks, inertias, '-o', color='black')\nplt.xlabel('number of clusters, k')\nplt.ylabel('inertia')\nplt.xticks(ks)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_4_1_Iris_files/figure-html/cell-53-output-1.png){width=589 height=427}\n:::\n:::\n\n\nThree seems ok. We clearly want no more than three.\n\nThese types of plots show an point about model complexity. More free parameters in the model (here the number of clusters) will improve how well the model captures the data, often with reducing returns. However, a model which overfits the data will not be able to fit new data well - referred to overfitting. Randomish internet blogs introduce the topic pretty well, see [here](https://elitedatascience.com/overfitting-in-machine-learning), and also wikipedia, see [here](https://en.wikipedia.org/wiki/Overfitting).\n\n\n### Missing values\n\nFinally, how we deal with missing values can impact the results of PCA and kMeans clustering.\n\nLets us load in the iris dataset again and randomly remove 10% of the data (see code from [here](https://stackoverflow.com/questions/42091018/randomly-insert-nas-values-in-a-pandas-dataframe-with-no-rows-completely-miss)).\n\n::: {.cell execution_count=53}\n``` {.python .cell-code}\nimport numpy as np\n\nx = load_iris()\n```\n:::\n\n\n::: {.cell execution_count=54}\n``` {.python .cell-code}\niris_df = pd.DataFrame(x.data, columns = x.feature_names)\n\nmask = np.random.choice([True, False], size = iris_df.shape, p = [0.2, 0.8])\nmask[mask.all(1),-1] = 0\n\ndf = iris_df.mask(mask)\n\ndf.isna().sum()\n```\n\n::: {.cell-output .cell-output-display execution_count=54}\n```\nsepal length (cm)    26\nsepal width (cm)     39\npetal length (cm)    28\npetal width (cm)     31\ndtype: int64\n```\n:::\n:::\n\n\n::: {.cell execution_count=55}\n``` {.python .cell-code}\ndf\n```\n\n::: {.cell-output .cell-output-display execution_count=55}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal length (cm)</th>\n      <th>sepal width (cm)</th>\n      <th>petal length (cm)</th>\n      <th>petal width (cm)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NaN</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>6.7</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.3</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>6.3</td>\n      <td>NaN</td>\n      <td>5.0</td>\n      <td>1.9</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>6.5</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>6.2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.3</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>5.9</td>\n      <td>3.0</td>\n      <td>5.1</td>\n      <td>1.8</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 4 columns</p>\n</div>\n```\n:::\n:::\n\n\nAbout 20% of the data is randomly an NaN.\n\n#### Zeroing\n\nWe can 0 them and fit our models.\n\n::: {.cell execution_count=56}\n``` {.python .cell-code}\ndf_1 = df.copy()\ndf_1 = df_1.fillna(0)\n```\n:::\n\n\n::: {.cell execution_count=57}\n``` {.python .cell-code}\ndf_1\n```\n\n::: {.cell-output .cell-output-display execution_count=57}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal length (cm)</th>\n      <th>sepal width (cm)</th>\n      <th>petal length (cm)</th>\n      <th>petal width (cm)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>6.7</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.3</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>6.3</td>\n      <td>0.0</td>\n      <td>5.0</td>\n      <td>1.9</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>6.5</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>6.2</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.3</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>5.9</td>\n      <td>3.0</td>\n      <td>5.1</td>\n      <td>1.8</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 4 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {.cell .column-page-right execution_count=58}\n``` {.python .cell-code}\nk_means_zero = KMeans(n_clusters = 4, init = 'random', n_init = 10)\nk_means_zero.fit(df_1)\ndf_1['Four clusters'] = pd.Series(k_means_zero.predict(df_1.iloc[:,0:4].values), index = df_1.index)\nsns.pairplot(df_1, hue = 'Four clusters')\n```\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_4_1_Iris_files/figure-html/cell-59-output-1.png){width=1043 height=946}\n:::\n:::\n\n\nWhat impact has zeroing the values had on our results?\n\nNow, onto PCA.\n\n::: {.cell execution_count=59}\n``` {.python .cell-code}\n# PCA analysis\nn_components = 2\n\npca = PCA(n_components=n_components)\ndf_1_pca = pca.fit(df_1.iloc[:,0:4])\n\n# Extract projected values\ndf_1_pca_vals = df_1_pca.transform(df_1.iloc[:,0:4])\ndf_1['c1'] = [item[0] for item in df_1_pca_vals]\ndf_1['c2'] = [item[1] for item in df_1_pca_vals]\n\nsns.scatterplot(data = df_1, x = 'c1', y = 'c2')\n```\n\n::: {.cell-output .cell-output-display execution_count=59}\n```\n<Axes: xlabel='c1', ylabel='c2'>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_4_1_Iris_files/figure-html/cell-60-output-2.png){width=582 height=427}\n:::\n:::\n\n\n::: {.cell execution_count=60}\n``` {.python .cell-code}\ndf_1_pca.explained_variance_\n```\n\n::: {.cell-output .cell-output-display execution_count=60}\n```\narray([5.96572517, 4.24113186])\n```\n:::\n:::\n\n\n::: {.cell execution_count=61}\n``` {.python .cell-code}\ndf_1_pca.components_\n```\n\n::: {.cell-output .cell-output-display execution_count=61}\n```\narray([[-0.83567034, -0.01269859, -0.53348328, -0.12995929],\n       [-0.54551254, -0.07656045,  0.82392635,  0.13304112]])\n```\n:::\n:::\n\n\n#### Replacing with the average\n\n::: {.cell execution_count=62}\n``` {.python .cell-code}\ndf_2 = df.copy()\nfor i in range(4):\n    df_2.iloc[:,i] = df_2.iloc[:,i].fillna(df_2.iloc[:,i].mean())\n```\n:::\n\n\n::: {.cell execution_count=63}\n``` {.python .cell-code}\ndf_2\n```\n\n::: {.cell-output .cell-output-display execution_count=63}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal length (cm)</th>\n      <th>sepal width (cm)</th>\n      <th>petal length (cm)</th>\n      <th>petal width (cm)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.100000</td>\n      <td>3.500000</td>\n      <td>1.40000</td>\n      <td>1.231092</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.900000</td>\n      <td>3.000000</td>\n      <td>1.40000</td>\n      <td>0.200000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.700000</td>\n      <td>3.200000</td>\n      <td>1.30000</td>\n      <td>0.200000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5.823387</td>\n      <td>3.100000</td>\n      <td>1.50000</td>\n      <td>0.200000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.000000</td>\n      <td>3.600000</td>\n      <td>1.40000</td>\n      <td>0.200000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>6.700000</td>\n      <td>3.000000</td>\n      <td>5.20000</td>\n      <td>2.300000</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>6.300000</td>\n      <td>3.081081</td>\n      <td>5.00000</td>\n      <td>1.900000</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>6.500000</td>\n      <td>3.000000</td>\n      <td>5.20000</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>6.200000</td>\n      <td>3.081081</td>\n      <td>3.67541</td>\n      <td>2.300000</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>5.900000</td>\n      <td>3.000000</td>\n      <td>5.10000</td>\n      <td>1.800000</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 4 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {.cell .column-page-right execution_count=64}\n``` {.python .cell-code}\nk_means_zero = KMeans(n_clusters = 4, init = 'random', n_init = 10)\nk_means_zero.fit(df_2)\ndf_2['Four clusters'] = pd.Series(k_means_zero.predict(df_2.iloc[:,0:4].values), index = df_2.index)\nsns.pairplot(df_2, hue = 'Four clusters')\n```\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_4_1_Iris_files/figure-html/cell-65-output-1.png){width=1043 height=946}\n:::\n:::\n\n\n::: {.cell execution_count=65}\n``` {.python .cell-code}\n# PCA analysis\nn_components = 2\n\npca = PCA(n_components=n_components)\ndf_2_pca = pca.fit(df_2.iloc[:,0:4])\n\n# Extract projected values\ndf_2_pca_vals = df_2_pca.transform(df_2.iloc[:,0:4])\ndf_2['c1'] = [item[0] for item in df_2_pca_vals]\ndf_2['c2'] = [item[1] for item in df_2_pca_vals]\n\nsns.scatterplot(data = df_2, x = 'c1', y = 'c2')\n```\n\n::: {.cell-output .cell-output-display execution_count=65}\n```\n<Axes: xlabel='c1', ylabel='c2'>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_4_1_Iris_files/figure-html/cell-66-output-2.png){width=593 height=431}\n:::\n:::\n\n\n::: {.cell execution_count=66}\n``` {.python .cell-code}\ndf_2_pca.explained_variance_\n```\n\n::: {.cell-output .cell-output-display execution_count=66}\n```\narray([3.11438682, 0.302875  ])\n```\n:::\n:::\n\n\n::: {.cell execution_count=67}\n``` {.python .cell-code}\ndf_2_pca.components_\n```\n\n::: {.cell-output .cell-output-display execution_count=67}\n```\narray([[ 0.30735739, -0.08845978,  0.89411274,  0.3134784 ],\n       [ 0.87380921,  0.2994531 , -0.33556897,  0.18487502]])\n```\n:::\n:::\n\n\n## Useful resources\n\nThe scikit learn UserGuide is very good. Both approaches here are often referred to as unsupervised learning methods and you can find the scikit learn section on these [here](https://scikit-learn.org/stable/unsupervised_learning.html).\n\nIf you have issues with the documentation then also look at the scikit-learn [examples](https://scikit-learn.org/stable/auto_examples/index.html).\n\nAlso, in no particular order:\n\n* The [In-Depth sections of the Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/index.html). More for machine learning but interesting all the same.\n* [Python for Data Analysis](https://www.amazon.co.uk/Python-Data-Analysis-Wes-Mckinney/dp/1491957662/ref=sr_1_3?dchild=1&keywords=Python+for+Data+Analysis%3A+Data+Wrangling&qid=1603809746&sr=8-3) (ebook is available via [Warwick library](https://encore.lib.warwick.ac.uk/iii/encore/search/C__Spython%20for%20data%20analysis__Orightresult__U;jsessionid=5A7D1DE9BAC479EE36B491F8FAC8F1FD?lang=eng))\n\nIn case you are bored:\n\n* [Stack abuse](https://stackabuse.com/tag/python/) - Some fun blog entries to look at\n* [Towards data science](https://towardsdatascience.com/) - a blog that contains a mix of intro, intermediate and advanced topics. Nice to skim through to try and undrestand something new.\n\nPlease do try out some of the techniques detailed in the lecture material The simple examples found in the scikit learn documentation are rather good. Generally, I find it much easier to try to understand a method using a simple dataset.\n\n",
    "supporting": [
      "IM939_Lab_4_1_Iris_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}