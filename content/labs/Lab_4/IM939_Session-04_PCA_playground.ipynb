{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Dimension Reduction Methods\n",
    "\n",
    "*A live document to complement the lectures in Week-04*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work with the very simple but highly popular Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline  \n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "column_names = [\"Sepal Length\", \"Sepal Width\", \"Petal Length\", \"Petal Width\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory analysis\n",
    "\n",
    "Let's do a bit of exploratory analysis to check the characteristics of the features and look for redundancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training points\n",
    "\n",
    "columnIndexToUseX = 2\n",
    "columnIndexToUseY = 3\n",
    "\n",
    "x_min, x_max = X[:, columnIndexToUseX].min() - .5, X[:, columnIndexToUseX].max() + .5\n",
    "y_min, y_max = X[:, columnIndexToUseY].min() - .5, X[:, columnIndexToUseY].max() + .5\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(X[:, columnIndexToUseX], X[:, columnIndexToUseY], c=y, cmap=plt.cm.Set1,\n",
    "            edgecolor='k' )\n",
    "plt.xlabel(column_names[columnIndexToUseX])\n",
    "plt.ylabel(column_names[columnIndexToUseY])\n",
    "\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It would be good to have a look at the pairwise correlations. \n",
    "# This might help one to identify some redundant variables at least as long as pairs of dimensions are considered.\n",
    "import seaborn as sns\n",
    "sns.set(style=\"ticks\")\n",
    "\n",
    "## we are cheating here and using a copy of the Iris data as made available as a Pandas DataFrame within the Seaborn package\n",
    "df = sns.load_dataset(\"iris\")\n",
    "sns.pairplot(df, hue=\"species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis \n",
    "\n",
    "Here we start working with PCA to find linear mappings of the original dimension space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# I am stating how many components I would like my final projection space to have\n",
    "n_components = 2\n",
    "\n",
    "# This line creates a PCA computation object\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "# And this line \"projects the data to the newly generated space\"\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# I set the colours that I can use to colour the different classes of iris flowers with. I may not always have this class information but in this instance, we have this information so we can use it for colouring.\n",
    "colors = ['navy', 'turquoise', 'darkorange']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], iris.target_names):\n",
    "    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1],\n",
    "                color=color, lw=2, label=target_name)\n",
    "\n",
    "plt.title(\"PCA of iris dataset\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(column_names)\n",
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### A bit on interpretation\n",
    "\n",
    "OK, what are we looking at here ...\n",
    "\n",
    "The array of numbers we are looking at above are actually the coefficients of the first two principal components, so these \"are\" the components.\n",
    "\n",
    "The first line consists of the \"loadings\" of the variables for the first component, this means: \n",
    "\n",
    "**Sepal Length**'s loading in principal component 1 (PC1) is 0.36138659, **Sepal Width**'s is -0.08452251, **Petal Length**'s is 0.85667061 and **Petal Width**'s is 0.3582892. And the second line of numbers correspond to the loadings of the variables within the \"second\" principal component.\n",
    "\n",
    "But then, what do these numbers tell us? One analysis we can do here is to look at the \"magnitude\" of these numbers. If you look at the four numbers, we notice that the largest in magnitude is for **Petal Length** (0.85667061) which indicates that the most **important** dimension for this component is **Petal Length**. As far as PCA is concerned, the notion of importance is how much of the \"variance\" of the data is explained by a principal component, so Petal Length is the feature that has the most influence on the variation within the data along the first component. And the first component is the computed axis that explains most of the variance in the data (i.e., has the highest eigenvalue in the computations -- see the slides). We can do a similar analysis for the second component, and notice that **Sepal Widht** is the most key variable in this component but I **Sepal Length** is also important as seen from its higher loading value.\n",
    "\n",
    "If we look at these four variables, the only variable that didn't get a high loading is **Petal Width**, this is a sign that this feature might not carry similar levels of variation as the others.\n",
    "\n",
    "#### How about the \"projection\"?\n",
    "\n",
    "What we look at above in the scatterplot, what we are seeing is all the observations \"projected\" onto this new space defined by the two principal components. This is why dimension reduction methods are often referred to as \"projection\" or \"embedding\" methods. The idea is that the new components give you a new \"space\" where you can observe your data points. This is what the `X_pca = pca.fit_transform(X)` line is doing in the code above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA\n",
    "Linear Discriminant Analysis is a method which considers the class labels as well as the data during the projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "X_lda = lda.fit(X, y).transform(X)\n",
    "# Notice here that we are also feeding in the class labels\n",
    "# Since we have three classes but LDA is a binary method, \n",
    "# it will automatically run 1 vs. 2 other classes for all the combinations\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], iris.target_names):\n",
    "    plt.scatter(X_lda[y == i, 0], X_lda[y == i, 1],\n",
    "                color=color, lw=2, label=target_name)\n",
    "\n",
    "plt.title(\"LDA of iris dataset\")\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(column_names)\n",
    "lda.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A regularised method -- SparcePCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A regularised version of PCA -- SparsePCA -- this is one of those Embedded methods that we talked about\n",
    "from sklearn.decomposition import SparsePCA\n",
    "\n",
    "s_pca = SparsePCA(n_components=n_components)\n",
    "Sparse_PCA_embdedded = s_pca.fit_transform(X)\n",
    "\n",
    "colors = ['navy', 'turquoise', 'darkorange']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], iris.target_names):\n",
    "    plt.scatter(Sparse_PCA_embdedded[y == i, 0], Sparse_PCA_embdedded[y == i, 1],\n",
    "                color=color, lw=2, label=target_name)\n",
    "\n",
    "plt.title(\"SparcePCA of iris dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if we observe differences\n",
    "print(column_names)\n",
    "s_pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A non-linear method called tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "TSNE_Model = TSNE(n_components=n_components)\n",
    "tSNE_embdedded = TSNE_Model.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], iris.target_names):\n",
    "    plt.scatter(tSNE_embdedded[y == i, 0], tSNE_embdedded[y == i, 1],\n",
    "                color=color, lw=2, label=target_name)\n",
    "\n",
    "plt.title(\"tSNE of iris dataset\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is tSNE results different?\n",
    "Notice how different the results came up with tSNE. The clusters are much more visible and better seperated. However, the spread within the clusters are mostly gone. And now the axes are not a linear combination of the original features any more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
