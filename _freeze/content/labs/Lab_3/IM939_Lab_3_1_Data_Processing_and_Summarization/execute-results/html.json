{
  "hash": "4bb2c25c0a1056542b33fed2f44db0bb",
  "result": {
    "engine": "jupyter",
    "markdown": "# Lab: Data Processing and Summarization\n\nHere we will be exploring different functions in data processing and summarization.\n\n\n::: {#cell-3 .cell execution_count=1}\n``` {.python .cell-code}\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Importing libraries\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy.spatial.distance import cdist # This is the function that computes the Mahalanobis distance\nfrom scipy.stats import skew, kurtosis # The skew and kurtosis of a distribution\nfrom statsmodels.robust.scale import mad # This computes the median absolute deviation of a distribution\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#%matplotlib inline # Display matplotlib plots inline with the text. +info: https://stackoverflow.com/a/43028034/1913457\n```\n:::\n\n\n## Outliers\n\nIn this section we will be applying several methods to identify outliers. We will be using a custom dataset called `accord_sedan.csv` that contains cars properties. The dataset is provided as a csv file.\n\n### Assess data\n\nAs usual, we will want to start loading the data and assessing it:\n\n::: {#cell-5 .cell execution_count=2}\n``` {.python .cell-code}\n# Loading Data\ndf = pd.read_csv('data/raw/accord_sedan.csv')\n\n# Inspecting the few first rows of the dataframe\ndf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>price</th>\n      <th>mileage</th>\n      <th>year</th>\n      <th>trim</th>\n      <th>engine</th>\n      <th>transmission</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>14995</td>\n      <td>67697</td>\n      <td>2006</td>\n      <td>ex</td>\n      <td>4 Cyl</td>\n      <td>Manual</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>11988</td>\n      <td>73738</td>\n      <td>2006</td>\n      <td>ex</td>\n      <td>4 Cyl</td>\n      <td>Manual</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>11999</td>\n      <td>80313</td>\n      <td>2006</td>\n      <td>lx</td>\n      <td>4 Cyl</td>\n      <td>Automatic</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>12995</td>\n      <td>86096</td>\n      <td>2006</td>\n      <td>lx</td>\n      <td>4 Cyl</td>\n      <td>Automatic</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11333</td>\n      <td>79607</td>\n      <td>2006</td>\n      <td>lx</td>\n      <td>4 Cyl</td>\n      <td>Automatic</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nAnd we can get some summary statistics, too:\n\n::: {#cell-7 .cell execution_count=3}\n``` {.python .cell-code}\ndf.describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>price</th>\n      <th>mileage</th>\n      <th>year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>417.000000</td>\n      <td>417.000000</td>\n      <td>417.0</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>12084.242206</td>\n      <td>89725.779376</td>\n      <td>2006.0</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>2061.430034</td>\n      <td>25957.872271</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>6900.000000</td>\n      <td>19160.000000</td>\n      <td>2006.0</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>10779.000000</td>\n      <td>71844.000000</td>\n      <td>2006.0</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>11995.000000</td>\n      <td>89900.000000</td>\n      <td>2006.0</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>13000.000000</td>\n      <td>106705.000000</td>\n      <td>2006.0</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>18995.000000</td>\n      <td>149269.000000</td>\n      <td>2006.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: callout-caution\n\n### Do this yourself:\n\nWe only get the summaries for the numeric values here. How about the others, such as `trim`, `engine` or `transmission`? Can you think of a way to get an idea of statistics that will provide you an overview of the distribution of these values? Also, do you think the mean of the `year` values make sense? What could be a better statistics here? \n\n::: {#cell-9 .cell execution_count=4}\n``` {.python .cell-code}\n## One thing to try is the describe function with an \"all\" parameter:\ndf.describe(include='all')\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>price</th>\n      <th>mileage</th>\n      <th>year</th>\n      <th>trim</th>\n      <th>engine</th>\n      <th>transmission</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>417.000000</td>\n      <td>417.000000</td>\n      <td>417.0</td>\n      <td>417</td>\n      <td>417</td>\n      <td>417</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>ex</td>\n      <td>4 Cyl</td>\n      <td>Automatic</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>288</td>\n      <td>238</td>\n      <td>382</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>12084.242206</td>\n      <td>89725.779376</td>\n      <td>2006.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>2061.430034</td>\n      <td>25957.872271</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>6900.000000</td>\n      <td>19160.000000</td>\n      <td>2006.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>10779.000000</td>\n      <td>71844.000000</td>\n      <td>2006.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>11995.000000</td>\n      <td>89900.000000</td>\n      <td>2006.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>13000.000000</td>\n      <td>106705.000000</td>\n      <td>2006.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>18995.000000</td>\n      <td>149269.000000</td>\n      <td>2006.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#cell-10 .cell execution_count=5}\n``` {.python .cell-code}\n# But this doesn't address out question. Think about visuals that can help you here? Maybe a histrogram would help?\n\nsns.histplot(data=?, x=\"?\")\n```\n:::\n\n\n\n:::\n\nOnce you have an idea of the descriptive statistics, you might want to focus on the outliers. It might be difficult to spot outliers just with the descriptive statistics. This is where we can use data visualisations to identify outliers visually and also make use of a few proxy metrics to help us assess data points.\n\n\n### Identify outliers visually\n\nIn this case we will **explore variables individually (1D)** by creating a _boxplot_[^boxplot]. We will visualise the columns `price` and `mileage` using matplotlib's subplots, which combines multiple plots into a single figure:\n\n[^boxplot]: Boxplots may be difficult to interpret if we are not used to them. It is used to visualise distributions, where the box represents the _Interquartile range (IQR)_, whereas the whiskers can be defined in multiple ways (e.g. the first and third quartiles, 1.5 IQR...) . ![A visualisation depicting how a Box Plot works. Source: [Wikipedia](https://en.wikipedia.org/wiki/Box_plot)](figs/440px-Boxplot_vs_PDF.svg.png) From [Wikipedia](https://en.wikipedia.org/wiki/Box_plot): \"In descriptive statistics, a box plot or boxplot is a method for graphically demonstrating the locality, spread and skewness groups of numerical data through their quartiles.\" \n\n::: {#cell-13 .cell execution_count=6}\n``` {.python .cell-code}\n# 1D Visualizations\nplt.figure(figsize=(9, 5)) # Setting the figure's size: format width, height (in inches)\nplt.subplot(1,2,1) # subplot(nrows, ncols, index, **kwargs)\nplt.boxplot(df.price)\nplt.title(\"Boxplot of the Price attribute\")\nplt.subplot(1,2,2)\nplt.boxplot(df.mileage)\nplt.title(\"Boxplot of the Mileage attribute\");\n# A semicolon in Python denotes separation, rather than termination. \n# It allows you to write multiple statements on the same line. \n\n```\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_3_1_Data_Processing_and_Summarization_files/figure-html/cell-7-output-1.png){width=741 height=431}\n:::\n:::\n\n\nBut we may want to **identify the 2D outliers** using a scatterplot\n\n::: {#cell-15 .cell execution_count=7}\n``` {.python .cell-code}\n# 2D Visualization\nplt.scatter(df.price, df.mileage, alpha = .5)\nplt.xlabel('Price')\nplt.ylabel('Mileage')\nplt.title('Mileage vs. Price\\n');\n```\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_3_1_Data_Processing_and_Summarization_files/figure-html/cell-8-output-1.png){width=619 height=467}\n:::\n:::\n\n\n::: callout-note\nVisually, outliers appear to be outside the 'normal' range of the rest of the points. A few outliers are quite obvious to spot, but the choice of the threshold (the limit after which you decide to label a point as an outlier) visually remains a very subjective matter.\n:::\n\n### Computing outliers' threshold: standard deviation\n\nAdd two new columns to the dataframe called `isOutlierPrice` and `isOutlierMileage`. We will define our threshold as **2 times standard deviations away from the mean** for `price`  and `mileage`, respectively.\n\n::: {#cell-18 .cell execution_count=8}\n``` {.python .cell-code}\n# Computing the isOutlierPrice column\nupper_threshold_price = df.price.mean() + 2*df.price.std() \nlower_threshold_price = df.price.mean() - 2*df.price.std()\ndf['isOutlierPrice'] = ((df.price > upper_threshold_price) | (df.price < lower_threshold_price))\n\n# Computing the isOutlierMileage column\nupper_threshold_mileage = df.mileage.mean() + 2*df.mileage.std()\nlower_threshold_mileage = df.mileage.mean() - 2*df.mileage.std()\ndf['isOutlierMileage'] = ((df.mileage > upper_threshold_mileage) | (df.mileage < lower_threshold_mileage))\n\n# Inspect the new DataFrame with the added columns\ndf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>price</th>\n      <th>mileage</th>\n      <th>year</th>\n      <th>trim</th>\n      <th>engine</th>\n      <th>transmission</th>\n      <th>isOutlierPrice</th>\n      <th>isOutlierMileage</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>14995</td>\n      <td>67697</td>\n      <td>2006</td>\n      <td>ex</td>\n      <td>4 Cyl</td>\n      <td>Manual</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>11988</td>\n      <td>73738</td>\n      <td>2006</td>\n      <td>ex</td>\n      <td>4 Cyl</td>\n      <td>Manual</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>11999</td>\n      <td>80313</td>\n      <td>2006</td>\n      <td>lx</td>\n      <td>4 Cyl</td>\n      <td>Automatic</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>12995</td>\n      <td>86096</td>\n      <td>2006</td>\n      <td>lx</td>\n      <td>4 Cyl</td>\n      <td>Automatic</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11333</td>\n      <td>79607</td>\n      <td>2006</td>\n      <td>lx</td>\n      <td>4 Cyl</td>\n      <td>Automatic</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: callout-tip\n\n### Alternative method\n\nWe may want to use this more succint approach:\n\n::: {#cell-20 .cell execution_count=9}\n``` {.python .cell-code}\n# Second way of doing the above using the np.where() function\ndf['isOutlierPrice'] = np.where(abs(df.price - df.price.mean()) < 2*df.price.std(), False, True)\ndf['isOutlierMileage'] = np.where(abs(df.mileage - df.mileage.mean()) < 2*df.mileage.std(), False, True)\n\n# Inspect the new DataFrame with the added columns\ndf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>price</th>\n      <th>mileage</th>\n      <th>year</th>\n      <th>trim</th>\n      <th>engine</th>\n      <th>transmission</th>\n      <th>isOutlierPrice</th>\n      <th>isOutlierMileage</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>14995</td>\n      <td>67697</td>\n      <td>2006</td>\n      <td>ex</td>\n      <td>4 Cyl</td>\n      <td>Manual</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>11988</td>\n      <td>73738</td>\n      <td>2006</td>\n      <td>ex</td>\n      <td>4 Cyl</td>\n      <td>Manual</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>11999</td>\n      <td>80313</td>\n      <td>2006</td>\n      <td>lx</td>\n      <td>4 Cyl</td>\n      <td>Automatic</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>12995</td>\n      <td>86096</td>\n      <td>2006</td>\n      <td>lx</td>\n      <td>4 Cyl</td>\n      <td>Automatic</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11333</td>\n      <td>79607</td>\n      <td>2006</td>\n      <td>lx</td>\n      <td>4 Cyl</td>\n      <td>Automatic</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nAs we'd expect, both methods produce the same output.\n\n:::\n\nNow that we have these two new columns, we could visualize these values with a different color in the plot. Observe whether they are the same as you would mark them:\n\n::: {#cell-23 .cell execution_count=10}\n``` {.python .cell-code}\n# Visualizing outliers in a different color\ncol = ['tomato' if i+j else 'seagreen' for i,j in zip(df.isOutlierPrice, df.isOutlierMileage)]\nplt.scatter(df.price, df.mileage, color = col)\nplt.xlabel('Price')\nplt.ylabel('Mileage')\nplt.title('Mileage vs. Price : Outliers 2+ std\\'s away from the mean\\n');\n```\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_3_1_Data_Processing_and_Summarization_files/figure-html/cell-11-output-1.png){width=619 height=467}\n:::\n:::\n\n\nVisually filtering out outliers can be an effective tactic if we're just trying to conduct a quick and dirty experimentation. However, when we need to perform a solid and founded analysis, it's better to have a robust justification for our choices.  \n\nIn this case, we can use the deviation from the mean to define a threshold that separates 'normal' values from 'outliers'. Here, we opted for a two standard deviation threshold. \n\nThe mathematical intuition behind this, is that under the normality assumption (if we assume our variable is normally distributed, which it almost is, refer to the next plot), then the probability of it having a value two standard deviations OR MORE away from the mean, is around 5%, which is very unlikely to happen. This is why we label these data points as outliers with respect to the (assumed) probability distribution of the variable. But this remains a way to identify 1D outliers only (identifying outliers within each column separately)\n\n::: {#cell-25 .cell execution_count=11}\n``` {.python .cell-code}\n# Histograms of Price and Mileage (checking the normality assumption)\nplt.subplot(1,2,1)\nplt.hist(df.price, bins = 12)\nplt.title('Price')\nplt.subplot(1,2,2)\nplt.hist(df.mileage, bins = 15)\nplt.title('Mileage');\n```\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_3_1_Data_Processing_and_Summarization_files/figure-html/cell-12-output-1.png){width=582 height=431}\n:::\n:::\n\n\n### Computing outliers' threshold: Mahalanobis distance\n\n1) Using the 2D Mahalanobis distance to find outliers\n\n::: {#cell-27 .cell .column-page-inset-right execution_count=12}\n``` {.python .cell-code}\n# Mean vector (computing the mean returns a Series, which we need to convert back to a DataFrame because cdist requires it)\nmean_v = df.iloc[:, 0:2].mean().to_frame().T # DataFrame.T returns the Transpose of the DataFrame\n#mean_v = np.asarray([df.price.mean(), df.mileage.mean() ]).reshape(1,2) # This is a better way of writing the line before (for our use case : cdist function)\n\n# Computing the Mahalanobis distance of each row to the mean vector\nd = cdist(df.iloc[:, 0:2], mean_v, metric='mahalanobis')\n#d = cdist(df[['price', 'mileage']].values, mean_v, metric='mahalanobis') # Another way of writing the line before\n\n# Visualizing the scatter plot while coloring each point (i.e row) with a color from a chosen gradient colormap corresponding to the mahalanobis score\nplt.figure(figsize=(12, 5))\nplt.scatter(df.price, df.mileage, c = d.flatten(), cmap = 'plasma') # in order to know why we use flatten() on d, try printing d with and without flatten\nplt.colorbar() # to show the colorbar\nplt.xlabel('Price')\nplt.ylabel('Mileage')\nplt.title('Mileage vs. Price colored by a 2D Mahalanobis score\\n');\n```\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_3_1_Data_Processing_and_Summarization_files/figure-html/cell-13-output-1.png){width=893 height=467}\n:::\n:::\n\n\n## Q-Q Plots\n\n### Data\n\nIn this case, we will be using data reported by countries to WHO and estimates of tuberculosis burden generated by WHO for the _Global Tuberculosis Report_, as part of WHO's [Global Tuberculosis Programme](https://www.who.int/teams/global-tuberculosis-programme/data). You can refer to [this section](https://www.who.int/teams/global-tuberculosis-programme/data#csv_files) at WHO's website to know more about the data and their variables.\n\nFor your convenience, we have stored a copy in a csv file, but you could try to replicate the code with more up-to-date data from the original website. As usual, it is a best practice to explore the data before proceeding any further:\n\n::: {#cell-30 .cell execution_count=13}\n``` {.python .cell-code}\n# Getting the Data\ndf_tuber = pd.read_csv('data/raw/TB_burden_countries_2014-09-29.csv')\n\ndf_tuber.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>country</th>\n      <th>iso2</th>\n      <th>iso3</th>\n      <th>iso_numeric</th>\n      <th>g_whoregion</th>\n      <th>year</th>\n      <th>e_pop_num</th>\n      <th>e_prev_100k</th>\n      <th>e_prev_100k_lo</th>\n      <th>e_prev_100k_hi</th>\n      <th>...</th>\n      <th>e_inc_tbhiv_100k</th>\n      <th>e_inc_tbhiv_100k_lo</th>\n      <th>e_inc_tbhiv_100k_hi</th>\n      <th>e_inc_tbhiv_num</th>\n      <th>e_inc_tbhiv_num_lo</th>\n      <th>e_inc_tbhiv_num_hi</th>\n      <th>source_tbhiv</th>\n      <th>c_cdr</th>\n      <th>c_cdr_lo</th>\n      <th>c_cdr_hi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Afghanistan</td>\n      <td>AF</td>\n      <td>AFG</td>\n      <td>4</td>\n      <td>EMR</td>\n      <td>1990</td>\n      <td>11731193</td>\n      <td>327.0</td>\n      <td>112.0</td>\n      <td>655.0</td>\n      <td>...</td>\n      <td>0.35</td>\n      <td>0.22</td>\n      <td>0.52</td>\n      <td>41.0</td>\n      <td>25.0</td>\n      <td>60.0</td>\n      <td>Model</td>\n      <td>20.0</td>\n      <td>13.0</td>\n      <td>32.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Afghanistan</td>\n      <td>AF</td>\n      <td>AFG</td>\n      <td>4</td>\n      <td>EMR</td>\n      <td>1991</td>\n      <td>12612043</td>\n      <td>359.0</td>\n      <td>172.0</td>\n      <td>613.0</td>\n      <td>...</td>\n      <td>0.36</td>\n      <td>0.19</td>\n      <td>0.58</td>\n      <td>45.0</td>\n      <td>24.0</td>\n      <td>73.0</td>\n      <td>Model</td>\n      <td>97.0</td>\n      <td>77.0</td>\n      <td>120.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Afghanistan</td>\n      <td>AF</td>\n      <td>AFG</td>\n      <td>4</td>\n      <td>EMR</td>\n      <td>1992</td>\n      <td>13811876</td>\n      <td>387.0</td>\n      <td>169.0</td>\n      <td>693.0</td>\n      <td>...</td>\n      <td>0.37</td>\n      <td>0.19</td>\n      <td>0.62</td>\n      <td>51.0</td>\n      <td>26.0</td>\n      <td>86.0</td>\n      <td>Model</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Afghanistan</td>\n      <td>AF</td>\n      <td>AFG</td>\n      <td>4</td>\n      <td>EMR</td>\n      <td>1993</td>\n      <td>15175325</td>\n      <td>412.0</td>\n      <td>186.0</td>\n      <td>724.0</td>\n      <td>...</td>\n      <td>0.38</td>\n      <td>0.20</td>\n      <td>0.63</td>\n      <td>58.0</td>\n      <td>30.0</td>\n      <td>95.0</td>\n      <td>Model</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Afghanistan</td>\n      <td>AF</td>\n      <td>AFG</td>\n      <td>4</td>\n      <td>EMR</td>\n      <td>1994</td>\n      <td>16485018</td>\n      <td>431.0</td>\n      <td>199.0</td>\n      <td>751.0</td>\n      <td>...</td>\n      <td>0.40</td>\n      <td>0.21</td>\n      <td>0.64</td>\n      <td>65.0</td>\n      <td>35.0</td>\n      <td>100.0</td>\n      <td>Model</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 39 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {#cell-31 .cell execution_count=14}\n``` {.python .cell-code}\ndf_tuber.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 4903 entries, 0 to 4902\nData columns (total 39 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   country                   4903 non-null   object \n 1   iso2                      4880 non-null   object \n 2   iso3                      4903 non-null   object \n 3   iso_numeric               4903 non-null   int64  \n 4   g_whoregion               4903 non-null   object \n 5   year                      4903 non-null   int64  \n 6   e_pop_num                 4903 non-null   int64  \n 7   e_prev_100k               4892 non-null   float64\n 8   e_prev_100k_lo            4892 non-null   float64\n 9   e_prev_100k_hi            4892 non-null   float64\n 10  e_prev_num                4892 non-null   float64\n 11  e_prev_num_lo             4892 non-null   float64\n 12  e_prev_num_hi             4892 non-null   float64\n 13  e_mort_exc_tbhiv_100k     4902 non-null   float64\n 14  e_mort_exc_tbhiv_100k_lo  4902 non-null   float64\n 15  e_mort_exc_tbhiv_100k_hi  4902 non-null   float64\n 16  e_mort_exc_tbhiv_num      4902 non-null   float64\n 17  e_mort_exc_tbhiv_num_lo   4902 non-null   float64\n 18  e_mort_exc_tbhiv_num_hi   4902 non-null   float64\n 19  source_mort               4902 non-null   object \n 20  e_inc_100k                4902 non-null   float64\n 21  e_inc_100k_lo             4902 non-null   float64\n 22  e_inc_100k_hi             4902 non-null   float64\n 23  e_inc_num                 4902 non-null   float64\n 24  e_inc_num_lo              4902 non-null   float64\n 25  e_inc_num_hi              4902 non-null   float64\n 26  e_tbhiv_prct              3653 non-null   float64\n 27  e_tbhiv_prct_lo           3457 non-null   float64\n 28  e_tbhiv_prct_hi           3457 non-null   float64\n 29  e_inc_tbhiv_100k          3597 non-null   float64\n 30  e_inc_tbhiv_100k_lo       3597 non-null   float64\n 31  e_inc_tbhiv_100k_hi       3597 non-null   float64\n 32  e_inc_tbhiv_num           3597 non-null   float64\n 33  e_inc_tbhiv_num_lo        3597 non-null   float64\n 34  e_inc_tbhiv_num_hi        3597 non-null   float64\n 35  source_tbhiv              4902 non-null   object \n 36  c_cdr                     4476 non-null   float64\n 37  c_cdr_lo                  4476 non-null   float64\n 38  c_cdr_hi                  4476 non-null   float64\ndtypes: float64(30), int64(3), object(6)\nmemory usage: 1.5+ MB\n```\n:::\n:::\n\n\nAs you can see we have a pretty large dataset numerical and categorical variables, some of which have many missing values. In this case we will be filling missing values with the mean[^missing-data]\n\n[^missing-data]: Refer to @sec-inputting-missing-data for a detailed explanation and alternative methods to inputting missing values.\n\n::: {#cell-33 .cell execution_count=15}\n``` {.python .cell-code}\n# Filling missing numeric values ONLY (categorical values will be untouched)\ndf_tuber = df_tuber.fillna(value=df_tuber.mean(numeric_only = True))\n\n# Count missing values\ndf_tuber.isna().sum()\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\ncountry                      0\niso2                        23\niso3                         0\niso_numeric                  0\ng_whoregion                  0\nyear                         0\ne_pop_num                    0\ne_prev_100k                  0\ne_prev_100k_lo               0\ne_prev_100k_hi               0\ne_prev_num                   0\ne_prev_num_lo                0\ne_prev_num_hi                0\ne_mort_exc_tbhiv_100k        0\ne_mort_exc_tbhiv_100k_lo     0\ne_mort_exc_tbhiv_100k_hi     0\ne_mort_exc_tbhiv_num         0\ne_mort_exc_tbhiv_num_lo      0\ne_mort_exc_tbhiv_num_hi      0\nsource_mort                  1\ne_inc_100k                   0\ne_inc_100k_lo                0\ne_inc_100k_hi                0\ne_inc_num                    0\ne_inc_num_lo                 0\ne_inc_num_hi                 0\ne_tbhiv_prct                 0\ne_tbhiv_prct_lo              0\ne_tbhiv_prct_hi              0\ne_inc_tbhiv_100k             0\ne_inc_tbhiv_100k_lo          0\ne_inc_tbhiv_100k_hi          0\ne_inc_tbhiv_num              0\ne_inc_tbhiv_num_lo           0\ne_inc_tbhiv_num_hi           0\nsource_tbhiv                 1\nc_cdr                        0\nc_cdr_lo                     0\nc_cdr_hi                     0\ndtype: int64\n```\n:::\n:::\n\n\n1) Pick one of the columns from the Tuberculosis data and copy it into a numpy array as before\n\n::: {#cell-35 .cell execution_count=16}\n``` {.python .cell-code}\n# Picking a column (I created a variable for this so I (and you (: ) can modify the column easily here and the change will be carried out everywhere I use the variable colname)\ncolname = 'e_prev_100k'\n\n# Creating a numpy array from our column\ncol = np.array(df_tuber[colname])\n\n# Printing the type of our newly created column\nprint(type(col))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'numpy.ndarray'>\n```\n:::\n:::\n\n\n1) Compare this selected column to a Normal distribution. Then Sample from a Normal distribution and show a second Q-Q plot\n\n::: {#cell-37 .cell execution_count=17}\n``` {.python .cell-code}\n# Plotting the Q-Q Plot for our column\nsm.qqplot(col, line='r')\nplt.title('Q-Q Plot of the \"{}\" column of our dataset'.format(colname));\n\n# Plotting the Q-Q Plot for the log of our column\nsm.qqplot(np.log(col), line='r')\nplt.title('Q-Q Plot of the Log of the \"{}\" column'.format(colname));\n\n# Sampling from a Gaussian and a uniform distribution\nsample_norm = np.random.randn(1000)\nsample_unif = np.random.rand(1000)\n\n# Plotting the second Q-Q Plot for our sample (that was generated using a normal distribution)\nsm.qqplot(sample_norm, line='r')\nplt.title('Q-Q Plot of the generated sample (Gaussian)')\nsm.qqplot(sample_unif, line='r')\nplt.title('Q-Q Plot of the generated sample (Uniform)');\n```\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_3_1_Data_Processing_and_Summarization_files/figure-html/cell-18-output-1.png){width=604 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_3_1_Data_Processing_and_Summarization_files/figure-html/cell-18-output-2.png){width=585 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_3_1_Data_Processing_and_Summarization_files/figure-html/cell-18-output-3.png){width=587 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_3_1_Data_Processing_and_Summarization_files/figure-html/cell-18-output-4.png){width=609 height=449}\n:::\n:::\n\n\n<p>\n    Go ahead and change the colname variable (question 1) into a different column name (that you can pick from the list you have just before question 1 (but do pick a numeric column). And re-execute the code from question 1 and question 2 and you'll see your new Q-Q plot of the column you just picked.\n</p>\n\n1) Have a look at the slides from Week 03 for different shapes\n\n<p>\n    Ok ? Now try to guess the shape of the distribution of our selected column (shape of its histogram) from its Q-Q Plot above.\n</p>\n\n1) Visualise the column on a histogram and reflect on whether the shape you inferred from Q-Q plots and the shape of the histogram correlate\n\n::: {#cell-42 .cell .column-page-inset-right execution_count=18}\n``` {.python .cell-code}\n# Histogramming the column we picked (not sure the verb exists though)\nplt.figure(figsize=(12, 5))\nplt.subplot(1,2,1)\nplt.title('Histogram of \"{}\"'.format(colname))\nplt.hist(col)\nplt.subplot(1,2,2)\nplt.title('Histogram of Log \"{}\"'.format(colname))\nplt.hist(np.log(col));\n```\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_3_1_Data_Processing_and_Summarization_files/figure-html/cell-19-output-1.png){width=955 height=431}\n:::\n:::\n\n\n<p>\n    Of course it does ! From the shape of the Q-Q Plot above (convex, slope upwards) and the Slide of Q-Q Plots from Week 3, we could conclude before looking at the histogram that our distribution was right tailed (or positively skewed if you're into complex vocabulary lol). And it is !\n</p>\n\n## Distributions, Sampling\n\n1) Inspecting the effect of sample size on descriptive statistics\n\n::: {#cell-46 .cell execution_count=19}\n``` {.python .cell-code}\n# Defining a few variables se we can change their values easiliy without having to change the rest of the code\nn = [5, 20, 100, 2000]\nstds = [0.5, 1, 3]\n\n# Initializing empty 2D arrays where we're going to store the results of our simulation\nmean = np.empty([len(n), len(stds)])\nstd = np.empty([len(n), len(stds)])\nskewness = np.empty([len(n), len(stds)])\nkurtos = np.empty([len(n), len(stds)])\n\n# Conducting the experiments and storing the results in the respective 2D arrays\nfor i, sample_size in enumerate(n):\n    for j, theoritical_std in enumerate(stds):\n        sample = np.random.normal(loc=0, scale=theoritical_std, size=sample_size)\n        mean[i,j] = sample.mean()\n        std[i,j] = sample.std()\n        skewness[i,j] = skew(sample)\n        kurtos[i,j] = kurtosis(sample)\n\n# Turning the mean 2D array into a pandas dataframe\nmean = pd.DataFrame(mean, columns = stds, index = n)\nmean = mean.rename_axis('Sample Size').rename_axis(\"Standard Deviation\", axis=\"columns\")\n\n# Turning the std 2D array into a pandas dataframe\nstd = pd.DataFrame(std, columns = stds, index = n)\nstd = std.rename_axis('Sample Size').rename_axis(\"Standard Deviation\", axis=\"columns\")\n\n# Turning the skewness 2D array into a pandas dataframe\nskewness = pd.DataFrame(skewness, columns = stds, index = n)\nskewness = skewness.rename_axis('Sample Size').rename_axis(\"Standard Deviation\", axis=\"columns\")\n\n# Turning the kurtosis 2D array into a pandas dataframe\nkurtos = pd.DataFrame(kurtos, columns = stds, index = n)\nkurtos = kurtos.rename_axis('Sample Size').rename_axis(\"Standard Deviation\", axis=\"columns\")\n\nprint(\"GAUSSIAN DISTRIBUTION\\n\")\nprint('Results for the Mean :')\nmean # This is a dataframe containing the means of the samples generated with different values of std and sample size\nprint('Results for the Standard Deviation :')\nstd # This is a dataframe containing the standard deviations of the samples generated with different values of std and sample size\nprint('Results for the Skewness :')\nskewness # This is a dataframe containing the skews of the samples generated with different values of std and sample size\nprint('Results for the Kurtosis :')\nkurtos # This is a dataframe containing the kurtosis of the samples generated with different values of std and sample size\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGAUSSIAN DISTRIBUTION\n\nResults for the Mean :\nResults for the Standard Deviation :\nResults for the Skewness :\nResults for the Kurtosis :\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=18}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Standard Deviation</th>\n      <th>0.5</th>\n      <th>1.0</th>\n      <th>3.0</th>\n    </tr>\n    <tr>\n      <th>Sample Size</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5</th>\n      <td>-0.989636</td>\n      <td>-1.264620</td>\n      <td>-1.095388</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>-0.821398</td>\n      <td>0.040254</td>\n      <td>-0.278572</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>-0.358892</td>\n      <td>0.540897</td>\n      <td>1.357763</td>\n    </tr>\n    <tr>\n      <th>2000</th>\n      <td>0.106809</td>\n      <td>-0.094725</td>\n      <td>-0.039734</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n<p>\n    Basically, the more data you have (the bigger your sample), the more accurate your empirical estimates are going to be. Observe for example the values of mean (1st DataFrame) and variance (2nd DataFrame) for the 2000 sample size (last row). In the first one, the values are all close to 0, because we generated our sample from a Gaussian with mean 0, and the values in the second one are all close to the values in the column names (which refer to the variance of the distribution of the sample). This means that for with a sample size of 2000, our estimates are really close to the \"True\" values (with which we generated the sample). Also, the Skew of a Gaussian distribution should be 0, and it is confirmed in the 3rd DataFrame where the values are close to 0 in the last row (i.e big sample size).\n</p>\n\n<b>2) Same as before but with a Poisson distribution (which has just one parameter lambda instead of 2 like the gaussian)</b>\n\n::: {#cell-49 .cell execution_count=20}\n``` {.python .cell-code}\n# Defining a few variables se we can change their values easiliy without having to change the rest of the code\nn = [5, 20, 100, 2000]\nlambd = [0.5, 1, 3] # In a gaussian we had two parameters, a var specified here and a mean we chose to be 0 \n#everywhere. Here we have one parameter called lambda.\n\n# Initializing empty 2D arrays where we're going to store the results of our simulation\nmean = np.empty([len(n), len(lambd)])\nstd = np.empty([len(n), len(lambd)])\nskewness = np.empty([len(n), len(lambd)])\nkurtos = np.empty([len(n), len(lambd)])\n\n# Conducting the experiments and storing the results in the respective 2D arrays\nfor i, sample_size in enumerate(n):\n    for j, theoritical_lambd in enumerate(lambd):\n        #**********************************************************************\n        sample = np.random.poisson(lam = theoritical_lambd, size = sample_size) # THIS IS WHAT WE CHANGED IN Q2 !\n        #**********************************************************************\n        mean[i,j] = sample.mean()\n        std[i,j] = sample.std()\n        skewness[i,j] = skew(sample)\n        kurtos[i,j] = kurtosis(sample)\n\n# Turning the mean 2D array into a pandas dataframe\nmean = pd.DataFrame(mean, columns = lambd, index = n)\nmean = mean.rename_axis('Sample Size').rename_axis(\"Lambda\", axis=\"columns\")\n\n# Turning the std 2D array into a pandas dataframe\nstd = pd.DataFrame(std, columns = lambd, index = n)\nstd = std.rename_axis('Sample Size').rename_axis(\"Lambda\", axis=\"columns\")\n\n# Turning the skewness 2D array into a pandas dataframe\nskewness = pd.DataFrame(skewness, columns = lambd, index = n)\nskewness = skewness.rename_axis('Sample Size').rename_axis(\"Lambda\", axis=\"columns\")\n\n# Turning the kurtosis 2D array into a pandas dataframe\nkurtos = pd.DataFrame(kurtos, columns = lambd, index = n)\nkurtos = kurtos.rename_axis('Sample Size').rename_axis(\"Lambda\", axis=\"columns\")\n\nprint(\"POISSON DISTRIBUTION\\n\")\nprint('Results for the Mean :')\nmean # This is a dataframe containing the means of the samples generated with different values of std and sample size\nprint('Results for the Standard Deviation :')\nstd # This is a dataframe containing the standard deviations of the samples generated with different values of std \n#and sample size\nprint('Results for the Skewness :')\nskewness # This is a dataframe containing the skews of the samples generated with different values of std and sample \n#size\nprint('Results for the Kurtosis :')\nkurtos # This is a dataframe containing the kurtosis of the samples generated with different values of std and sample \n#size\n\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPOISSON DISTRIBUTION\n\nResults for the Mean :\nResults for the Standard Deviation :\nResults for the Skewness :\nResults for the Kurtosis :\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=19}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Lambda</th>\n      <th>0.5</th>\n      <th>1.0</th>\n      <th>3.0</th>\n    </tr>\n    <tr>\n      <th>Sample Size</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5</th>\n      <td>-1.833333</td>\n      <td>-0.500000</td>\n      <td>-0.623736</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.611570</td>\n      <td>-0.985100</td>\n      <td>-1.206222</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.932329</td>\n      <td>1.057900</td>\n      <td>-0.013258</td>\n    </tr>\n    <tr>\n      <th>2000</th>\n      <td>1.596932</td>\n      <td>0.400179</td>\n      <td>0.241682</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n<p>\n    Just remember, the lambda parameter that defines the Poisson distribution is also the mean of the distribution. This is confirmed in the first DataFrame where the values (means of samples) are close to the column labels (theoretical lambda which is also equal to theoretical mean), especially in the last row.\n</p>\n\n## Robust Statistics\n\n1) Choose a number of columns with different shapes, for instance, \"e_prev_100k_hi\" is left skewed or some columns where the variation is high or you notice potential outliers. You can make use of a series of boxplots to exploratively analyse the data for outliers\n\n::: {#cell-53 .cell execution_count=21}\n``` {.python .cell-code}\n# Listing the columns\ndf_tuber.columns\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\nIndex(['country', 'iso2', 'iso3', 'iso_numeric', 'g_whoregion', 'year',\n       'e_pop_num', 'e_prev_100k', 'e_prev_100k_lo', 'e_prev_100k_hi',\n       'e_prev_num', 'e_prev_num_lo', 'e_prev_num_hi', 'e_mort_exc_tbhiv_100k',\n       'e_mort_exc_tbhiv_100k_lo', 'e_mort_exc_tbhiv_100k_hi',\n       'e_mort_exc_tbhiv_num', 'e_mort_exc_tbhiv_num_lo',\n       'e_mort_exc_tbhiv_num_hi', 'source_mort', 'e_inc_100k', 'e_inc_100k_lo',\n       'e_inc_100k_hi', 'e_inc_num', 'e_inc_num_lo', 'e_inc_num_hi',\n       'e_tbhiv_prct', 'e_tbhiv_prct_lo', 'e_tbhiv_prct_hi',\n       'e_inc_tbhiv_100k', 'e_inc_tbhiv_100k_lo', 'e_inc_tbhiv_100k_hi',\n       'e_inc_tbhiv_num', 'e_inc_tbhiv_num_lo', 'e_inc_tbhiv_num_hi',\n       'source_tbhiv', 'c_cdr', 'c_cdr_lo', 'c_cdr_hi'],\n      dtype='object')\n```\n:::\n:::\n\n\n::: {#cell-54 .cell .column-screen-inset execution_count=22}\n``` {.python .cell-code}\n# Alright I already know a few columns with outliers but let's try to find them together exploratively using BoxPlots\ncolname = 'c_cdr' # change the column name by choosing different ones from above (numeric ones)\nplt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nplt.hist(df_tuber[colname])\nplt.title('Histogram of \"{}\"'.format(colname))\nplt.subplot(1,2,2)\nplt.boxplot(df_tuber[colname]);\nplt.title('Boxplot of \"{}\"'.format(colname));\n```\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_3_1_Data_Processing_and_Summarization_files/figure-html/cell-23-output-1.png){width=1179 height=431}\n:::\n:::\n\n\n::: {#cell-55 .cell execution_count=23}\n``` {.python .cell-code}\n# Chosen columns : I picked 3, feel free to change them and experiment\nchosen_colnames = ['e_pop_num', 'e_prev_100k', 'c_cdr']\n```\n:::\n\n\n<b>2) For the chosen columns, estimate both the conventional and the robust descriptive statistics and compare. Observe how these pairs deviate from each other based on the characteristics of the underlying data</b>\n\n::: {#cell-57 .cell execution_count=24}\n``` {.python .cell-code}\n# Central Tendency : Mean vs Median (Median is the robust version of the mean, because it takes into account \n#the ordering of the points and not the actual values like the mean does)\ndf_tuber[chosen_colnames].describe().loc[['mean', '50%'], :] # The 50% is the median (50% quantile)\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>e_pop_num</th>\n      <th>e_prev_100k</th>\n      <th>c_cdr</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>mean</th>\n      <td>2.899179e+07</td>\n      <td>207.694422</td>\n      <td>67.570706</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>5.140332e+06</td>\n      <td>93.000000</td>\n      <td>70.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nLook at how the values are different between the mean and the median ... LOOOOOOOK ! This is why when you have a skewed (unsymmetrical) distribution it's usually more interesting to use the median as a measure of the central tendency of the data. One important thing to note here, for the two first attributes, the mean is higher than the median, but for the last it's the opposite. This can tell you a thing or two about the shape of your distribution : if the mean is higher than the median, this means that the distribution is skewed to the right (right tail) which pulls the mean higher. And vice-versa.\n\n\nMoral of the story is ... outliers are a pain in the a**. \n\n::: {#cell-59 .cell execution_count=25}\n``` {.python .cell-code}\n# Spread : Standard Deviation vs Inter-Quartile Range vs Median Absolute Deviation (MAD)\nstds = df_tuber[chosen_colnames].std()\niqrs = df_tuber[chosen_colnames].quantile(0.75) - df_tuber[chosen_colnames].quantile(0.25)\nmedianAD = mad(df_tuber[chosen_colnames])\n\noutput = pd.DataFrame(stds, columns = ['std']).T\noutput = pd.concat([output, pd.DataFrame(iqrs, columns = ['IQR']).T], ignore_index=False)\noutput = pd.concat([output, pd.DataFrame(medianAD, columns = ['MAD'], index = chosen_colnames).T], ignore_index=False, names = ['std', 'iqr', 'mad'])\noutput\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>e_pop_num</th>\n      <th>e_prev_100k</th>\n      <th>c_cdr</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>std</th>\n      <td>1.177827e+08</td>\n      <td>269.418159</td>\n      <td>25.234773</td>\n    </tr>\n    <tr>\n      <th>IQR</th>\n      <td>1.677193e+07</td>\n      <td>280.500000</td>\n      <td>34.000000</td>\n    </tr>\n    <tr>\n      <th>MAD</th>\n      <td>7.454908e+06</td>\n      <td>120.090780</td>\n      <td>25.204238</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThe values here are different as well, maybe more so for the \"e_pop_num\" attribute than the others, but that is just because of the scaling : \"e_pop_num\" takes big values overall compared to the other columns, which you can check with the mean values right above.\n\nFor the first attribute, the standard deviation is higher, and both the IQR and MAD are close to each other. For the second attribute, the inter-quartile range is slightly higher than the standard deviation, but the MAD is far below (less than half) the other two values, and the reason for that is a little bit involved : Basically, the standard deviation measures the spread by computing the squared deviation from the mean while the median absolute deviation evaluates the spread by computing the absolute deviation. This means that when the outliers have much bigger values than the \"normal\" points, the squared difference explodes (figuratively of course ;p) compared to the absolute difference. And this is actually the case for our second distribution (e_prev_100k) where most values are between 50 and 300 while many outliers lay above the 750 mark and go all the way up to 1800 (look at the boxplots below). For the third attribute the values are somewhat close, especially the std and the MAD, that's because if you inspect the boxplot, this column doesn't have many outliers to begin with.\n\nNonetheless, the differences are real, and if we don't want to have to handle outliers, then we should be using robust statistics like the median to describe the central tendency and inter-quartile range or median absolute deviation to measure the spread of our data.\n\n::: {#cell-61 .cell .column-page-inset-right execution_count=26}\n``` {.python .cell-code}\n# Boxplots of the different columns\nplt.figure(figsize=(12,20))\n\nplt.subplot(3,2,1)\nplt.hist(df_tuber[chosen_colnames[0]])\nplt.title('Histogram of \"{}\"'.format(chosen_colnames[0]))\nplt.subplot(3,2,2)\nplt.boxplot(df_tuber[chosen_colnames[0]])\nplt.title('Boxplot of \"{}\"'.format(chosen_colnames[0]))\n\nplt.subplot(3,2,3)\nplt.hist(df_tuber[chosen_colnames[1]])\nplt.title('Histogram of \"{}\"'.format(chosen_colnames[1]))\nplt.subplot(3,2,4)\nplt.boxplot(df_tuber[chosen_colnames[1]])\nplt.title('Boxplot of \"{}\"'.format(chosen_colnames[1]))\n\nplt.subplot(3,2,5)\nplt.hist(df_tuber[chosen_colnames[2]])\nplt.title('Histogram of \"{}\"'.format(chosen_colnames[2]))\nplt.subplot(3,2,6)\nplt.boxplot(df_tuber[chosen_colnames[2]])\nplt.title('Boxplot of \"{}\"'.format(chosen_colnames[2]));\n```\n\n::: {.cell-output .cell-output-display}\n![](IM939_Lab_3_1_Data_Processing_and_Summarization_files/figure-html/cell-27-output-1.png){width=955 height=1540}\n:::\n:::\n\n\n",
    "supporting": [
      "IM939_Lab_3_1_Data_Processing_and_Summarization_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}